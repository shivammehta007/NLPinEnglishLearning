{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generation of Blanks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "18wNtFnmO3y2d3nSyqb0nTr02PtpTUBS3",
      "authorship_tag": "ABX9TyMz4SR1+JMMKiS71bLGCVqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/Generation_of_Blanks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU4dAfBwhK3b",
        "colab_type": "text"
      },
      "source": [
        "# Fill in the blank Generator With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpdPjQ5jSn_",
        "colab_type": "text"
      },
      "source": [
        "## Download Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDrIopZhIaO",
        "colab_type": "code",
        "outputId": "c9b74520-284c-4758-b834-4e1f0bfa7e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!rm -rf QuestionGenerator\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import subprocess\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone --single-branch --branch master  https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, 'QuestionGenerator')\n",
        "stdout = subprocess.Popen(cmd_string, shell=True, stdout=subprocess.PIPE)\n",
        "print(stdout.communicate()[0])\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: shivammehta007\n",
            "Password: ··········\n",
            "b''\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7g4AhObih2-",
        "colab_type": "code",
        "outputId": "fa7583bb-a27c-4da4-875f-c90f0d2f86d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -rf QuestionGenerator/FromScratch\n",
        "!rm -rf QuestionGenerator/classifier\n",
        "%cd QuestionGenerator/FITBGenerator/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/FITBGenerator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcDcDeTylq76",
        "colab_type": "code",
        "outputId": "bbaa2a38-c82f-41d8-ce7c-5e03bfccd6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/  datasetloader.py    lossfunction.py  preprocessdata.py  utility.py\n",
            "\u001b[01;34mdata\u001b[0m/    helperfunctions.py  \u001b[01;34mmodel\u001b[0m/           train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "6f07d6e2-3f7e-4c9f-8148-6391194da53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2744  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "83eb335c-5036-4151-cca9-b92d093425c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/QuestionGenerator/FITBGenerator\n",
            " 98% 377M/386M [00:06<00:00, 82.0MB/s]\n",
            "100% 386M/386M [00:06<00:00, 61.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "8e339d1f-1520-4092-b5e6-069d961f4e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3GrnD7jOpx",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSBYYV6qHXFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python preprocessdata.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ4_Jw_lp3x",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6sicYFjuV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1bd2206-6391-4153-df70-2e51208f8952"
      },
      "source": [
        "!python train.py -n 50"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:221 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234)\n",
            "[DEBUG | train.py:222 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:224 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:74 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:85 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:94 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:228 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:66 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:92 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:98 - initialize_new_model() ] Model Initialized with 440,577 trainiable parameters\n",
            "[DEBUG | train.py:110 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:253 -             <module>() ] RNNHiddenClassifier(\n",
            "  (embedding): Embedding(564, 300, padding_idx=1)\n",
            "  (rnn): LSTMWithPackPaddedSequences(\n",
            "    (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 5/5 [00:00<00:00, 82.54it/s]\n",
            "100% 1/1 [00:00<00:00, 258.00it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.565 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.323 |  Val. Acc: 51.94%\n",
            "100% 5/5 [00:00<00:00, 153.41it/s]\n",
            "100% 1/1 [00:00<00:00, 237.69it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.527 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.321 |  Val. Acc: 52.80%\n",
            "100% 5/5 [00:00<00:00, 145.39it/s]\n",
            "100% 1/1 [00:00<00:00, 184.33it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.498 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.318 |  Val. Acc: 56.47%\n",
            "100% 5/5 [00:00<00:00, 176.42it/s]\n",
            "100% 1/1 [00:00<00:00, 282.69it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.466 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.312 |  Val. Acc: 62.50%\n",
            "100% 5/5 [00:00<00:00, 173.06it/s]\n",
            "100% 1/1 [00:00<00:00, 278.27it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.434 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.311 |  Val. Acc: 67.03%\n",
            "100% 5/5 [00:00<00:00, 183.52it/s]\n",
            "100% 1/1 [00:00<00:00, 267.51it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.405 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.291 |  Val. Acc: 71.12%\n",
            "100% 5/5 [00:00<00:00, 178.02it/s]\n",
            "100% 1/1 [00:00<00:00, 283.97it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.386 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.290 |  Val. Acc: 72.84%\n",
            "100% 5/5 [00:00<00:00, 177.50it/s]\n",
            "100% 1/1 [00:00<00:00, 275.54it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.361 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.296 |  Val. Acc: 71.55%\n",
            "100% 5/5 [00:00<00:00, 181.86it/s]\n",
            "100% 1/1 [00:00<00:00, 282.37it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.345 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.296 |  Val. Acc: 71.98%\n",
            "100% 5/5 [00:00<00:00, 183.37it/s]\n",
            "100% 1/1 [00:00<00:00, 283.09it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.334 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.292 |  Val. Acc: 73.49%\n",
            "100% 5/5 [00:00<00:00, 174.45it/s]\n",
            "100% 1/1 [00:00<00:00, 287.10it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.318 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.276 |  Val. Acc: 74.57%\n",
            "100% 5/5 [00:00<00:00, 180.19it/s]\n",
            "100% 1/1 [00:00<00:00, 285.29it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.305 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.277 |  Val. Acc: 73.92%\n",
            "100% 5/5 [00:00<00:00, 172.17it/s]\n",
            "100% 1/1 [00:00<00:00, 286.03it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 74.57%\n",
            "100% 5/5 [00:00<00:00, 183.49it/s]\n",
            "100% 1/1 [00:00<00:00, 261.83it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.295 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.272 |  Val. Acc: 74.57%\n",
            "100% 5/5 [00:00<00:00, 178.50it/s]\n",
            "100% 1/1 [00:00<00:00, 281.63it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.270 |  Val. Acc: 74.78%\n",
            "100% 5/5 [00:00<00:00, 181.95it/s]\n",
            "100% 1/1 [00:00<00:00, 265.03it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.269 |  Val. Acc: 75.65%\n",
            "100% 5/5 [00:00<00:00, 182.02it/s]\n",
            "100% 1/1 [00:00<00:00, 275.29it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.276 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.264 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 182.84it/s]\n",
            "100% 1/1 [00:00<00:00, 275.20it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.276 |  Val. Acc: 73.92%\n",
            "100% 5/5 [00:00<00:00, 183.82it/s]\n",
            "100% 1/1 [00:00<00:00, 288.11it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.272 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.276 |  Val. Acc: 73.49%\n",
            "100% 5/5 [00:00<00:00, 180.15it/s]\n",
            "100% 1/1 [00:00<00:00, 276.32it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.257 |  Val. Acc: 75.86%\n",
            "100% 5/5 [00:00<00:00, 176.69it/s]\n",
            "100% 1/1 [00:00<00:00, 262.59it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.277 |  Val. Acc: 75.65%\n",
            "100% 5/5 [00:00<00:00, 178.15it/s]\n",
            "100% 1/1 [00:00<00:00, 236.21it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.261 |  Val. Acc: 75.86%\n",
            "100% 5/5 [00:00<00:00, 146.71it/s]\n",
            "100% 1/1 [00:00<00:00, 241.90it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.276 |  Val. Acc: 75.00%\n",
            "100% 5/5 [00:00<00:00, 176.45it/s]\n",
            "100% 1/1 [00:00<00:00, 273.23it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.273 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 168.50it/s]\n",
            "100% 1/1 [00:00<00:00, 188.03it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.286 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 166.65it/s]\n",
            "100% 1/1 [00:00<00:00, 269.16it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 161.47it/s]\n",
            "100% 1/1 [00:00<00:00, 271.46it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.281 |  Val. Acc: 74.35%\n",
            "100% 5/5 [00:00<00:00, 186.10it/s]\n",
            "100% 1/1 [00:00<00:00, 280.89it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.278 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 184.03it/s]\n",
            "100% 1/1 [00:00<00:00, 281.35it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.240 |  Val. Acc: 76.29%\n",
            "100% 5/5 [00:00<00:00, 162.95it/s]\n",
            "100% 1/1 [00:00<00:00, 281.86it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.277 |  Val. Acc: 75.65%\n",
            "100% 5/5 [00:00<00:00, 183.74it/s]\n",
            "100% 1/1 [00:00<00:00, 276.29it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.280 |  Val. Acc: 75.43%\n",
            "100% 5/5 [00:00<00:00, 168.90it/s]\n",
            "100% 1/1 [00:00<00:00, 281.80it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.266 |  Val. Acc: 76.08%\n",
            "100% 5/5 [00:00<00:00, 184.32it/s]\n",
            "100% 1/1 [00:00<00:00, 267.56it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.259 |  Val. Acc: 76.51%\n",
            "100% 5/5 [00:00<00:00, 184.27it/s]\n",
            "100% 1/1 [00:00<00:00, 276.50it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.287 |  Val. Acc: 76.08%\n",
            "100% 5/5 [00:00<00:00, 184.87it/s]\n",
            "100% 1/1 [00:00<00:00, 285.15it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.264 |  Val. Acc: 76.94%\n",
            "100% 5/5 [00:00<00:00, 185.03it/s]\n",
            "100% 1/1 [00:00<00:00, 275.58it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.291 |  Val. Acc: 76.72%\n",
            "100% 5/5 [00:00<00:00, 183.79it/s]\n",
            "100% 1/1 [00:00<00:00, 282.08it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.268 |  Val. Acc: 77.37%\n",
            "100% 5/5 [00:00<00:00, 172.81it/s]\n",
            "100% 1/1 [00:00<00:00, 282.50it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.274 |  Val. Acc: 77.37%\n",
            "100% 5/5 [00:00<00:00, 185.63it/s]\n",
            "100% 1/1 [00:00<00:00, 280.54it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.278 |  Val. Acc: 77.16%\n",
            "100% 5/5 [00:00<00:00, 184.92it/s]\n",
            "100% 1/1 [00:00<00:00, 259.12it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.280 |  Val. Acc: 77.16%\n",
            "100% 5/5 [00:00<00:00, 164.54it/s]\n",
            "100% 1/1 [00:00<00:00, 250.72it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.300 |  Val. Acc: 76.72%\n",
            "100% 5/5 [00:00<00:00, 157.16it/s]\n",
            "100% 1/1 [00:00<00:00, 255.45it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 77.59%\n",
            "100% 5/5 [00:00<00:00, 166.84it/s]\n",
            "100% 1/1 [00:00<00:00, 278.65it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.281 |  Val. Acc: 78.23%\n",
            "100% 5/5 [00:00<00:00, 175.89it/s]\n",
            "100% 1/1 [00:00<00:00, 281.78it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.268 |  Val. Acc: 79.09%\n",
            "100% 5/5 [00:00<00:00, 181.03it/s]\n",
            "100% 1/1 [00:00<00:00, 274.55it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.277 |  Val. Acc: 77.59%\n",
            "100% 5/5 [00:00<00:00, 176.92it/s]\n",
            "100% 1/1 [00:00<00:00, 280.54it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.291 |  Val. Acc: 77.59%\n",
            "100% 5/5 [00:00<00:00, 182.28it/s]\n",
            "100% 1/1 [00:00<00:00, 285.39it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.294 |  Val. Acc: 77.80%\n",
            "100% 5/5 [00:00<00:00, 182.41it/s]\n",
            "100% 1/1 [00:00<00:00, 282.06it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.297 |  Val. Acc: 76.29%\n",
            "100% 5/5 [00:00<00:00, 183.85it/s]\n",
            "100% 1/1 [00:00<00:00, 276.58it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.301 |  Val. Acc: 76.94%\n",
            "100% 5/5 [00:00<00:00, 181.53it/s]\n",
            "100% 1/1 [00:00<00:00, 281.52it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.264 |  Val. Acc: 77.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqx_D6V3X8ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSvjvGbeptr",
        "colab_type": "text"
      },
      "source": [
        "## Editor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4kQ9SRId1AT",
        "colab_type": "text"
      },
      "source": [
        "### RNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3klJdlraBoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd1ca4d9-54aa-4168-915b-0bc3fc5baec9"
      },
      "source": [
        "%%writefile model/RNNClassifiers.py\n",
        "\"\"\"\n",
        "Model Architectures of RNN based Classifiers\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from config.root import LOGGING_FORMAT, LOGGING_LEVEL\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "class LSTMWithPackPaddedSequences(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, bidirectional, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, embedded, text_lengths):\n",
        "\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        packed_output, (hidden, _) = self.rnn(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        return output, output_lengths, hidden\n",
        "\n",
        "\n",
        "class RNNHiddenClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This classifier concatenates the last hidden\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        n_layers,\n",
        "        bidirectional,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.rnn = LSTMWithPackPaddedSequences(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        output, output_lengths, hidden = self.rnn(embedded, text_lengths)\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model/RNNClassifiers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcWtered2k2",
        "colab_type": "text"
      },
      "source": [
        "### HelperFunctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6q0bvKPafpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcd463ad-58a0-43c1-b8d1-692d3d06b314"
      },
      "source": [
        "%%writefile helperfunctions.py\n",
        "\"\"\"\n",
        "Helper Functions containing training and evaluation methods\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from utility import categorical_accuracy, binary_accuracy\n",
        "from config.root import device\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths = batch.answer\n",
        "        max_len = text.shape[1]\n",
        "\n",
        "        predictions = model(text, text_lengths)\n",
        "\n",
        "        mask, key = get_mask_key_from_batch(batch, text, max_len, text_lengths)\n",
        "\n",
        "        loss = criterion(predictions, key, mask)\n",
        "\n",
        "        acc = binary_accuracy(predictions, predictions, mask)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def get_mask_key_from_batch(batch, text, max_len, text_lengths):\n",
        "    key, _ = batch.key\n",
        "\n",
        "    key = (\n",
        "        torch.from_numpy(np.where(np.isin(text.cpu().numpy(), key.cpu().numpy()), 1, 0))\n",
        "        .float()\n",
        "        .to(device)\n",
        "        .unsqueeze(2)\n",
        "    )\n",
        "\n",
        "    mask = (\n",
        "        (\n",
        "            torch.arange(max_len, device=device).expand(len(text_lengths), max_len)\n",
        "            < text_lengths.unsqueeze(1)\n",
        "        )\n",
        "        .float()\n",
        "        .unsqueeze(2)\n",
        "    )\n",
        "    return mask, key\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "            text, text_lengths = batch.answer\n",
        "            max_len = text.shape[1]\n",
        "            predictions = model(text, text_lengths)\n",
        "\n",
        "            mask, key = get_mask_key_from_batch(batch, text, max_len, text_lengths)\n",
        "\n",
        "            loss = criterion(predictions, key, mask)\n",
        "\n",
        "            acc = binary_accuracy(predictions, key, mask)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting helperfunctions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPAkY61QlMWk",
        "colab_type": "text"
      },
      "source": [
        "### DatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dl9TYFad6xg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee9d450f-aff5-4f24-d97b-0ef97299b20c"
      },
      "source": [
        "%%writefile datasetloader.py\n",
        "\"\"\"\n",
        "Load Dataset\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext import data, datasets\n",
        "\n",
        "from config.data import (\n",
        "    DATASET_FOLDER,\n",
        "    PROCESSED_DATASET,\n",
        "    PROCESSED_DATASET_FOLDER,\n",
        "    PROCESSED_DATASET_TRAIN_FILENAME,\n",
        "    PROCESSED_DATASET_TEST_FILENAME,\n",
        "    TEMP_DIR,\n",
        ")\n",
        "from config.hyperparameters import BATCH_SIZE, MAX_VOCAB\n",
        "from config.root import LOGGING_FORMAT, LOGGING_LEVEL, device\n",
        "from utility import tokenizer\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "class GrammarDasetAnswerKey:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.dataset_location = PROCESSED_DATASET\n",
        "\n",
        "        self.answer = data.Field(\n",
        "            tokenize=tokenizer, include_lengths=True, batch_first=True\n",
        "        )\n",
        "        self.key = data.Field(\n",
        "            tokenize=tokenizer, include_lengths=True, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fields = None\n",
        "        self.trainset = None\n",
        "        self.testset = None\n",
        "        self.train_iterator, self.test_iterator = None, None\n",
        "\n",
        "    @classmethod\n",
        "    def get_iterators(cls, batch_size):\n",
        "        \"\"\"\n",
        "        Load dataset and return iterators\n",
        "        \"\"\"\n",
        "        grammar_dataset = cls()\n",
        "\n",
        "        grammar_dataset.fields = [\n",
        "            (\"answer\", grammar_dataset.answer),\n",
        "            (\"key\", grammar_dataset.key),\n",
        "        ]\n",
        "\n",
        "        if not os.path.exists(PROCESSED_DATASET[\"train\"]) or not os.path.exists(\n",
        "            PROCESSED_DATASET[\"test\"]\n",
        "        ):\n",
        "            raise FileNotFoundError(\n",
        "                \"Please run the preprocessdata.py first by executing python preprocessdata.py\"\n",
        "            )\n",
        "\n",
        "        grammar_dataset.trainset, grammar_dataset.testset = data.TabularDataset.splits(\n",
        "            path=os.path.join(DATASET_FOLDER, PROCESSED_DATASET_FOLDER),\n",
        "            train=PROCESSED_DATASET_TRAIN_FILENAME,\n",
        "            test=PROCESSED_DATASET_TEST_FILENAME,\n",
        "            format=\"tsv\",\n",
        "            fields=grammar_dataset.fields,\n",
        "            skip_header=True,\n",
        "        )\n",
        "\n",
        "        logger.debug(\"Data Loaded Successfully!\")\n",
        "\n",
        "        grammar_dataset.answer.build_vocab(\n",
        "            grammar_dataset.trainset,\n",
        "            max_size=MAX_VOCAB,\n",
        "            vectors=\"glove.6B.300d\",\n",
        "            unk_init=torch.Tensor.normal_,\n",
        "        )\n",
        "\n",
        "        grammar_dataset.key.vocab = grammar_dataset.answer.vocab\n",
        "\n",
        "        logger.debug(\"Vocabulary Loaded\")\n",
        "\n",
        "        grammar_dataset.train_iterator, grammar_dataset.test_iterator = data.BucketIterator.splits(\n",
        "            (grammar_dataset.trainset, grammar_dataset.testset),\n",
        "            batch_size=batch_size,\n",
        "            sort_within_batch=True,\n",
        "            sort_key=lambda x: len(x.answer),\n",
        "            device=device,\n",
        "        )\n",
        "        logger.debug(\"Created Iterators\")\n",
        "\n",
        "        return grammar_dataset\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting datasetloader.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4hBHH4JM1EE",
        "colab_type": "text"
      },
      "source": [
        "### Train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ICmDogClReh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "586e65c5-2f81-4ff6-b609-5cee9c8aaf67"
      },
      "source": [
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Training script for the model\n",
        "\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from config.hyperparameters import (\n",
        "    BATCH_SIZE,\n",
        "    BIDIRECTION,\n",
        "    DROPOUT,\n",
        "    EMBEDDING_DIM,\n",
        "    EPOCHS,\n",
        "    FREEZE_EMBEDDINGS,\n",
        "    HIDDEN_DIM,\n",
        "    LR,\n",
        "    N_LAYERS,\n",
        "    WEIGHT_DECAY,\n",
        "    CNN_N_FILTER,\n",
        "    CNN_FILTER_SIZES,\n",
        "    LINEAR_HIDDEN_DIM,\n",
        ")\n",
        "from config.root import (\n",
        "    LOGGING_FORMAT,\n",
        "    LOGGING_LEVEL,\n",
        "    TRAINED_CLASSIFIER_FOLDER,\n",
        "    TRAINED_CLASSIFIER_RNNHIDDEN,\n",
        "    device,\n",
        "    seed_all,\n",
        "    SEED,\n",
        ")\n",
        "from datasetloader import GrammarDasetAnswerKey\n",
        "from helperfunctions import evaluate, train\n",
        "from model import RNNHiddenClassifier\n",
        "from utility import categorical_accuracy, epoch_time\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Method to count the number of parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def initialize_new_model(\n",
        "    classifier_type,\n",
        "    dataset,\n",
        "    embedding_dim,\n",
        "    hidden_dim,\n",
        "    n_layers,\n",
        "    bidirectional,\n",
        "    dropout,\n",
        "    freeze_embeddings,\n",
        "    linear_hidden_dim,\n",
        "):\n",
        "    \"\"\"Method to initialise new model, takes in dataset object and hyperparameters as parameter\"\"\"\n",
        "    logger.debug(\"Initializing Model\")\n",
        "    VOCAB_SIZE = len(dataset.answer.vocab)\n",
        "    PAD_IDX = dataset.answer.vocab.stoi[dataset.answer.pad_token]\n",
        "    pretrained_embeddings = dataset.answer.vocab.vectors\n",
        "    UNK_IDX = dataset.answer.vocab.stoi[dataset.answer.unk_token]\n",
        "    OUTPUT_LAYERS = 1\n",
        "    if classifier_type == \"RNNHiddenClassifier\":\n",
        "\n",
        "        model = RNNHiddenClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            OUTPUT_LAYERS,\n",
        "            n_layers,\n",
        "            bidirectional,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "    else:\n",
        "        raise TypeError(\"Invalid Classifier selected\")\n",
        "\n",
        "    if freeze_embeddings:\n",
        "        model.embedding.weight.requires_grad = False\n",
        "\n",
        "    logger.debug(\n",
        "        \"Freeze Embeddings Value {}: {}\".format(\n",
        "            freeze_embeddings, model.embedding.weight.requires_grad\n",
        "        )\n",
        "    )\n",
        "\n",
        "    logger.info(\n",
        "        \"Model Initialized with {:,} trainiable parameters\".format(\n",
        "            count_parameters(model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Initialize pretrained word embeddings\n",
        "\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "    # Initialize Padding and Unknown as 0\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
        "\n",
        "    logger.debug(\"Copied PreTrained Embeddings\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Utility to train the Model\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-s\",\n",
        "        \"--seed\",\n",
        "        default=SEED,\n",
        "        help=\"Set custom seed for reproducibility\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-loc\",\n",
        "        \"--model-location\",\n",
        "        default=None,\n",
        "        help=\"Give an already trained model location to use and train more epochs on it\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-b\",\n",
        "        \"--bidirectional\",\n",
        "        default=BIDIRECTION,\n",
        "        help=\"Makes the model Bidirectional\",\n",
        "        type=bool,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-d\",\n",
        "        \"--dropout\",\n",
        "        default=DROPOUT,\n",
        "        help=\"Dropout count for the model\",\n",
        "        type=float,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-e\",\n",
        "        \"--embedding-dim\",\n",
        "        default=EMBEDDING_DIM,\n",
        "        help=\"Embedding Dimensions\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-hd\",\n",
        "        \"--hidden-dim\",\n",
        "        default=HIDDEN_DIM,\n",
        "        help=\"Hidden dimensions of the RNN\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-l\", \"--n-layers\", default=N_LAYERS, help=\"Number of layers in RNN\", type=int\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-lr\",\n",
        "        \"--learning-rate\",\n",
        "        default=LR,\n",
        "        help=\"Learning rate of Adam Optimizer\",\n",
        "        type=float,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-n\",\n",
        "        \"--epochs\",\n",
        "        default=EPOCHS,\n",
        "        help=\"Number of Epochs to train model\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-batch\",\n",
        "        \"--batch_size\",\n",
        "        default=BATCH_SIZE,\n",
        "        help=\"Number of Epochs to train model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--freeze-embeddings\",\n",
        "        default=FREEZE_EMBEDDINGS,\n",
        "        help=\"Freeze Embeddings of Model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-l2\",\n",
        "        \"--l2-regularization\",\n",
        "        default=WEIGHT_DECAY,\n",
        "        help=\"Value of alpha in l2 regularization 0 means no regularization \",\n",
        "        type=float,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-m\",\n",
        "        \"--model\",\n",
        "        default=\"RNNHiddenClassifier\",\n",
        "        choices=[\"RNNHiddenClassifier\"],\n",
        "        help=\"select the classifier to train on\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-lhd\",\n",
        "        \"--linear-hidden-dim\",\n",
        "        default=LINEAR_HIDDEN_DIM,\n",
        "        help=\"Freeze Embeddings of Model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    seed_all(args.seed)\n",
        "    logger.debug(args)\n",
        "    logger.debug(\"Custom seed set with: {}\".format(args.seed))\n",
        "\n",
        "    logger.info(\"Loading Dataset\")\n",
        "\n",
        "    dataset = GrammarDasetAnswerKey.get_iterators(args.batch_size)\n",
        "\n",
        "    logger.info(\"Dataset Loaded Successfully\")\n",
        "\n",
        "    if args.model_location:\n",
        "        model = torch.load(args.model_location)\n",
        "    else:\n",
        "        model = initialize_new_model(\n",
        "            args.model,\n",
        "            dataset,\n",
        "            args.embedding_dim,\n",
        "            args.hidden_dim,\n",
        "            args.n_layers,\n",
        "            args.bidirectional,\n",
        "            args.dropout,\n",
        "            args.freeze_embeddings,\n",
        "            args.linear_hidden_dim,\n",
        "        )\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LR, weight_decay=args.l2_regularization\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    logger.info(model)\n",
        "\n",
        "    if not os.path.exists(TRAINED_CLASSIFIER_FOLDER):\n",
        "        os.mkdir(TRAINED_CLASSIFIER_FOLDER)\n",
        "\n",
        "    best_test_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(int(args.epochs)):\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train(\n",
        "            model, dataset.train_iterator, optimizer, criterion\n",
        "        )\n",
        "        test_loss, test_acc = evaluate(model, dataset.test_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if test_loss < best_test_loss:\n",
        "            best_test_loss = test_loss\n",
        "            torch.save(\n",
        "                model,\n",
        "                os.path.join(TRAINED_CLASSIFIER_FOLDER, TRAINED_CLASSIFIER_RNNHIDDEN),\n",
        "            )\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
        "        print(f\"\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zTD8wSAST5f",
        "colab_type": "text"
      },
      "source": [
        "### Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ERtNvsSVap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcd5db2e-aa08-4401-8ccd-2462f301ad4c"
      },
      "source": [
        "%%writefile utility.py\n",
        "\"\"\"\n",
        "Utility methods to be used for training and dataloading purposes\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "\n",
        "def isin(ar1, ar2):\n",
        "    \"\"\"\n",
        "    Takes two torch tensors as input and returns \n",
        "    \"\"\"\n",
        "    return (ar1[..., None] == ar2).any(-1)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    results = []\n",
        "    text = text.replace(\"<blank>\", \" BLANKK \")\n",
        "    text = text.replace(\"<slash>\", \" SLASHH \")\n",
        "    text = text.replace(\"<Q>\", \" QSTARTIT \")\n",
        "    text = text.replace(\"</Q>\", \" QENDIT \")\n",
        "    text = text.replace(\"<K>\", \" KSTARTIT \")\n",
        "    text = text.replace(\"</K>\", \" KENDIT \")\n",
        "    text = text.replace(\"<A>\", \" ASTARTIT \")\n",
        "    text = text.replace(\"</A>\", \" AENDIT \")\n",
        "    for token in nlp(text):\n",
        "        if token.text == \"BLANKK\":\n",
        "            results.append(\"<blank>\")\n",
        "        elif token.text == \"SLASHH\":\n",
        "            results.append(\"<slash>\")\n",
        "        elif token.text == \"QSTARTIT\":\n",
        "            results.append(\"<Q>\")\n",
        "        elif token.text == \"QENDIT\":\n",
        "            results.append(\"</Q>\")\n",
        "        elif token.text == \"KSTARTIT\":\n",
        "            results.append(\"<K>\")\n",
        "        elif token.text == \"KENDIT\":\n",
        "            results.append(\"</K>\")\n",
        "        elif token.text == \"ASTARTIT\":\n",
        "            results.append(\"<A>\")\n",
        "        elif token.text == \"AENDIT\":\n",
        "            results.append(\"</A>\")\n",
        "        else:\n",
        "            results.append(token.text)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim=1, keepdim=True)\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y, mask):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = ((rounded_preds == y) * mask).float()  # convert into float for division\n",
        "    acc = correct.sum() / mask.sum()\n",
        "    return acc\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting utility.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfk5QnwTf6u-",
        "colab_type": "text"
      },
      "source": [
        "# Testing ground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8wBEnsagEEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpH2Ct7OgDYG",
        "colab": {}
      },
      "source": [
        "a = torch.Tensor([1,2 ,3, 4, 5])\n",
        "b = torch.Tensor([2, 4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxcL2W2DgAMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a778b3e-5d00-41dc-d9cf-f75fb23617e5"
      },
      "source": [
        "(a[..., None] == b).any(-1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False,  True, False,  True, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKaxuuNTgFnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.ones([4, 10])\n",
        "b = torch.zeros([4, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzwb55rol9iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a[0][4] = 3\n",
        "a[0][7] = 5\n",
        "b[0][0] = 3\n",
        "b[0][1] = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEUQzW4kmV0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97fa9bcb-8fc2-4ec8-e7e1-ca85cc41cf25"
      },
      "source": [
        "a[0], b[0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 3., 5., 1., 1., 1., 1.]), tensor([3., 5.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4SRt0zjjwlS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "46be7bf0-2e0f-46f8-9605-cfb445f50274"
      },
      "source": [
        "test1 = torch.tensor([1, 2, 3])\n",
        "test2 = torch.tensor([2])\n",
        "isin(test1 , test2)\n",
        "(test1[... , None] == test2)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [2],\n",
              "        [3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omq0-Cfxl1cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c279595-fbaf-4453-9002-0f0e6dbeb6bb"
      },
      "source": [
        "a[0], b[0]"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 3., 5., 1., 5., 1., 1.]), tensor([3., 5.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJjB7pV-izz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.isin(a.numpy(), b.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC-RYosjgw3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "601d8231-a98d-499c-924f-c12497699543"
      },
      "source": [
        "np.where(x, 1, 0)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiAiLWSyn3UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}