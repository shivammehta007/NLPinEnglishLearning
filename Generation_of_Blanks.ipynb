{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generation of Blanks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "18wNtFnmO3y2d3nSyqb0nTr02PtpTUBS3",
      "authorship_tag": "ABX9TyMS1qrr/byCM9EbR+OJRVvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/Generation_of_Blanks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU4dAfBwhK3b",
        "colab_type": "text"
      },
      "source": [
        "# Fill in the blank Generator With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpdPjQ5jSn_",
        "colab_type": "text"
      },
      "source": [
        "## Download Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDrIopZhIaO",
        "colab_type": "code",
        "outputId": "6e839f46-7aa8-47df-ccc8-e33e936a9410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!rm -rf QuestionGenerator\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import subprocess\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone --single-branch --branch master  https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, 'QuestionGenerator')\n",
        "stdout = subprocess.Popen(cmd_string, shell=True, stdout=subprocess.PIPE)\n",
        "print(stdout.communicate()[0])\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: shivammehta007\n",
            "Password: ··········\n",
            "b''\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7g4AhObih2-",
        "colab_type": "code",
        "outputId": "34beb92f-b249-45bb-b092-838366d25542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -rf QuestionGenerator/FromScratch\n",
        "!rm -rf QuestionGenerator/classifier\n",
        "%cd QuestionGenerator/FITBGenerator/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/FITBGenerator/QuestionGenerator/FITBGenerator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcDcDeTylq76",
        "colab_type": "code",
        "outputId": "08e9410b-7ece-4f9d-f386-16d878a799ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/  datasetloader.py    lossfunction.py  preprocessdata.py  utility.py\n",
            "\u001b[01;34mdata\u001b[0m/    helperfunctions.py  \u001b[01;34mmodel\u001b[0m/           train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "f85d9506-a109-46fe-e96a-556aee1e208e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2852  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "3c879544-0db0-474a-f3f8-f0feb3e7ffd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/QuestionGenerator/FITBGenerator/QuestionGenerator/FITBGenerator/Seq2Seq\n",
            " 98% 378M/386M [00:10<00:00, 30.5MB/s]\n",
            "100% 386M/386M [00:10<00:00, 38.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "63eea22e-aa95-40e4-c629-706eaa63e57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3GrnD7jOpx",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSBYYV6qHXFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python preprocessdata.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ4_Jw_lp3x",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6sicYFjuV3",
        "colab_type": "code",
        "outputId": "8946aa1f-8cda-4ba6-f91c-2447264d21ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50 --n-layers 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:221 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234)\n",
            "[DEBUG | train.py:222 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:224 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:74 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:329 -                cache() ] Loading vectors from glove.6B.300d.txt\n",
            "100% 399789/400000 [00:52<00:00, 7514.86it/s][INFO | vocab.py:381 -                cache() ] Saving vectors to .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:85 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:94 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:228 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:66 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:92 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:98 - initialize_new_model() ] Model Initialized with 440,577 trainiable parameters\n",
            "[DEBUG | train.py:110 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:253 -             <module>() ] RNNHiddenClassifier(\n",
            "  (embedding): Embedding(564, 300, padding_idx=1)\n",
            "  (rnn): LSTMWithPackPaddedSequences(\n",
            "    (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "100% 5/5 [00:00<00:00, 11.05it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 209.68it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.677 | Train Acc: 59.77%\n",
            "\t Val. Loss: 0.683 |  Val. Acc: 51.51%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 124.84it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 199.63it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.640 | Train Acc: 65.16%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 52.80%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 122.74it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 207.63it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.611 | Train Acc: 67.11%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 57.11%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 94.31it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 211.69it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.579 | Train Acc: 71.35%\n",
            "\t Val. Loss: 0.669 |  Val. Acc: 62.93%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 120.86it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 215.20it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.546 | Train Acc: 76.55%\n",
            "\t Val. Loss: 0.670 |  Val. Acc: 66.16%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 123.84it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 211.62it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.513 | Train Acc: 80.54%\n",
            "\t Val. Loss: 0.648 |  Val. Acc: 71.12%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 111.08it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 212.63it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.487 | Train Acc: 81.56%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 72.84%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 120.84it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 219.60it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.473 | Train Acc: 82.34%\n",
            "\t Val. Loss: 0.647 |  Val. Acc: 72.41%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 126.46it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 207.32it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.454 | Train Acc: 82.23%\n",
            "\t Val. Loss: 0.650 |  Val. Acc: 72.41%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 123.94it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 218.46it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.451 | Train Acc: 82.59%\n",
            "\t Val. Loss: 0.644 |  Val. Acc: 73.49%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 115.81it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 225.23it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Acc: 83.97%\n",
            "\t Val. Loss: 0.621 |  Val. Acc: 74.57%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 123.09it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 218.03it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.415 | Train Acc: 84.92%\n",
            "\t Val. Loss: 0.623 |  Val. Acc: 74.14%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 112.55it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 206.90it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.420 | Train Acc: 84.02%\n",
            "\t Val. Loss: 0.625 |  Val. Acc: 73.28%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 123.80it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 207.37it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.398 | Train Acc: 85.59%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 73.92%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 109.47it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 206.59it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.399 | Train Acc: 85.42%\n",
            "\t Val. Loss: 0.611 |  Val. Acc: 75.22%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 120.38it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 204.92it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.387 | Train Acc: 86.38%\n",
            "\t Val. Loss: 0.612 |  Val. Acc: 75.86%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.91it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 222.39it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.373 | Train Acc: 87.11%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 75.65%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 103.93it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 225.34it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.376 | Train Acc: 86.47%\n",
            "\t Val. Loss: 0.609 |  Val. Acc: 75.22%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 126.89it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 219.45it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.378 | Train Acc: 86.06%\n",
            "\t Val. Loss: 0.609 |  Val. Acc: 74.78%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 122.10it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 172.82it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train Acc: 85.51%\n",
            "\t Val. Loss: 0.594 |  Val. Acc: 75.43%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 110.69it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 178.10it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.372 | Train Acc: 86.52%\n",
            "\t Val. Loss: 0.619 |  Val. Acc: 74.35%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 110.44it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 204.85it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.361 | Train Acc: 86.99%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 76.51%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 122.01it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 218.37it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.354 | Train Acc: 87.83%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 75.43%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.62it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 211.32it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.348 | Train Acc: 87.59%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.22%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 129.28it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 218.41it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.348 | Train Acc: 88.11%\n",
            "\t Val. Loss: 0.621 |  Val. Acc: 74.78%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 124.06it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 230.89it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.350 | Train Acc: 87.67%\n",
            "\t Val. Loss: 0.594 |  Val. Acc: 75.65%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 123.68it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 232.23it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.335 | Train Acc: 88.90%\n",
            "\t Val. Loss: 0.611 |  Val. Acc: 74.78%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 124.43it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 187.30it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.351 | Train Acc: 87.38%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.00%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 128.31it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 213.93it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.337 | Train Acc: 88.19%\n",
            "\t Val. Loss: 0.574 |  Val. Acc: 76.94%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 114.19it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 233.59it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.337 | Train Acc: 88.42%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.86%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 127.64it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 225.68it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.329 | Train Acc: 88.83%\n",
            "\t Val. Loss: 0.598 |  Val. Acc: 75.86%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 128.04it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 208.24it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.332 | Train Acc: 88.32%\n",
            "\t Val. Loss: 0.589 |  Val. Acc: 76.29%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 126.85it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 222.21it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.324 | Train Acc: 89.07%\n",
            "\t Val. Loss: 0.592 |  Val. Acc: 75.86%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.47it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 223.80it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train Acc: 89.46%\n",
            "\t Val. Loss: 0.616 |  Val. Acc: 75.22%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 111.70it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 222.64it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.314 | Train Acc: 89.70%\n",
            "\t Val. Loss: 0.585 |  Val. Acc: 77.59%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 126.02it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 204.51it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.326 | Train Acc: 89.39%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 75.86%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.20it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 219.13it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train Acc: 89.06%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 76.94%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 127.86it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 204.25it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train Acc: 89.77%\n",
            "\t Val. Loss: 0.583 |  Val. Acc: 77.16%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 108.20it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 220.50it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.312 | Train Acc: 90.00%\n",
            "\t Val. Loss: 0.603 |  Val. Acc: 76.72%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 126.11it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 214.00it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.315 | Train Acc: 89.05%\n",
            "\t Val. Loss: 0.603 |  Val. Acc: 76.94%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 107.61it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 217.50it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.299 | Train Acc: 90.74%\n",
            "\t Val. Loss: 0.596 |  Val. Acc: 76.72%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.66it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 229.79it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.305 | Train Acc: 89.44%\n",
            "\t Val. Loss: 0.592 |  Val. Acc: 77.59%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 112.50it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 133.95it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train Acc: 90.19%\n",
            "\t Val. Loss: 0.575 |  Val. Acc: 78.45%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.28it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 224.71it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.308 | Train Acc: 89.95%\n",
            "\t Val. Loss: 0.577 |  Val. Acc: 78.66%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 116.56it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 224.90it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.299 | Train Acc: 90.68%\n",
            "\t Val. Loss: 0.586 |  Val. Acc: 77.59%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.57it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 208.28it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train Acc: 89.52%\n",
            "\t Val. Loss: 0.608 |  Val. Acc: 77.80%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.51it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 193.80it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train Acc: 90.01%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 76.94%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.85it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 222.73it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train Acc: 90.21%\n",
            "\t Val. Loss: 0.599 |  Val. Acc: 76.29%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.54it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 216.21it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.301 | Train Acc: 90.37%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 76.51%\n",
            "\n",
            "100% 5/5 [00:00<00:00, 125.62it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 212.52it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train Acc: 90.30%\n",
            "\t Val. Loss: 0.556 |  Val. Acc: 77.80%\n",
            "100% 399789/400000 [01:07<00:00, 5920.48it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXHK1gyh2ok9",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq To generate Blanks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7EmnZGfKhiO",
        "colab_type": "code",
        "outputId": "8a980b68-112e-4645-ae68-34bcd4fdcc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/QuestionGenerator/FITBGenerator'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnoJOcu07JrL",
        "colab_type": "code",
        "outputId": "1114e060-7f13-4f23-ede0-79cf8302f471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%mkdir Seq2Seq\n",
        "%cd Seq2Seq\n",
        "%mkdir data\n",
        "%mkdir data/processed"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/FITBGenerator/QuestionGenerator/FITBGenerator/Seq2Seq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqx_D6V3X8ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9BWyC9M3GQR",
        "colab_type": "text"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaTyUwIx7WVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iN9xR7x2vjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = pd.read_csv(\"../data/processed/train.tsv\", sep='\\t')\n",
        "dataset_test = pd.read_csv(\"../data/processed/test.tsv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcxSXh9BdUL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4GDFmBpdSnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_len(x):\n",
        "    return len([t.text for t in nlp(x)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Lxihbzc2H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_len = dataset_train['feature'].apply(get_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1jE0hsMdumj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_len = dataset_train[\"key\"].apply(get_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrnEDStDdilP",
        "colab_type": "code",
        "outputId": "2fcea9f1-d553-43fa-8be8-5ea8bb12d9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence_len.mean()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.90311418685121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc-kqqBWdyOy",
        "colab_type": "code",
        "outputId": "0bf9ed4e-388a-4d33-e593-7cc712e8d5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "key_len.mean()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3079584775086506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYHspG_87VXs",
        "colab_type": "code",
        "outputId": "21381806-60b0-4d22-fe77-8bbd47ff2a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_train.shape, dataset_test.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((289, 2), (51, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4wd0RmHFclk",
        "colab_type": "code",
        "outputId": "9d7097b7-2e98-4527-d49a-8efc34d4c913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset_train.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He is in the garden.</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have given some money to Julia.</td>\n",
              "      <td>given</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>He is early.</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You will have been learning about computers fo...</td>\n",
              "      <td>will have been learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We weren't tired when we arrived.</td>\n",
              "      <td>were n't</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             feature                      key\n",
              "0                               He is in the garden.                       is\n",
              "1                  I have given some money to Julia.                    given\n",
              "2                                       He is early.                       is\n",
              "3  You will have been learning about computers fo...  will have been learning\n",
              "4                  We weren't tired when we arrived.                 were n't"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhR2XxsW7m9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train, dataset_valid = train_test_split(dataset_train, random_state=1234, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6wf39iL-S1U",
        "colab_type": "code",
        "outputId": "ff0e0e7b-7598-4467-e1e1-9e527a4d0bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_train.shape, dataset_valid.shape, dataset_test.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((260, 2), (29, 2), (51, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQmIlGVFBTlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write(dataframe, dataset_name, folder=\"data/processed\"):\n",
        "    feature = dataframe[\"feature\"]\n",
        "    key = dataframe[\"key\"]\n",
        "    feature.to_csv('{}/{}.feature'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
        "    key.to_csv('{}/{}.key'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwVZ4xy6G4np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write(dataset_train, \"train\")\n",
        "write(dataset_valid, \"valid\")\n",
        "write(dataset_test, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHNqVnpLOj4e",
        "colab_type": "text"
      },
      "source": [
        "## FairSeq Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAE2RUwpHKnN",
        "colab_type": "code",
        "outputId": "43948804-0618-4bde-96f3-eac634e6176b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!fairseq-preprocess --source-lang feature --target-lang key \\\n",
        "     --trainpref data/processed/train --testpref data/processed/test --validpref data/processed/valid\\\n",
        "     --destdir preprocessed_data --seed 1234 --nwordssrc 5000 --nwordstgt 5000"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='preprocessed_data', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=5000, nwordstgt=5000, only_source=False, optimizer='nag', padding_factor=8, seed=1234, source_lang='feature', srcdict=None, target_lang='key', task='translation', tensorboard_logdir='', testpref='data/processed/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='data/processed/train', user_dir=None, validpref='data/processed/valid', workers=1)\n",
            "| [feature] Dictionary: 639 types\n",
            "| [feature] data/processed/train.feature: 260 sents, 2284 tokens, 0.0% replaced by <unk>\n",
            "| [feature] Dictionary: 639 types\n",
            "| [feature] data/processed/valid.feature: 29 sents, 224 tokens, 17.0% replaced by <unk>\n",
            "| [feature] Dictionary: 639 types\n",
            "| [feature] data/processed/test.feature: 51 sents, 444 tokens, 17.8% replaced by <unk>\n",
            "| [key] Dictionary: 159 types\n",
            "| [key] data/processed/train.key: 260 sents, 862 tokens, 0.0% replaced by <unk>\n",
            "| [key] Dictionary: 159 types\n",
            "| [key] data/processed/valid.key: 29 sents, 91 tokens, 11.0% replaced by <unk>\n",
            "| [key] Dictionary: 159 types\n",
            "| [key] data/processed/test.key: 51 sents, 172 tokens, 9.3% replaced by <unk>\n",
            "| Wrote preprocessed data to preprocessed_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZjzhRS6OmVk",
        "colab_type": "text"
      },
      "source": [
        "## FairSeq Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfXrTjq-QJRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip ../.vector_cache/glove6b300dtxt.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yEP3sKWbsz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9DBmaQYIG6B",
        "colab_type": "code",
        "outputId": "cb6af97e-cf45-473e-ef0a-e425f3f146fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/  --clip-norm 5 --batch-size 64 \\\n",
        "                      --save-dir checkpoints/lstm --arch lstm --max-epoch 15 --encoder-hidden-size 258 \\\n",
        "                      --encoder-layers 2  --decoder-hidden-size 258 --decoder-layers 2 --optimizer adam --lr 0.001  \\\n",
        "                      --dropout 0.3 --encoder-embed-path glove.6B.300d.txt --encoder-bidirectional --encoder-embed-dim 300 \\\n",
        "                      --decoder-embed-dim 300 --no-epoch-checkpoints --decoder-embed-path glove.6B.300d.txt --decoder-out-embed-dim 300 \\\n",
        "                      --num-workers 3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', arch='lstm', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=5.0, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, decoder_embed_dim=300, decoder_embed_path='glove.6B.300d.txt', decoder_freeze_embed=False, decoder_hidden_size=258, decoder_layers=2, decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_bidirectional=True, encoder_dropout_in=0.3, encoder_dropout_out=0.3, encoder_embed_dim=300, encoder_embed_path='glove.6B.300d.txt', encoder_freeze_embed=False, encoder_hidden_size=258, encoder_layers=2, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/lstm', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [feature] dictionary: 640 types\n",
            "| [key] dictionary: 160 types\n",
            "| loaded 29 examples from: preprocessed_data/valid.feature-key.feature\n",
            "| loaded 29 examples from: preprocessed_data/valid.feature-key.key\n",
            "| preprocessed_data/ valid feature-key 29 examples\n",
            "| Found 358/640 types in embedding file.\n",
            "| Found 143/160 types in embedding file.\n",
            "LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (embed_tokens): Embedding(640, 300, padding_idx=1)\n",
            "    (lstm): LSTM(300, 258, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (embed_tokens): Embedding(160, 300, padding_idx=1)\n",
            "    (encoder_hidden_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (encoder_cell_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(558, 258)\n",
            "      (1): LSTMCell(258, 258)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=258, out_features=516, bias=False)\n",
            "      (output_proj): Linear(in_features=774, out_features=258, bias=False)\n",
            "    )\n",
            "    (additional_fc): Linear(in_features=258, out_features=300, bias=True)\n",
            "    (fc_out): Linear(in_features=300, out_features=160, bias=True)\n",
            "  )\n",
            ")\n",
            "| model lstm, criterion CrossEntropyCriterion\n",
            "| num. model params: 5101708 (num. trained: 5101708)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| no existing checkpoint found checkpoints/lstm/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 260 examples from: preprocessed_data/train.feature-key.feature\n",
            "| loaded 260 examples from: preprocessed_data/train.feature-key.key\n",
            "| preprocessed_data/ train feature-key 260 examples\n",
            "| epoch 001:  20% 1/5 [00:00<00:00,  4.69it/s, loss=7.276, ppl=154.94, wps=1026, ups=5, wpb=226.000, bsz=64.000, num_updates=1, lr=0.001, gnorm=1.055, clip=0.000, oom=0.000, wall=0, train_wall=0]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [11] is 11 which does not match the computed number of elements 20. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (20,).\n",
            "| epoch 001 | loss 6.178 | ppl 72.42 | wps 2184 | ups 14 | wpb 172.400 | bsz 52.000 | num_updates 5 | lr 0.001 | gnorm 2.505 | clip 0.200 | oom 0.000 | wall 1 | train_wall 0\n",
            "| epoch 001 | valid on 'valid' subset | loss 5.256 | ppl 38.2 | num_updates 5\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 1 @ 5 updates) (writing took 0.1761770248413086 seconds)\n",
            "| epoch 002 | loss 5.009 | ppl 32.2 | wps 2267 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 10 | lr 0.001 | gnorm 2.929 | clip 0.000 | oom 0.000 | wall 1 | train_wall 1\n",
            "| epoch 002 | valid on 'valid' subset | loss 5.133 | ppl 35.08 | num_updates 10 | best_loss 5.13274\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 2 @ 10 updates) (writing took 0.286052942276001 seconds)\n",
            "| epoch 003 | loss 4.611 | ppl 24.43 | wps 2286 | ups 13 | wpb 172.400 | bsz 52.000 | num_updates 15 | lr 0.001 | gnorm 2.429 | clip 0.000 | oom 0.000 | wall 2 | train_wall 1\n",
            "| epoch 003 | valid on 'valid' subset | loss 4.886 | ppl 29.57 | num_updates 15 | best_loss 4.88596\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 3 @ 15 updates) (writing took 0.18532800674438477 seconds)\n",
            "| epoch 004 | loss 4.265 | ppl 19.23 | wps 2848 | ups 13 | wpb 172.400 | bsz 52.000 | num_updates 20 | lr 0.001 | gnorm 2.444 | clip 0.000 | oom 0.000 | wall 3 | train_wall 1\n",
            "| epoch 004 | valid on 'valid' subset | loss 4.527 | ppl 23.06 | num_updates 20 | best_loss 4.52727\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 4 @ 20 updates) (writing took 2.7541706562042236 seconds)\n",
            "| epoch 005 | loss 3.951 | ppl 15.47 | wps 2230 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 25 | lr 0.001 | gnorm 2.112 | clip 0.000 | oom 0.000 | wall 6 | train_wall 2\n",
            "| epoch 005 | valid on 'valid' subset | loss 4.392 | ppl 21 | num_updates 25 | best_loss 4.39203\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 5 @ 25 updates) (writing took 0.1873486042022705 seconds)\n",
            "| epoch 006 | loss 3.683 | ppl 12.85 | wps 2119 | ups 12 | wpb 172.400 | bsz 52.000 | num_updates 30 | lr 0.001 | gnorm 1.987 | clip 0.000 | oom 0.000 | wall 7 | train_wall 2\n",
            "| epoch 006 | valid on 'valid' subset | loss 4.079 | ppl 16.9 | num_updates 30 | best_loss 4.07887\n",
            "| epoch 007 | loss 3.662 | ppl 12.66 | wps 2810 | ups 13 | wpb 172.400 | bsz 52.000 | num_updates 35 | lr 0.001 | gnorm 2.296 | clip 0.000 | oom 0.000 | wall 10 | train_wall 2\n",
            "| epoch 007 | valid on 'valid' subset | loss 4.096 | ppl 17.1 | num_updates 35 | best_loss 4.07887\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 7 @ 35 updates) (writing took 0.17694807052612305 seconds)\n",
            "| epoch 008 | loss 3.426 | ppl 10.75 | wps 2277 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 40 | lr 0.001 | gnorm 1.892 | clip 0.000 | oom 0.000 | wall 11 | train_wall 2\n",
            "| epoch 008 | valid on 'valid' subset | loss 4.094 | ppl 17.08 | num_updates 40 | best_loss 4.07887\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 8 @ 40 updates) (writing took 1.144531488418579 seconds)\n",
            "| epoch 009 | loss 3.222 | ppl 9.33 | wps 2300 | ups 14 | wpb 172.400 | bsz 52.000 | num_updates 45 | lr 0.001 | gnorm 1.805 | clip 0.000 | oom 0.000 | wall 13 | train_wall 3\n",
            "| epoch 009 | valid on 'valid' subset | loss 3.932 | ppl 15.27 | num_updates 45 | best_loss 3.93233\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 9 @ 45 updates) (writing took 1.147951602935791 seconds)\n",
            "| epoch 010 | loss 2.922 | ppl 7.58 | wps 2302 | ups 14 | wpb 172.400 | bsz 52.000 | num_updates 50 | lr 0.001 | gnorm 1.539 | clip 0.000 | oom 0.000 | wall 15 | train_wall 3\n",
            "| epoch 010 | valid on 'valid' subset | loss 3.764 | ppl 13.58 | num_updates 50 | best_loss 3.76364\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 10 @ 50 updates) (writing took 1.7931735515594482 seconds)\n",
            "| epoch 011 | loss 2.864 | ppl 7.28 | wps 2310 | ups 14 | wpb 172.400 | bsz 52.000 | num_updates 55 | lr 0.001 | gnorm 1.907 | clip 0.000 | oom 0.000 | wall 17 | train_wall 3\n",
            "| epoch 011 | valid on 'valid' subset | loss 4.014 | ppl 16.16 | num_updates 55 | best_loss 3.76364\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 11 @ 55 updates) (writing took 1.1388449668884277 seconds)\n",
            "| epoch 012 | loss 2.690 | ppl 6.45 | wps 2464 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 60 | lr 0.001 | gnorm 1.794 | clip 0.000 | oom 0.000 | wall 19 | train_wall 4\n",
            "| epoch 012 | valid on 'valid' subset | loss 3.614 | ppl 12.24 | num_updates 60 | best_loss 3.61401\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 12 @ 60 updates) (writing took 0.3047308921813965 seconds)\n",
            "| epoch 013 | loss 2.471 | ppl 5.54 | wps 2513 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 65 | lr 0.001 | gnorm 1.602 | clip 0.000 | oom 0.000 | wall 19 | train_wall 4\n",
            "| epoch 013 | valid on 'valid' subset | loss 3.611 | ppl 12.22 | num_updates 65 | best_loss 3.61067\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 13 @ 65 updates) (writing took 0.1859426498413086 seconds)\n",
            "| epoch 014 | loss 2.298 | ppl 4.92 | wps 2607 | ups 16 | wpb 172.400 | bsz 52.000 | num_updates 70 | lr 0.001 | gnorm 1.326 | clip 0.000 | oom 0.000 | wall 20 | train_wall 4\n",
            "| epoch 014 | valid on 'valid' subset | loss 3.475 | ppl 11.12 | num_updates 70 | best_loss 3.47482\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 14 @ 70 updates) (writing took 2.751106023788452 seconds)\n",
            "| epoch 015 | loss 2.281 | ppl 4.86 | wps 2549 | ups 15 | wpb 172.400 | bsz 52.000 | num_updates 75 | lr 0.001 | gnorm 1.535 | clip 0.000 | oom 0.000 | wall 23 | train_wall 4\n",
            "| epoch 015 | valid on 'valid' subset | loss 3.740 | ppl 13.37 | num_updates 75 | best_loss 3.47482\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 15 @ 75 updates) (writing took 1.142934799194336 seconds)\n",
            "| done training in 24.7 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-kUZSqxOXtu",
        "colab_type": "code",
        "outputId": "50e99a11-7ec9-46c0-dd50-30b83d6aab49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/lstm/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > lstm_last.out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3HtuN6DO8BA",
        "colab_type": "code",
        "outputId": "f2a2a4ea-f78f-4fc0-c4b8-1a46abebfebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!grep ^H lstm_last.out | cut -f3- > lstm_last.out.sys\n",
        "!grep ^T lstm_last.out | cut -f2- > lstm_last.out.ref\n",
        "!fairseq-score --sys lstm_last.out.sys --ref lstm_last.out.ref --ignore-case"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='lstm_last.out.ref', sacrebleu=False, sentence_bleu=False, sys='lstm_last.out.sys')\n",
            "BLEU4 = 0.00, 43.5/23.8/18.2/0.0 (BP=1.000, ratio=1.083, syslen=131, reflen=121)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBlDQxWnZU2w",
        "colab_type": "code",
        "outputId": "3d0678db-1d9c-4b7f-b988-649e2a8d34b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!fairseq-score -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: fairseq-score [-h] [-s SYS] -r REF [-o N] [--ignore-case] [--sacrebleu]\n",
            "                     [--sentence-bleu]\n",
            "\n",
            "Command-line script for BLEU scoring.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help         show this help message and exit\n",
            "  -s SYS, --sys SYS  system output\n",
            "  -r REF, --ref REF  references\n",
            "  -o N, --order N    consider ngrams up to this order\n",
            "  --ignore-case      case-insensitive scoring\n",
            "  --sacrebleu        score with sacrebleu\n",
            "  --sentence-bleu    report sentence-level BLEUs (i.e., with +1 smoothing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfvfTZWAQBSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a5e1a23-8901-4bfe-c5bf-48c54f7aca0c"
      },
      "source": [
        "!fairseq-train --help"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval N]\n",
            "                     [--log-format {json,none,simple,tqdm}]\n",
            "                     [--tensorboard-logdir DIR] [--seed N] [--cpu] [--fp16]\n",
            "                     [--memory-efficient-fp16]\n",
            "                     [--fp16-init-scale FP16_INIT_SCALE]\n",
            "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
            "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
            "                     [--min-loss-scale D]\n",
            "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
            "                     [--user-dir USER_DIR]\n",
            "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
            "                     [--criterion {cross_entropy,adaptive_loss,label_smoothed_cross_entropy,sentence_prediction,legacy_masked_lm_loss,composite_loss,masked_lm,nat_loss,sentence_ranking,binary_cross_entropy,label_smoothed_cross_entropy_with_alignment}]\n",
            "                     [--tokenizer {nltk,space,moses}]\n",
            "                     [--bpe {sentencepiece,bert,gpt2,subword_nmt,fastbpe}]\n",
            "                     [--optimizer {adamax,adam,adagrad,sgd,adafactor,nag,adadelta}]\n",
            "                     [--lr-scheduler {inverse_sqrt,triangular,reduce_lr_on_plateau,fixed,cosine,polynomial_decay,tri_stage}]\n",
            "                     [--task TASK] [--num-workers N]\n",
            "                     [--skip-invalid-size-inputs-valid-test] [--max-tokens N]\n",
            "                     [--max-sentences N] [--required-batch-size-multiple N]\n",
            "                     [--dataset-impl FORMAT] [--train-subset SPLIT]\n",
            "                     [--valid-subset SPLIT] [--validate-interval N]\n",
            "                     [--fixed-validation-seed N] [--disable-validation]\n",
            "                     [--max-tokens-valid N] [--max-sentences-valid N]\n",
            "                     [--curriculum N] [--distributed-world-size N]\n",
            "                     [--distributed-rank DISTRIBUTED_RANK]\n",
            "                     [--distributed-backend DISTRIBUTED_BACKEND]\n",
            "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
            "                     [--distributed-port DISTRIBUTED_PORT]\n",
            "                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n",
            "                     [--ddp-backend {c10d,no_c10d}] [--bucket-cap-mb MB]\n",
            "                     [--fix-batches-to-gpus] [--find-unused-parameters]\n",
            "                     [--fast-stat-sync] --arch ARCH [--max-epoch N]\n",
            "                     [--max-update N] [--clip-norm NORM] [--sentence-avg]\n",
            "                     [--update-freq N1,N2,...,N_K] [--lr LR_1,LR_2,...,LR_N]\n",
            "                     [--min-lr LR] [--use-bmuf] [--save-dir DIR]\n",
            "                     [--restore-file RESTORE_FILE] [--reset-dataloader]\n",
            "                     [--reset-lr-scheduler] [--reset-meters]\n",
            "                     [--reset-optimizer] [--optimizer-overrides DICT]\n",
            "                     [--save-interval N] [--save-interval-updates N]\n",
            "                     [--keep-interval-updates N] [--keep-last-epochs N]\n",
            "                     [--no-save] [--no-epoch-checkpoints]\n",
            "                     [--no-last-checkpoints] [--no-save-optimizer-state]\n",
            "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
            "                     [--maximize-best-checkpoint-metric]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --no-progress-bar     disable progress bar\n",
            "  --log-interval N      log progress every N batches (when progress bar is\n",
            "                        disabled)\n",
            "  --log-format {json,none,simple,tqdm}\n",
            "                        log format to use\n",
            "  --tensorboard-logdir DIR\n",
            "                        path to save logs for tensorboard, should match\n",
            "                        --logdir of running tensorboard (default: no\n",
            "                        tensorboard logging)\n",
            "  --seed N              pseudo random number generator seed\n",
            "  --cpu                 use CPU instead of CUDA\n",
            "  --fp16                use FP16\n",
            "  --memory-efficient-fp16\n",
            "                        use a memory-efficient version of FP16 training;\n",
            "                        implies --fp16\n",
            "  --fp16-init-scale FP16_INIT_SCALE\n",
            "                        default FP16 loss scale\n",
            "  --fp16-scale-window FP16_SCALE_WINDOW\n",
            "                        number of updates before increasing loss scale\n",
            "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
            "                        pct of updates that can overflow before decreasing the\n",
            "                        loss scale\n",
            "  --min-loss-scale D    minimum FP16 loss scale, after which training is\n",
            "                        stopped\n",
            "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
            "                        threshold FP16 loss scale from below\n",
            "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
            "                        (tasks and/or architectures)\n",
            "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
            "                        how often to clear the PyTorch CUDA cache (0 to\n",
            "                        disable)\n",
            "  --criterion {cross_entropy,adaptive_loss,label_smoothed_cross_entropy,sentence_prediction,legacy_masked_lm_loss,composite_loss,masked_lm,nat_loss,sentence_ranking,binary_cross_entropy,label_smoothed_cross_entropy_with_alignment}\n",
            "  --tokenizer {nltk,space,moses}\n",
            "  --bpe {sentencepiece,bert,gpt2,subword_nmt,fastbpe}\n",
            "  --optimizer {adamax,adam,adagrad,sgd,adafactor,nag,adadelta}\n",
            "  --lr-scheduler {inverse_sqrt,triangular,reduce_lr_on_plateau,fixed,cosine,polynomial_decay,tri_stage}\n",
            "  --task TASK           task\n",
            "  --dataset-impl FORMAT\n",
            "                        output dataset implementation\n",
            "\n",
            "Dataset and data loading:\n",
            "  --num-workers N       how many subprocesses to use for data loading\n",
            "  --skip-invalid-size-inputs-valid-test\n",
            "                        ignore too long or too short lines in valid and test\n",
            "                        set\n",
            "  --max-tokens N        maximum number of tokens in a batch\n",
            "  --max-sentences N, --batch-size N\n",
            "                        maximum number of sentences in a batch\n",
            "  --required-batch-size-multiple N\n",
            "                        batch size will be a multiplier of this value\n",
            "  --train-subset SPLIT  data subset to use for training (train, valid, test)\n",
            "  --valid-subset SPLIT  comma separated list of data subsets to use for\n",
            "                        validation (train, valid, valid1, test, test1)\n",
            "  --validate-interval N\n",
            "                        validate every N epochs\n",
            "  --fixed-validation-seed N\n",
            "                        specified random seed for validation\n",
            "  --disable-validation  disable validation\n",
            "  --max-tokens-valid N  maximum number of tokens in a validation batch\n",
            "                        (defaults to --max-tokens)\n",
            "  --max-sentences-valid N\n",
            "                        maximum number of sentences in a validation batch\n",
            "                        (defaults to --max-sentences)\n",
            "  --curriculum N        don't shuffle batches for first N epochs\n",
            "\n",
            "Distributed training:\n",
            "  --distributed-world-size N\n",
            "                        total number of GPUs across all nodes (default: all\n",
            "                        visible GPUs)\n",
            "  --distributed-rank DISTRIBUTED_RANK\n",
            "                        rank of the current worker\n",
            "  --distributed-backend DISTRIBUTED_BACKEND\n",
            "                        distributed backend\n",
            "  --distributed-init-method DISTRIBUTED_INIT_METHOD\n",
            "                        typically tcp://hostname:port that will be used to\n",
            "                        establish initial connetion\n",
            "  --distributed-port DISTRIBUTED_PORT\n",
            "                        port number (not required if using --distributed-init-\n",
            "                        method)\n",
            "  --device-id DEVICE_ID, --local_rank DEVICE_ID\n",
            "                        which GPU to use (usually configured automatically)\n",
            "  --distributed-no-spawn\n",
            "                        do not spawn multiple processes even if multiple GPUs\n",
            "                        are visible\n",
            "  --ddp-backend {c10d,no_c10d}\n",
            "                        DistributedDataParallel backend\n",
            "  --bucket-cap-mb MB    bucket size for reduction\n",
            "  --fix-batches-to-gpus\n",
            "                        don't shuffle batches between GPUs; this reduces\n",
            "                        overall randomness and may affect precision but avoids\n",
            "                        the cost of re-reading the data\n",
            "  --find-unused-parameters\n",
            "                        disable unused parameter detection (not applicable to\n",
            "                        no_c10d ddp-backend\n",
            "  --fast-stat-sync      Enable fast sync of stats between nodes, this\n",
            "                        hardcodes to sync only some default stats from\n",
            "                        logging_output.\n",
            "\n",
            "Model configuration:\n",
            "  --arch ARCH, -a ARCH  Model Architecture\n",
            "\n",
            "Optimization:\n",
            "  --max-epoch N, --me N\n",
            "                        force stop training at specified epoch\n",
            "  --max-update N, --mu N\n",
            "                        force stop training at specified update\n",
            "  --clip-norm NORM      clip threshold of gradients\n",
            "  --sentence-avg        normalize gradients by the number of sentences in a\n",
            "                        batch (default is to normalize by number of tokens)\n",
            "  --update-freq N1,N2,...,N_K\n",
            "                        update parameters every N_i batches, when in epoch i\n",
            "  --lr LR_1,LR_2,...,LR_N, --learning-rate LR_1,LR_2,...,LR_N\n",
            "                        learning rate for the first N epochs; all epochs >N\n",
            "                        using LR_N (note: this may be interpreted differently\n",
            "                        depending on --lr-scheduler)\n",
            "  --min-lr LR           stop training when the learning rate reaches this\n",
            "                        minimum\n",
            "  --use-bmuf            specify global optimizer for syncing models on\n",
            "                        different GPUs/shards\n",
            "\n",
            "Checkpointing:\n",
            "  --save-dir DIR        path to save checkpoints\n",
            "  --restore-file RESTORE_FILE\n",
            "                        filename from which to load checkpoint (default:\n",
            "                        <save-dir>/checkpoint_last.pt\n",
            "  --reset-dataloader    if set, does not reload dataloader state from the\n",
            "                        checkpoint\n",
            "  --reset-lr-scheduler  if set, does not load lr scheduler state from the\n",
            "                        checkpoint\n",
            "  --reset-meters        if set, does not load meters from the checkpoint\n",
            "  --reset-optimizer     if set, does not load optimizer state from the\n",
            "                        checkpoint\n",
            "  --optimizer-overrides DICT\n",
            "                        a dictionary used to override optimizer args when\n",
            "                        loading a checkpoint\n",
            "  --save-interval N     save a checkpoint every N epochs\n",
            "  --save-interval-updates N\n",
            "                        save a checkpoint (and validate) every N updates\n",
            "  --keep-interval-updates N\n",
            "                        keep the last N checkpoints saved with --save-\n",
            "                        interval-updates\n",
            "  --keep-last-epochs N  keep last N epoch checkpoints\n",
            "  --no-save             don't save models or checkpoints\n",
            "  --no-epoch-checkpoints\n",
            "                        only store last and best checkpoints\n",
            "  --no-last-checkpoints\n",
            "                        don't store last checkpoints\n",
            "  --no-save-optimizer-state\n",
            "                        don't save optimizer-state as part of checkpoint\n",
            "  --best-checkpoint-metric BEST_CHECKPOINT_METRIC\n",
            "                        metric to use for saving \"best\" checkpoints\n",
            "  --maximize-best-checkpoint-metric\n",
            "                        select the largest metric value for saving \"best\"\n",
            "                        checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BCutg6DIhlL",
        "colab_type": "text"
      },
      "source": [
        "## Conv Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4lKFYoeaDM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2da99ab5-b9be-49ae-a305-c4980247815a"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/ --batch-size 64 \\\n",
        "                      --save-dir checkpoints/conv --arch fconv_iwslt_de_en --max-epoch 15 \\\n",
        "                      --optimizer adam --lr 0.001  \\\n",
        "                      --dropout 0.3 --encoder-embed-path glove.6B.300d.txt --encoder-embed-dim 300 \\\n",
        "                      --decoder-embed-dim 300 --no-epoch-checkpoints --decoder-embed-path glove.6B.300d.txt --decoder-out-embed-dim 300 \\\n",
        "                      --num-workers 3"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, arch='fconv_iwslt_de_en', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=300, decoder_embed_path='glove.6B.300d.txt', decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_embed_dim=300, encoder_embed_path='glove.6B.300d.txt', encoder_layers='[(256, 3)] * 4', fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/conv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [feature] dictionary: 640 types\n",
            "| [key] dictionary: 160 types\n",
            "| loaded 29 examples from: preprocessed_data/valid.feature-key.feature\n",
            "| loaded 29 examples from: preprocessed_data/valid.feature-key.key\n",
            "| preprocessed_data/ valid feature-key 29 examples\n",
            "| Found 358/640 types in embedding file.\n",
            "| Found 143/160 types in embedding file.\n",
            "FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (embed_tokens): Embedding(640, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (embed_tokens): Embedding(160, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "    (fc3): Linear(in_features=300, out_features=160, bias=True)\n",
            "  )\n",
            ")\n",
            "| model fconv_iwslt_de_en, criterion CrossEntropyCriterion\n",
            "| num. model params: 4435960 (num. trained: 4435960)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| no existing checkpoint found checkpoints/conv/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 260 examples from: preprocessed_data/train.feature-key.feature\n",
            "| loaded 260 examples from: preprocessed_data/train.feature-key.key\n",
            "| preprocessed_data/ train feature-key 260 examples\n",
            "| epoch 001:   0% 0/5 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "| epoch 001 | loss 6.541 | ppl 93.09 | wps 3568 | ups 22 | wpb 172.400 | bsz 52.000 | num_updates 5 | lr 0.001 | gnorm 5.306 | clip 0.000 | oom 0.000 | wall 1 | train_wall 0\n",
            "| epoch 001 | valid on 'valid' subset | loss 6.122 | ppl 69.64 | num_updates 5\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 1 @ 5 updates) (writing took 0.15093135833740234 seconds)\n",
            "| epoch 002 | loss 5.451 | ppl 43.74 | wps 3420 | ups 23 | wpb 172.400 | bsz 52.000 | num_updates 10 | lr 0.001 | gnorm 5.427 | clip 0.000 | oom 0.000 | wall 1 | train_wall 1\n",
            "| epoch 002 | valid on 'valid' subset | loss 4.958 | ppl 31.07 | num_updates 10 | best_loss 4.95755\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 2 @ 10 updates) (writing took 0.16647839546203613 seconds)\n",
            "| epoch 003 | loss 4.814 | ppl 28.13 | wps 3460 | ups 20 | wpb 172.400 | bsz 52.000 | num_updates 15 | lr 0.001 | gnorm 5.702 | clip 0.000 | oom 0.000 | wall 2 | train_wall 1\n",
            "| epoch 003 | valid on 'valid' subset | loss 5.049 | ppl 33.11 | num_updates 15 | best_loss 4.95755\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 3 @ 15 updates) (writing took 2.4736862182617188 seconds)\n",
            "| epoch 004 | loss 4.385 | ppl 20.89 | wps 4496 | ups 21 | wpb 172.400 | bsz 52.000 | num_updates 20 | lr 0.001 | gnorm 4.321 | clip 0.000 | oom 0.000 | wall 5 | train_wall 1\n",
            "| epoch 004 | valid on 'valid' subset | loss 4.497 | ppl 22.58 | num_updates 20 | best_loss 4.49715\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 4 @ 20 updates) (writing took 0.17333054542541504 seconds)\n",
            "| epoch 005 | loss 3.879 | ppl 14.71 | wps 3472 | ups 23 | wpb 172.400 | bsz 52.000 | num_updates 25 | lr 0.001 | gnorm 3.859 | clip 0.000 | oom 0.000 | wall 6 | train_wall 1\n",
            "| epoch 005 | valid on 'valid' subset | loss 4.222 | ppl 18.66 | num_updates 25 | best_loss 4.22211\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 5 @ 25 updates) (writing took 2.4528815746307373 seconds)\n",
            "| epoch 006 | loss 3.623 | ppl 12.32 | wps 3810 | ups 22 | wpb 172.400 | bsz 52.000 | num_updates 30 | lr 0.001 | gnorm 3.873 | clip 0.000 | oom 0.000 | wall 8 | train_wall 1\n",
            "| epoch 006 | valid on 'valid' subset | loss 4.040 | ppl 16.45 | num_updates 30 | best_loss 4.04024\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 6 @ 30 updates) (writing took 1.015841007232666 seconds)\n",
            "| epoch 007 | loss 3.488 | ppl 11.22 | wps 4675 | ups 22 | wpb 172.400 | bsz 52.000 | num_updates 35 | lr 0.001 | gnorm 3.704 | clip 0.000 | oom 0.000 | wall 10 | train_wall 1\n",
            "| epoch 007 | valid on 'valid' subset | loss 4.074 | ppl 16.85 | num_updates 35 | best_loss 4.04024\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 7 @ 35 updates) (writing took 2.850076198577881 seconds)\n",
            "| epoch 008 | loss 3.320 | ppl 9.99 | wps 3275 | ups 22 | wpb 172.400 | bsz 52.000 | num_updates 40 | lr 0.001 | gnorm 2.862 | clip 0.000 | oom 0.000 | wall 13 | train_wall 2\n",
            "| epoch 008 | valid on 'valid' subset | loss 3.998 | ppl 15.98 | num_updates 40 | best_loss 3.99784\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 8 @ 40 updates) (writing took 0.40033674240112305 seconds)\n",
            "| epoch 009 | loss 3.052 | ppl 8.3 | wps 3074 | ups 19 | wpb 172.400 | bsz 52.000 | num_updates 45 | lr 0.001 | gnorm 2.387 | clip 0.000 | oom 0.000 | wall 14 | train_wall 2\n",
            "| epoch 009 | valid on 'valid' subset | loss 3.985 | ppl 15.83 | num_updates 45 | best_loss 3.98495\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 9 @ 45 updates) (writing took 2.4619805812835693 seconds)\n",
            "| epoch 010 | loss 2.867 | ppl 7.29 | wps 3373 | ups 21 | wpb 172.400 | bsz 52.000 | num_updates 50 | lr 0.001 | gnorm 3.031 | clip 0.000 | oom 0.000 | wall 17 | train_wall 2\n",
            "| epoch 010 | valid on 'valid' subset | loss 3.942 | ppl 15.37 | num_updates 50 | best_loss 3.94182\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 10 @ 50 updates) (writing took 1.030979871749878 seconds)\n",
            "| epoch 011 | loss 2.886 | ppl 7.39 | wps 3315 | ups 19 | wpb 172.400 | bsz 52.000 | num_updates 55 | lr 0.001 | gnorm 3.766 | clip 0.000 | oom 0.000 | wall 19 | train_wall 2\n",
            "| epoch 011 | valid on 'valid' subset | loss 4.058 | ppl 16.66 | num_updates 55 | best_loss 3.94182\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 11 @ 55 updates) (writing took 1.0150012969970703 seconds)\n",
            "| epoch 012 | loss 2.682 | ppl 6.42 | wps 3631 | ups 21 | wpb 172.400 | bsz 52.000 | num_updates 60 | lr 0.001 | gnorm 4.363 | clip 0.000 | oom 0.000 | wall 20 | train_wall 2\n",
            "| epoch 012 | valid on 'valid' subset | loss 4.039 | ppl 16.44 | num_updates 60 | best_loss 3.94182\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 12 @ 60 updates) (writing took 1.01507568359375 seconds)\n",
            "| epoch 013 | loss 2.665 | ppl 6.34 | wps 3550 | ups 21 | wpb 172.400 | bsz 52.000 | num_updates 65 | lr 0.001 | gnorm 4.320 | clip 0.000 | oom 0.000 | wall 22 | train_wall 2\n",
            "| epoch 013 | valid on 'valid' subset | loss 3.959 | ppl 15.55 | num_updates 65 | best_loss 3.94182\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 13 @ 65 updates) (writing took 0.13382172584533691 seconds)\n",
            "| epoch 014 | loss 2.545 | ppl 5.83 | wps 3223 | ups 20 | wpb 172.400 | bsz 52.000 | num_updates 70 | lr 0.001 | gnorm 2.185 | clip 0.000 | oom 0.000 | wall 22 | train_wall 3\n",
            "| epoch 014 | valid on 'valid' subset | loss 3.837 | ppl 14.29 | num_updates 70 | best_loss 3.83717\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 14 @ 70 updates) (writing took 0.16521286964416504 seconds)\n",
            "| epoch 015 | loss 2.388 | ppl 5.24 | wps 3941 | ups 23 | wpb 172.400 | bsz 52.000 | num_updates 75 | lr 0.001 | gnorm 2.942 | clip 0.000 | oom 0.000 | wall 23 | train_wall 3\n",
            "| epoch 015 | valid on 'valid' subset | loss 3.818 | ppl 14.11 | num_updates 75 | best_loss 3.81824\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 15 @ 75 updates) (writing took 2.4466915130615234 seconds)\n",
            "| done training in 25.6 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImfiM5PLISnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d32ddfc7-686f-40b3-fe6f-e9ba02dafd04"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/conv/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > conv_last.out"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0% 0/1 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNC07vruPo0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47dc7024-76a5-4ca6-de1d-0bca953338c2"
      },
      "source": [
        "!grep ^H conv_last.out | cut -f3- > conv_last.out.sys\n",
        "!grep ^T conv_last.out | cut -f2- > conv_last.out.ref\n",
        "!fairseq-score --sys conv_last.out.sys --ref conv_last.out.ref --ignore-case"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='conv_last.out.ref', sacrebleu=False, sentence_bleu=False, sys='conv_last.out.sys')\n",
            "BLEU4 = 0.00, 38.2/20.8/7.7/0.0 (BP=1.000, ratio=1.017, syslen=123, reflen=121)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCrLk-zCPuiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}