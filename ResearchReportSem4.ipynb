{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResearchReportSem4.ipynb",
      "provenance": [],
      "mount_file_id": "1B9xn20f36VNC_nP1c9iKu5VHJcsk34jZ",
      "authorship_tag": "ABX9TyNZUJMy1FYK8h8vo/BP0FQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/ResearchReportSem4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7ouSKNHTUa",
        "colab_type": "text"
      },
      "source": [
        "# Installing FairSeq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_aCj_e36puh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "cf0e6a0d-5904-4dae-d96d-2ef9ba636b95"
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\r\u001b[K     |█                               | 10kB 26.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 33.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 38.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 23.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 122kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 163kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 174kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 184kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 194kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 204kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 215kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 225kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 235kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 245kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 256kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 266kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 286kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 296kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/f6/21311ebe1af3e2e6d959cc5ce69e09cae6b6267bbd6db294fcd8744316f6/sacrebleu-1.4.8-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.38.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (3.6.6)\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 197kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2033637 sha256=f6ad1414149d9204125b63910ccd58af344d4c7ea167ec91dd205aa9af1c44cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, mecab-python3, sacrebleu, fairseq\n",
            "Successfully installed fairseq-0.9.0 mecab-python3-0.996.5 portalocker-1.7.0 sacrebleu-1.4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaAyiCOHWUp",
        "colab_type": "text"
      },
      "source": [
        "# Download dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Xcx3pz5z-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osxgoL5mHd44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1f6c290b-d528-4eb0-ffe1-c9c6d2193cd4"
      },
      "source": [
        "!kaggle datasets list --user devicharith"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                             title                                  size  lastUpdated          downloadCount  \n",
            "----------------------------------------------  -------------------------------------  ----  -------------------  -------------  \n",
            "devicharith/language-translation-englishfrench  Language Translation (English-French)   4MB  2020-04-08 11:35:32             13  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-g0ThfHeCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "abe11649-a9b3-471c-b399-f8ff65570593"
      },
      "source": [
        "!kaggle datasets download devicharith/language-translation-englishfrench"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading language-translation-englishfrench.zip to /content\n",
            "\r  0% 0.00/3.51M [00:00<?, ?B/s]\n",
            "\r100% 3.51M/3.51M [00:00<00:00, 58.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THU0ViA1HunH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb01cd26-cb7f-4fcf-f3d3-1b412297fbf6"
      },
      "source": [
        "!unzip language-translation-englishfrench.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  language-translation-englishfrench.zip\n",
            "  inflating: eng_-french.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcIUf4dnIBAm",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTgsZjVqH08y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_auml_TIhua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"eng_-french.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbeQFlxoIh6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "83f5a4b8-3bd1-47eb-d1e5-32b66dda9019"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English words/sentences French words/sentences\n",
              "0                     Hi.                 Salut!\n",
              "1                    Run!                Cours !\n",
              "2                    Run!               Courez !\n",
              "3                    Who?                  Qui ?\n",
              "4                    Wow!             Ça alors !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqPM4vcyIiKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a8439d6-b0a5-4fe3-ce6a-02a8a1c6065c"
      },
      "source": [
        "trainset, testset = train_test_split(dataset, test_size=0.15, random_state=1234)\n",
        "trainset, validset = train_test_split(trainset, test_size=0.1, random_state=1234)\n",
        "trainset.shape, testset.shape, validset.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((134349, 2), (26344, 2), (14928, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQqmpb9_Jubp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeGhmp95L4HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write(dataframe, dataset_name, folder=\"dataset\"):\n",
        "    en = dataframe[\"English words/sentences\"]\n",
        "    french = dataframe[\"French words/sentences\"]\n",
        "    en.to_csv('{}/{}.en'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
        "    french.to_csv('{}/{}.fr'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcUeqNXQJzMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write(trainset, \"train\")\n",
        "write(testset, \"valid\")\n",
        "write(validset, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmleyKnrhSht",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agiCKuON6JF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "91b97d7d-bb48-45bb-ed07-a9ea770bf66a"
      },
      "source": [
        "!fairseq-preprocess --source-lang en --target-lang fr \\\n",
        "     --trainpref dataset/train --testpref dataset/test --validpref dataset/valid\\\n",
        "     --destdir preprocessed_data --seed 1234 --nwordssrc 25000 --nwordstgt 25000"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='preprocessed_data', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=25000, nwordstgt=25000, only_source=False, optimizer='nag', padding_factor=8, seed=1234, source_lang='en', srcdict=None, target_lang='fr', task='translation', tensorboard_logdir='', testpref='dataset/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='dataset/train', user_dir=None, validpref='dataset/valid', workers=1)\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/train.en: 134349 sents, 969752 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/valid.en: 26344 sents, 190333 tokens, 0.871% replaced by <unk>\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/test.en: 14928 sents, 107474 tokens, 0.929% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/train.fr: 134349 sents, 1045841 tokens, 1.19% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/valid.fr: 26344 sents, 205246 tokens, 2.44% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/test.fr: 14928 sents, 115853 tokens, 2.5% replaced by <unk>\n",
            "| Wrote preprocessed data to preprocessed_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9jMDRJahZz8",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB1puWhkhdOx",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebeOxkAhnvE",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcN5ahW59MPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09ff8546-3931-4432-b45d-876027103037"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/  --clip-norm 5 --batch-size 64 \\\n",
        "                      --save-dir checkpoints/lstm --arch lstm --max-epoch 15 --encoder-hidden-size 258 \\\n",
        "                      --encoder-layers 2  --decoder-hidden-size 258 --decoder-layers 2 --optimizer adam --lr 0.001  \\\n",
        "                      --dropout 0.3 --encoder-bidirectional --encoder-embed-dim 300 \\\n",
        "                      --decoder-embed-dim 300 --no-epoch-checkpoints --decoder-out-embed-dim 300 \\\n",
        "                      --num-workers 3"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', arch='lstm', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=5.0, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, decoder_embed_dim=300, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=258, decoder_layers=2, decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_bidirectional=True, encoder_dropout_in=0.3, encoder_dropout_out=0.3, encoder_embed_dim=300, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=258, encoder_layers=2, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/lstm', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (embed_tokens): Embedding(23320, 300, padding_idx=1)\n",
            "    (lstm): LSTM(300, 258, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (embed_tokens): Embedding(25000, 300, padding_idx=1)\n",
            "    (encoder_hidden_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (encoder_cell_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(558, 258)\n",
            "      (1): LSTMCell(258, 258)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=258, out_features=516, bias=False)\n",
            "      (output_proj): Linear(in_features=774, out_features=258, bias=False)\n",
            "    )\n",
            "    (additional_fc): Linear(in_features=258, out_features=300, bias=True)\n",
            "    (fc_out): Linear(in_features=300, out_features=25000, bias=True)\n",
            "  )\n",
            ")\n",
            "| model lstm, criterion CrossEntropyCriterion\n",
            "| num. model params: 26834548 (num. trained: 26834548)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| no existing checkpoint found checkpoints/lstm/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 001:   2% 43/2100 [00:04<02:34, 13.27it/s, loss=11.095, ppl=2186.77, wps=6260, ups=12, wpb=526.023, bsz=64.000, num_updates=43, lr=0.001, gnorm=3.126, clip=0.140, oom=0.000, wall=5, train_wall=4]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [16] is 16 which does not match the computed number of elements 19. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (19,).\n",
            "| epoch 001:   6% 129/2100 [00:10<02:32, 12.93it/s, loss=9.780, ppl=878.94, wps=6139, ups=12, wpb=502.171, bsz=64.000, num_updates=129, lr=0.001, gnorm=2.171, clip=0.062, oom=0.000, wall=12, train_wall=10]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [19] is 19 which does not match the computed number of elements 45. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (45,).\n",
            "| epoch 001 | loss 5.564 | ppl 47.32 | wps 6236 | ups 13 | wpb 498.020 | bsz 63.976 | num_updates 2100 | lr 0.001 | gnorm 1.162 | clip 0.004 | oom 0.000 | wall 169 | train_wall 160\n",
            "| epoch 001 | valid on 'valid' subset:  99% 409/412 [00:09<00:00, 27.93it/s]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [45] is 45 which does not match the computed number of elements 47. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (47,).\n",
            "| epoch 001 | valid on 'valid' subset | loss 3.386 | ppl 10.45 | num_updates 2100\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 1 @ 2100 updates) (writing took 1.9189412593841553 seconds)\n",
            "| epoch 002 | loss 3.341 | ppl 10.14 | wps 6208 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 4200 | lr 0.001 | gnorm 1.019 | clip 0.000 | oom 0.000 | wall 350 | train_wall 320\n",
            "| epoch 002 | valid on 'valid' subset | loss 2.518 | ppl 5.73 | num_updates 4200 | best_loss 2.51799\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 2 @ 4200 updates) (writing took 1.9292066097259521 seconds)\n",
            "| epoch 003 | loss 2.675 | ppl 6.38 | wps 6219 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 6300 | lr 0.001 | gnorm 0.966 | clip 0.000 | oom 0.000 | wall 530 | train_wall 480\n",
            "| epoch 003 | valid on 'valid' subset | loss 2.231 | ppl 4.7 | num_updates 6300 | best_loss 2.23115\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 3 @ 6300 updates) (writing took 1.888312578201294 seconds)\n",
            "| epoch 004 | loss 2.306 | ppl 4.95 | wps 6212 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 8400 | lr 0.001 | gnorm 0.925 | clip 0.000 | oom 0.000 | wall 710 | train_wall 639\n",
            "| epoch 004 | valid on 'valid' subset | loss 2.030 | ppl 4.08 | num_updates 8400 | best_loss 2.02976\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 4 @ 8400 updates) (writing took 1.8792660236358643 seconds)\n",
            "| epoch 005 | loss 2.069 | ppl 4.2 | wps 6199 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 10500 | lr 0.001 | gnorm 0.895 | clip 0.000 | oom 0.000 | wall 891 | train_wall 800\n",
            "| epoch 005 | valid on 'valid' subset | loss 1.929 | ppl 3.81 | num_updates 10500 | best_loss 1.92928\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 5 @ 10500 updates) (writing took 1.8253521919250488 seconds)\n",
            "| epoch 006 | loss 1.897 | ppl 3.72 | wps 6206 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 12600 | lr 0.001 | gnorm 0.870 | clip 0.000 | oom 0.000 | wall 1071 | train_wall 960\n",
            "| epoch 006 | valid on 'valid' subset | loss 1.858 | ppl 3.63 | num_updates 12600 | best_loss 1.85847\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 6 @ 12600 updates) (writing took 2.1757524013519287 seconds)\n",
            "| epoch 007 | loss 1.768 | ppl 3.41 | wps 6207 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 14700 | lr 0.001 | gnorm 0.850 | clip 0.000 | oom 0.000 | wall 1252 | train_wall 1120\n",
            "| epoch 007 | valid on 'valid' subset | loss 1.836 | ppl 3.57 | num_updates 14700 | best_loss 1.83627\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 7 @ 14700 updates) (writing took 1.9732916355133057 seconds)\n",
            "| epoch 008 | loss 1.665 | ppl 3.17 | wps 6208 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 16800 | lr 0.001 | gnorm 0.832 | clip 0.000 | oom 0.000 | wall 1432 | train_wall 1280\n",
            "| epoch 008 | valid on 'valid' subset | loss 1.797 | ppl 3.47 | num_updates 16800 | best_loss 1.7965\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 8 @ 16800 updates) (writing took 1.9086954593658447 seconds)\n",
            "| epoch 009 | loss 1.588 | ppl 3.01 | wps 6213 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 18900 | lr 0.001 | gnorm 0.820 | clip 0.000 | oom 0.000 | wall 1612 | train_wall 1439\n",
            "| epoch 009 | valid on 'valid' subset | loss 1.762 | ppl 3.39 | num_updates 18900 | best_loss 1.76191\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 9 @ 18900 updates) (writing took 1.8824272155761719 seconds)\n",
            "| epoch 010 | loss 1.518 | ppl 2.86 | wps 6215 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 21000 | lr 0.001 | gnorm 0.808 | clip 0.000 | oom 0.000 | wall 1793 | train_wall 1599\n",
            "| epoch 010 | valid on 'valid' subset | loss 1.745 | ppl 3.35 | num_updates 21000 | best_loss 1.74522\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 10 @ 21000 updates) (writing took 1.8681750297546387 seconds)\n",
            "| epoch 011 | loss 1.456 | ppl 2.74 | wps 6189 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 23100 | lr 0.001 | gnorm 0.796 | clip 0.000 | oom 0.000 | wall 1973 | train_wall 1759\n",
            "| epoch 011 | valid on 'valid' subset | loss 1.765 | ppl 3.4 | num_updates 23100 | best_loss 1.74522\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 11 @ 23100 updates) (writing took 0.7003545761108398 seconds)\n",
            "| epoch 012 | loss 1.408 | ppl 2.65 | wps 6199 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 25200 | lr 0.001 | gnorm 0.784 | clip 0.000 | oom 0.000 | wall 2153 | train_wall 1920\n",
            "| epoch 012 | valid on 'valid' subset | loss 1.732 | ppl 3.32 | num_updates 25200 | best_loss 1.732\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 12 @ 25200 updates) (writing took 1.995889663696289 seconds)\n",
            "| epoch 013 | loss 1.362 | ppl 2.57 | wps 6201 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 27300 | lr 0.001 | gnorm 0.777 | clip 0.000 | oom 0.000 | wall 2334 | train_wall 2080\n",
            "| epoch 013 | valid on 'valid' subset | loss 1.723 | ppl 3.3 | num_updates 27300 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 13 @ 27300 updates) (writing took 1.8983409404754639 seconds)\n",
            "| epoch 014 | loss 1.324 | ppl 2.5 | wps 6172 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 29400 | lr 0.001 | gnorm 0.768 | clip 0.000 | oom 0.000 | wall 2515 | train_wall 2241\n",
            "| epoch 014 | valid on 'valid' subset | loss 1.731 | ppl 3.32 | num_updates 29400 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 14 @ 29400 updates) (writing took 0.6554539203643799 seconds)\n",
            "| epoch 015 | loss 1.287 | ppl 2.44 | wps 6194 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 31500 | lr 0.001 | gnorm 0.761 | clip 0.000 | oom 0.000 | wall 2695 | train_wall 2401\n",
            "| epoch 015 | valid on 'valid' subset | loss 1.747 | ppl 3.36 | num_updates 31500 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 15 @ 31500 updates) (writing took 0.6092920303344727 seconds)\n",
            "| done training in 2704.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6tiPCl6hhtF",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzHkHB4KF2ZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "f719f642-26df-47d9-e12d-05e1a45df8eb"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/lstm/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > lstm_last.out"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2% 5/234 [00:01<00:46,  4.91it/s, wps=1228]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [3] is 3 which does not match the computed number of elements 4. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (4,).\n",
            " 10% 23/234 [00:04<00:50,  4.19it/s, wps=1363]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [4] is 4 which does not match the computed number of elements 5. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (5,).\n",
            " 25% 58/234 [00:13<00:49,  3.57it/s, wps=1375]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [5] is 5 which does not match the computed number of elements 6. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (6,).\n",
            " 44% 102/234 [00:27<00:42,  3.10it/s, wps=1370]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [6] is 6 which does not match the computed number of elements 7. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (7,).\n",
            " 62% 145/234 [00:41<00:29,  2.98it/s, wps=1404]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [7] is 7 which does not match the computed number of elements 8. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (8,).\n",
            " 76% 177/234 [00:51<00:20,  2.81it/s, wps=1435]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [8] is 8 which does not match the computed number of elements 9. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (9,).\n",
            " 85% 199/234 [01:00<00:15,  2.23it/s, wps=1447]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [9] is 9 which does not match the computed number of elements 10. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (10,).\n",
            " 91% 212/234 [01:06<00:09,  2.25it/s, wps=1449]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [10] is 10 which does not match the computed number of elements 11. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (11,).\n",
            " 94% 220/234 [01:09<00:06,  2.15it/s, wps=1457]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [11] is 11 which does not match the computed number of elements 12. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (12,).\n",
            " 96% 225/234 [01:12<00:04,  2.13it/s, wps=1463]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [12] is 12 which does not match the computed number of elements 13. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (13,).\n",
            " 97% 227/234 [01:13<00:03,  1.92it/s, wps=1463]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [13] is 13 which does not match the computed number of elements 14. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (14,).\n",
            " 98% 229/234 [01:14<00:02,  1.94it/s, wps=1466]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [14] is 14 which does not match the computed number of elements 15. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (15,).\n",
            " 99% 231/234 [01:15<00:01,  1.84it/s, wps=1469]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [15] is 15 which does not match the computed number of elements 17. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (17,).\n",
            " 99% 232/234 [01:15<00:01,  1.80it/s, wps=1471]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [17] is 17 which does not match the computed number of elements 21. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (21,).\n",
            "100% 233/234 [01:16<00:00,  1.69it/s, wps=1474]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [21] is 21 which does not match the computed number of elements 34. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (34,).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GneTJgbugZIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a3f10f2-b862-49b5-9f6d-45118618e3f7"
      },
      "source": [
        "!grep ^H lstm_last.out | cut -f3- > lstm_last.out.sys\n",
        "!grep ^T lstm_last.out | cut -f2- > lstm_last.out.ref\n",
        "!fairseq-score --sys lstm_last.out.sys --ref lstm_last.out.ref --ignore-case"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='lstm_last.out.ref', sacrebleu=False, sentence_bleu=False, sys='lstm_last.out.sys')\n",
            "BLEU4 = 43.91, 68.4/50.6/39.0/30.7 (BP=0.973, ratio=0.974, syslen=98285, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9W9AziYrZsi",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSH2elWIrpvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef154b31-7ce6-4647-cd55-1141f622a7db"
      },
      "source": [
        "!tail -n 100 lstm_last.out"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H-10523\t-0.3092171549797058\tÀ la fin d'un travail , tout le monde est pressé de rentrer chez lui.\n",
            "P-10523\t-1.3950 -0.1059 -0.0389 -0.3817 -0.4143 -0.5681 -0.3332 -0.1484 -0.0013 -0.0155 -0.0579 -0.1914 -0.2386 -0.5319 -0.5254 0.0000\n",
            "S-4980\tThe only reason Tom went to the party was that he expected Mary to be there.\n",
            "T-4980\tLa seule raison pour laquelle Tom est venu à la fête était qu'il espérait que Marie soit <<unk>>\n",
            "H-4980\t-0.36923620104789734\tLa seule raison raison , Tom est allé à la fête qu'il espérait que Marie soit là.\n",
            "P-4980\t-0.0055 -0.0055 -0.0000 -0.7288 -0.9190 -0.0065 -0.8930 -0.5125 -0.1288 -0.0066 -0.0221 -0.5502 -1.6436 -0.0478 -0.6189 -0.4241 -0.1333 -0.0000\n",
            "S-9787\tIt's no wonder Tom's sleeping <unk> he drinks up to twelve cups of coffee a day.\n",
            "T-9787\tCe n'est pas surprenant que Tom dorme mal , il boit jusqu'à douze tasses de café par jour.\n",
            "H-9787\t-0.595396101474762\tCe n'est pas étonnant que Tom dorme un verre de café à condition douze jours.\n",
            "P-9787\t-0.2619 -0.0016 -0.0111 -0.8643 -0.0453 -0.0035 -1.3139 -0.5687 -0.4753 -0.0994 -0.0056 -1.4583 -2.2704 -1.0734 -1.0735 0.0000\n",
            "S-10808\tThough it was a muggy night , she went to bed with all the windows closed.\n",
            "T-10808\tBien que ce fût une nuit <<unk>> , elle est allée au lit avec toutes les fenêtres fermées.\n",
            "H-10808\t-0.2141532450914383\tBien que c'était une nuit lourd , elle est allée au lit avec toutes les fenêtres fermés.\n",
            "P-10808\t-0.0361 -0.0635 -0.7582 -0.0529 -0.3345 -0.8319 -0.0129 -0.0132 -0.7616 -0.2122 -0.2624 -0.0312 -0.0201 -0.3298 -0.0021 -0.0000 -0.1321 0.0000\n",
            "S-6263\tYou're the only person I know besides me who would prefer to live in a tent.\n",
            "T-6263\tTu es la seule personne que je connaisse , à part moi , qui préférerais vivre dans une tente.\n",
            "H-6263\t-0.09729104489088058\tTu es la seule personne que je connaisse à part moi qui préférerais vivre dans une tente.\n",
            "P-6263\t-0.5059 -0.0016 -0.0014 -0.0111 -0.0002 -0.1163 -0.0150 -0.0434 -0.1019 -0.0354 -0.0183 -0.4467 -0.4497 -0.0023 -0.0007 -0.0013 -0.0001 0.0000\n",
            "S-12129\tI thought I was about to be captured so I ran as fast as I could.\n",
            "T-12129\tJe pensais que j'étais sur le point d'être capturée , alors je courus aussi vite que je le pus.\n",
            "H-12129\t-0.2322169840335846\tJe pensais que j'étais sur le point d'être capturé , alors j'ai couru aussi vite que je le pouvais.\n",
            "P-12129\t-0.0155 -0.0567 -0.1101 -0.1975 -0.3868 -0.0450 -0.0094 -0.0035 -0.8702 -0.6841 -0.5043 -0.4823 -0.0010 -0.0631 -0.0461 -0.0018 -0.4928 -0.6121 -0.0620 0.0000\n",
            "S-10886\tPlease don't make me laugh. I did too many sit-ups yesterday and my stomach muscles hurt.\n",
            "T-10886\tS'il te plait ne me fais pas rire. J'ai fait trop <<unk>> hier et mes muscles <<unk>> me font mal.\n",
            "H-10886\t-0.3982280492782593\tJe te prie de ne pas me faire rire. J'ai fait trop <unk> , hier et mes muscles <unk> mal.\n",
            "P-10886\t-1.9304 -0.4362 -0.0376 -0.0007 -0.0072 -0.1315 -0.0190 -0.1566 -0.2404 -0.4613 -0.0026 -0.0376 -0.0118 -0.6816 -0.2040 -0.7573 -0.5343 -0.0004 -1.2893 -1.4231 0.0000\n",
            "S-4502\tI don't think it'll rain , but I'll take an umbrella just in case it does.\n",
            "T-4502\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie juste au cas où cela se <<unk>>\n",
            "H-4502\t-0.14987024664878845\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie.\n",
            "P-4502\t-0.0075 -0.0183 -0.0283 -0.0220 -0.0105 -0.6233 -0.0369 -0.0416 -0.0031 -0.0709 -0.5187 -0.0726 -0.0116 -0.7518 -0.0310\n",
            "S-319\tI thought if I broke up with you , I'd never have to see you again.\n",
            "T-319\tJe pensais que si je <<unk>> avec toi , je n'aurais plus jamais à te revoir.\n",
            "H-319\t-0.26804235577583313\tJe pensais si je me suis rompu avec vous , je ne devrais jamais vous revoir.\n",
            "P-319\t-0.0978 -0.1009 -1.0724 -0.0216 -0.3855 -0.0640 -1.2317 -0.0491 -0.4439 -0.0027 -0.0087 -0.0505 -0.4893 -0.0175 -0.5025 -0.0186 0.0000\n",
            "S-5700\tIt's still too hard to find a job. And even if you have a job , chances are you're having a tougher time paying the rising costs of everything from <unk> to gas.\n",
            "T-5700\tC'est encore trop difficile de trouver un emploi. Et même quand on en a un , il y a des chances qu'on ait davantage de difficultés à payer le coût de tout , de <<unk>> au gaz.\n",
            "H-5700\t-0.5950573086738586\tC'est encore trop difficile pour trouver un boulot , même si tu as un boulot , mais vous avez un boulot de temps en <unk>\n",
            "P-5700\t-0.6696 -0.3579 -0.0034 -0.0711 -0.3546 -0.0152 -0.0116 -0.4062 -0.1495 -0.6232 -0.0018 -0.3676 -0.3742 -0.0874 -0.5002 -0.3835 -1.3831 -0.9486 -0.7781 -0.9716 -1.2587 -0.8680 -1.2014 -1.1214 -0.9728 -1.5910\n",
            "S-10885\tIt was bad enough that he usually came to work late , but coming in drunk was the last straw , and I'm going to have to let him go.\n",
            "T-10885\tC'était suffisamment grave qu'il ait l'habitude d'arriver en retard au travail , mais qu'il arrive soûl est un comble , et je vais devoir m'en séparer.\n",
            "H-10885\t-0.7556149959564209\tIl fut assez mauvais qu'il est venu en retard , mais je ne vais pas étudier en retard , mais je vais le laisser partir.\n",
            "P-10885\t-0.4331 -1.0967 -0.6396 -0.0999 -0.5489 -1.4321 -0.6969 -1.4414 -0.2558 -0.3193 -0.0098 -0.5201 -2.8518 -0.3701 -0.3946 -1.4623 -0.9579 -0.4928 -0.4849 -0.0516 -0.1213 -0.1185 -2.7304 -0.7580 -1.3581 -0.0000\n",
            "S-6030\tIn the same amount of time it would take me to correct all the mistakes in your report , I could write a better report myself.\n",
            "T-6030\tDans le même <<unk>> de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi-même.\n",
            "H-6030\t-0.3895154893398285\tDans la même fois qu'il <unk> à corriger toutes les erreurs , je pourrais écrire un meilleur rapport dans la tête.\n",
            "P-6030\t-0.2692 -0.0210 -0.0001 -0.4367 -1.0404 -0.5308 -1.0081 -0.0836 -0.6450 -0.0011 -0.0018 -0.2731 -0.3790 -0.8338 -0.0742 -0.1003 -0.1233 -0.0491 -1.0774 -1.3290 -0.2924 0.0000\n",
            "S-4762\tDid you know that in Japan , if you have a tattoo , you won't be allowed to bathe in many of the hot spring <unk>\n",
            "T-4762\tSaviez-vous qu'au Japon , si vous portez un tatouage , vous ne serez pas autorisé à vous baigner dans beaucoup des <<unk>> <<unk>> ?\n",
            "H-4762\t-0.4053613841533661\tSavais-tu cela au Japon , si tu as un tatouage , si tu auras un tatouage , vous ne serez pas autorisé à <unk> dans beaucoup de printemps , si vous ne serez pas <unk>\n",
            "P-4762\t-0.8459 -1.2334 -0.1052 -0.0001 -0.0101 -0.0218 -0.5053 -0.5076 -0.0049 -0.0013 -0.1564 -0.5802 -0.4033 -0.9214 -0.0618 -0.0072 -0.9607 -0.8450 -0.0342 -0.0052 -0.0018 -0.9527 -0.0074 -0.5582 -0.5079 -0.0973 -0.2682 -0.0814 -0.4304 -1.6255 -1.0822 -0.2639 -0.0013 -0.0030 -0.8837 -0.6174\n",
            "S-11299\tI know that it's highly unlikely that you'd ever want to go out with me , but I still need to ask at least once.\n",
            "T-11299\tJe sais qu'il est très improbable que tu veuilles jamais sortir avec moi , mais je dois quand même demander au moins une fois.\n",
            "H-11299\t-0.17841894924640656\tJe sais qu'il est hautement improbable que vous vouliez jamais sortir avec moi , mais j'ai encore besoin de demander au moins une fois.\n",
            "P-11299\t-0.0051 -0.0065 -0.0806 -0.0087 -0.0029 -0.0112 -0.0022 -0.9706 -0.9093 -0.0667 -0.4964 -0.0400 -0.0009 -0.0297 -0.0035 -0.5183 -0.1709 -0.0007 -0.0542 -0.1245 -0.6647 -0.0282 -0.2014 -0.0633 -0.0000\n",
            "S-9140\tIf you don't eat breakfast , you'll probably be hungry during the morning and won't be as efficient at work as you could be.\n",
            "T-9140\tSi tu ne <<unk>> pas , tu auras probablement faim au cours de la matinée et tu ne seras pas aussi efficace au travail que tu pourrais l'être.\n",
            "H-9140\t-0.31194645166397095\tSi vous ne prenez pas le petit déjeuner , tu auras probablement faim pendant le matin et vous ne serez pas aussi efficace au travail.\n",
            "P-9140\t-0.0009 -0.6038 -0.0634 -0.5961 -0.0168 -0.2219 -0.4228 -0.0223 -0.0029 -0.6202 -0.8722 -0.1905 -0.0008 -0.2572 -0.3038 -0.3397 -0.0290 -1.8965 -0.5640 -0.0237 -0.0732 -0.1440 -0.0577 -0.2077 -0.5791 -0.0002\n",
            "S-13683\tIf you don't want to put on sunscreen , that's your problem. Just don't come complaining to me when you get a sunburn.\n",
            "T-13683\tSi tu ne veux pas mettre de crème solaire c'est ton problème , mais ne viens pas te plaindre quand <<unk>> des coups de soleil.\n",
            "H-13683\t-0.4888012707233429\tSi tu ne veux pas mettre de crème solaire , c'est ton problème , juste quand tu auras un coup de soleil.\n",
            "P-13683\t-0.0022 -0.4659 -0.0278 -0.0118 -0.0020 -1.5814 -0.0823 -0.0091 -0.0709 -0.0006 -0.2368 -1.7831 -0.2228 -0.5850 -2.1608 -1.3105 -0.5808 -1.3423 -0.0111 -0.0370 -0.0188 -0.6993 -0.0000\n",
            "S-13070\tDuring hard times , people might not go on a trip , but they might be willing to pay extra for good coffee.\n",
            "T-13070\tQuand les temps sont durs , les gens peuvent ne pas partir en voyage mais ils peuvent être disposés à payer davantage pour du café de bonne qualité.\n",
            "H-13070\t-0.5406641960144043\tPendant la fois de nombreuses fois , les gens ne sont pas en voyage , mais ils se sont <unk> pour payer un bon café.\n",
            "P-13070\t-0.3247 -2.0434 -0.9499 -0.5002 -1.9616 -0.0350 -0.2147 -0.0394 -0.0261 -0.7406 -0.4989 -0.0275 -1.1779 -0.1581 -0.1574 -0.0087 -1.5190 -1.0986 -0.3093 -1.1500 -0.5654 -0.0909 -0.4264 -0.0246 -0.0089 0.0000\n",
            "S-10827\tI still have a scar on my left leg from a car accident I was in when I was thirteen years old.\n",
            "T-10827\tJ'ai encore une cicatrice sur ma jambe gauche à la suite d'un accident de voiture dans lequel j'ai été impliqué quand j'avais treize ans.\n",
            "H-10827\t-0.21723514795303345\tJ'ai encore une cicatrice sur la jambe gauche.\n",
            "P-10827\t-0.0997 -0.3617 -0.0402 -0.0003 -0.0472 -0.6609 -0.0083 -0.7345 -0.0023\n",
            "S-13283\tI never see a library without wishing I had time to go there and stay till I had read everything in it.\n",
            "T-13283\tJe ne vois jamais une bibliothèque sans souhaiter avoir le temps de m'y rendre et d'y rester jusqu'à ce que j'y aie tout lu.\n",
            "H-13283\t-0.607856273651123\tJe ne vois jamais une bibliothèque sans <unk> que j'ai eu le temps d'y aller , et de rester jusqu'à ce que je devais tout lire.\n",
            "P-13283\t-0.0580 -0.2052 -0.0098 -0.0065 -0.1251 -0.0392 -0.0001 -0.2895 -2.1609 -0.9237 -1.3194 -0.3525 -0.0249 -1.1650 -0.4096 -1.0684 -0.0732 -2.0596 -1.0305 -1.1899 -0.0642 -0.6192 -1.2240 -1.6706 -0.2727 -0.0503 0.0000\n",
            "S-4837\tPolice are <unk> people not to pick up <unk> as they search for two prisoners on the run after escaping from jail.\n",
            "T-4837\tLa police <<unk>> les gens à ne pas prendre <<unk>> car ils recherchent deux prisonniers en cavale après leur <<unk>>\n",
            "H-4837\t-0.7455959320068359\tLa police est <unk> aux gens de ne pas prendre les <unk> , comme ils <unk> deux prisonniers sur les <unk> de la prison.\n",
            "P-4837\t-0.0012 -0.0045 -0.2320 -0.1504 -1.1384 -0.0169 -0.8876 -0.0699 -0.0110 -0.9242 -0.7453 -1.6871 -1.4357 -2.4364 -1.6325 -0.7937 -0.3436 -0.0498 -0.6521 -0.9672 -1.1275 -1.3427 -1.2668 -0.7232 0.0000\n",
            "S-248\tWhen we started out <unk> web pages , we were the only ones doing it in this part of the country.\n",
            "T-248\tQuand nous avons commencé à concevoir des pages web , nous étions les seuls à le faire dans cette partie du pays.\n",
            "H-248\t-0.5113544464111328\tLorsque nous avons commencé à <unk> <unk> , nous étions les seules à le faire dans cette partie de la campagne.\n",
            "P-248\t-0.4518 -0.0127 -0.2498 -0.2641 -0.3244 -1.3885 -2.1747 -0.2284 -0.1176 -0.3180 -0.5396 -0.1679 -1.2175 -1.0514 -0.2821 -1.1703 -0.1228 -0.0311 -0.6858 -0.2932 -0.1582 0.0000\n",
            "S-11828\tI was looking forward to seeing a scenic view of Mt. Fuji , but unfortunately it was completely hidden behind clouds.\n",
            "T-11828\tJ'avais hâte de voir une vue <<unk>> du mont Fuji mais , malheureusement , il était complètement caché par les nuages.\n",
            "H-11828\t-0.3577519357204437\tJ'étais impatiente de voir une vue malade du Mont Fuji , mais tout le monde fut complètement caché derrière les nuages.\n",
            "P-11828\t-0.5648 -0.1464 -0.0625 -0.1662 -0.0198 -0.1668 -2.5503 -0.3932 -0.2556 -0.0340 -0.0202 -0.0174 -1.4024 -0.2092 -0.1026 -0.9909 -0.2960 -0.4603 -0.0005 -0.0065 -0.0048 0.0000\n",
            "S-7610\tI tried to stay inside the house all day , but I ended up going outside and sitting in the garden.\n",
            "T-7610\tJ'ai essayé de rester dans la maison toute la journée , mais j'ai fini par sortir m'asseoir dans le jardin.\n",
            "H-7610\t-0.26735180616378784\tJ'ai essayé de rester à l'intérieur toute la maison , mais j'ai fini par sortir dans le jardin.\n",
            "P-7610\t-0.1674 -0.0489 -0.0659 -0.0205 -0.2618 -0.0071 -0.1632 -0.0007 -0.2752 -0.0721 -0.0035 -0.1481 -1.4353 -0.5221 -0.1740 -1.1388 -0.4853 -0.0900 -0.0000\n",
            "S-6513\t<unk> told police that the train was <unk> way over the speed limit when it <unk> going around a <unk>\n",
            "T-6513\tDes <<unk>> ont déclaré à la police que le train roulait bien au-dessus de la limite de vitesse lorsqu'il a <<unk>> dans un <<unk>>\n",
            "H-6513\t-0.6118181347846985\tOn dit que le train a dit que le train était <unk> à la vitesse de la vitesse quand ça <unk> autour d'un <unk>\n",
            "P-6513\t-1.3675 -0.0101 -0.0647 -0.1152 -0.0000 -1.3611 -0.5313 -0.0266 -0.0625 -0.0000 -0.5073 -1.4963 -1.9704 -0.5262 -0.2134 -0.9079 -0.4953 -0.1088 -1.4005 -1.9786 -0.3412 -0.5282 -0.8028 -0.4793 -0.0001\n",
            "S-10561\tI suppose you want to ask me how I was able to make so much money in so little time.\n",
            "T-10561\tJe suppose que vous voulez me demander comment j'ai été en mesure de me faire autant d'argent en si peu de temps.\n",
            "H-10561\t-0.3360203504562378\tJe suppose que tu veux me demander comment j'ai été en mesure de faire autant d'argent dans un peu de temps.\n",
            "P-10561\t-0.0055 -0.0223 -0.0027 -0.6870 -0.0366 -0.0006 -0.0042 -0.0061 -1.3233 -0.0723 -0.7302 -0.0031 -0.0276 -0.9772 -0.2762 -0.1041 -0.8012 -0.3717 -0.5554 -1.3743 -0.0108 0.0000\n",
            "| Translated 14928 sentences (113213 tokens) in 25.7s (580.01 sentences/s, 4398.79 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 43.71, 67.9/50.4/38.8/30.6 (BP=0.973, ratio=0.974, syslen=98285, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsL5n7FHhq12",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5EquxnviIHo",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXVAic1uhIDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3093c4f4-63b9-49c1-f1ab-e85972bd7453"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/ \\\n",
        "     --lr 0.001 --clip-norm 0.1 --dropout 0.3 --max-epoch 15 --optimizer adam\\\n",
        "     --arch fconv_iwslt_de_en --save-dir checkpoints/fconv --batch-size 128 --no-epoch-checkpoints \\\n",
        "     --encoder-embed-dim 300 --decoder-embed-dim 300 --decoder-out-embed-dim 300 --num-workers 3"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, arch='fconv_iwslt_de_en', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=300, decoder_embed_path=None, decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_embed_dim=300, encoder_embed_path=None, encoder_layers='[(256, 3)] * 4', fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=128, max_sentences_valid=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/fconv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (embed_tokens): Embedding(23320, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (embed_tokens): Embedding(25000, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "    (fc3): Linear(in_features=300, out_features=25000, bias=True)\n",
            "  )\n",
            ")\n",
            "| model fconv_iwslt_de_en, criterion CrossEntropyCriterion\n",
            "| num. model params: 26193640 (num. trained: 26193640)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 128\n",
            "| no existing checkpoint found checkpoints/fconv/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 001 | loss 5.623 | ppl 49.3 | wps 11121 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 1050 | lr 0.001 | gnorm 1.162 | clip 1.000 | oom 0.000 | wall 96 | train_wall 90\n",
            "| epoch 001 | valid on 'valid' subset | loss 3.422 | ppl 10.72 | num_updates 1050\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 1 @ 1050 updates) (writing took 1.7692511081695557 seconds)\n",
            "| epoch 002 | loss 3.201 | ppl 9.19 | wps 11046 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 2100 | lr 0.001 | gnorm 0.877 | clip 1.000 | oom 0.000 | wall 199 | train_wall 180\n",
            "| epoch 002 | valid on 'valid' subset | loss 2.491 | ppl 5.62 | num_updates 2100 | best_loss 2.49082\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 2 @ 2100 updates) (writing took 1.7268321514129639 seconds)\n",
            "| epoch 003 | loss 2.453 | ppl 5.47 | wps 11070 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 3150 | lr 0.001 | gnorm 0.818 | clip 1.000 | oom 0.000 | wall 301 | train_wall 270\n",
            "| epoch 003 | valid on 'valid' subset | loss 2.186 | ppl 4.55 | num_updates 3150 | best_loss 2.18624\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 3 @ 3150 updates) (writing took 1.799570083618164 seconds)\n",
            "| epoch 004 | loss 2.037 | ppl 4.1 | wps 11061 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 4200 | lr 0.001 | gnorm 0.770 | clip 1.000 | oom 0.000 | wall 404 | train_wall 360\n",
            "| epoch 004 | valid on 'valid' subset | loss 2.022 | ppl 4.06 | num_updates 4200 | best_loss 2.02162\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 4 @ 4200 updates) (writing took 1.7686834335327148 seconds)\n",
            "| epoch 005 | loss 1.780 | ppl 3.43 | wps 11057 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 5250 | lr 0.001 | gnorm 0.734 | clip 1.000 | oom 0.000 | wall 506 | train_wall 450\n",
            "| epoch 005 | valid on 'valid' subset | loss 1.955 | ppl 3.88 | num_updates 5250 | best_loss 1.95471\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 5 @ 5250 updates) (writing took 1.7555854320526123 seconds)\n",
            "| epoch 006 | loss 1.596 | ppl 3.02 | wps 11042 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 6300 | lr 0.001 | gnorm 0.702 | clip 1.000 | oom 0.000 | wall 609 | train_wall 540\n",
            "| epoch 006 | valid on 'valid' subset | loss 1.904 | ppl 3.74 | num_updates 6300 | best_loss 1.90355\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 6 @ 6300 updates) (writing took 1.7877705097198486 seconds)\n",
            "| epoch 007 | loss 1.460 | ppl 2.75 | wps 10987 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 7350 | lr 0.001 | gnorm 0.674 | clip 1.000 | oom 0.000 | wall 712 | train_wall 631\n",
            "| epoch 007 | valid on 'valid' subset | loss 1.840 | ppl 3.58 | num_updates 7350 | best_loss 1.83965\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 7 @ 7350 updates) (writing took 1.701216220855713 seconds)\n",
            "| epoch 008 | loss 1.352 | ppl 2.55 | wps 10982 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 8400 | lr 0.001 | gnorm 0.649 | clip 1.000 | oom 0.000 | wall 815 | train_wall 722\n",
            "| epoch 008 | valid on 'valid' subset | loss 1.814 | ppl 3.52 | num_updates 8400 | best_loss 1.81382\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 8 @ 8400 updates) (writing took 1.7253515720367432 seconds)\n",
            "| epoch 009 | loss 1.269 | ppl 2.41 | wps 11071 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 9450 | lr 0.001 | gnorm 0.629 | clip 1.000 | oom 0.000 | wall 918 | train_wall 811\n",
            "| epoch 009 | valid on 'valid' subset | loss 1.783 | ppl 3.44 | num_updates 9450 | best_loss 1.78253\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 9 @ 9450 updates) (writing took 1.7163794040679932 seconds)\n",
            "| epoch 010 | loss 1.198 | ppl 2.29 | wps 11044 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 10500 | lr 0.001 | gnorm 0.610 | clip 1.000 | oom 0.000 | wall 1021 | train_wall 902\n",
            "| epoch 010 | valid on 'valid' subset | loss 1.798 | ppl 3.48 | num_updates 10500 | best_loss 1.78253\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 10 @ 10500 updates) (writing took 0.6765499114990234 seconds)\n",
            "| epoch 011 | loss 1.134 | ppl 2.2 | wps 11052 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 11550 | lr 0.001 | gnorm 0.595 | clip 1.000 | oom 0.000 | wall 1122 | train_wall 992\n",
            "| epoch 011 | valid on 'valid' subset | loss 1.780 | ppl 3.43 | num_updates 11550 | best_loss 1.78029\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 11 @ 11550 updates) (writing took 1.6620514392852783 seconds)\n",
            "| epoch 012 | loss 1.083 | ppl 2.12 | wps 11009 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 12600 | lr 0.001 | gnorm 0.584 | clip 1.000 | oom 0.000 | wall 1225 | train_wall 1082\n",
            "| epoch 012 | valid on 'valid' subset | loss 1.779 | ppl 3.43 | num_updates 12600 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 12 @ 12600 updates) (writing took 1.7584583759307861 seconds)\n",
            "| epoch 013 | loss 1.034 | ppl 2.05 | wps 11057 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 13650 | lr 0.001 | gnorm 0.570 | clip 1.000 | oom 0.000 | wall 1327 | train_wall 1173\n",
            "| epoch 013 | valid on 'valid' subset | loss 1.795 | ppl 3.47 | num_updates 13650 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 13 @ 13650 updates) (writing took 0.6580889225006104 seconds)\n",
            "| epoch 014 | loss 0.996 | ppl 1.99 | wps 11046 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 14700 | lr 0.001 | gnorm 0.559 | clip 1.000 | oom 0.000 | wall 1429 | train_wall 1263\n",
            "| epoch 014 | valid on 'valid' subset | loss 1.787 | ppl 3.45 | num_updates 14700 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 14 @ 14700 updates) (writing took 0.6093218326568604 seconds)\n",
            "| epoch 015 | loss 0.959 | ppl 1.94 | wps 11056 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 15750 | lr 0.001 | gnorm 0.551 | clip 1.000 | oom 0.000 | wall 1531 | train_wall 1353\n",
            "| epoch 015 | valid on 'valid' subset | loss 1.768 | ppl 3.41 | num_updates 15750 | best_loss 1.76801\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 15 @ 15750 updates) (writing took 1.8601069450378418 seconds)\n",
            "| done training in 1537.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IyqxKTQlWLW",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa-ow3Y-iGUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a203011c-f18e-4055-b622-38d23ab475c8"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/fconv/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > conv_out.out"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R88p7yf2kg19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "55bccdee-a06f-448f-f437-ca82241b6185"
      },
      "source": [
        "!grep ^H conv_out.out | cut -f3- > conv_out.out.sys\n",
        "!grep ^T conv_out.out | cut -f2- > conv_out.out.ref\n",
        "!fairseq-score --sys conv_out.out.sys --ref conv_out.out.ref --ignore-case"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='conv_out.out.ref', sacrebleu=False, sentence_bleu=False, sys='conv_out.out.sys')\n",
            "BLEU4 = 42.56, 65.4/48.1/36.7/28.4 (BP=1.000, ratio=1.025, syslen=103402, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_kmbcq2rsWF",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tJq8k7NruSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072b34dc-5ad3-49c1-a008-4c2726292b3a"
      },
      "source": [
        "!tail -n 100 conv_out.out"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H-10523\t-0.36244139075279236\tÀ la fin de travail , tout le monde est pressé , tout le monde est pressé de rentrer à la maison.\n",
            "P-10523\t-1.2422 -0.1022 -0.0002 -0.1192 -0.6157 -0.0312 -0.6946 -0.0011 -0.0007 -0.5180 -0.5462 -0.3651 -0.3874 -0.0029 -0.0007 -0.8464 -0.6123 -0.2805 -0.7026 -0.7963 -0.1867 -0.2839 -0.0000\n",
            "S-4980\tThe only reason Tom went to the party was that he expected Mary to be there.\n",
            "T-4980\tLa seule raison pour laquelle Tom est venu à la fête était qu'il espérait que Marie soit <<unk>>\n",
            "H-4980\t-0.5381535291671753\tLa seule raison pour que Tom est allé à la fête était là que Mary s'y soit allé.\n",
            "P-4980\t-0.1132 -0.1080 -0.0003 -0.7223 -0.9748 -0.0002 -0.7911 -0.0597 -0.1847 -0.0571 -0.0705 -1.3654 -0.6302 -1.7399 -0.5191 -1.0186 -1.2311 -0.6382 -0.0005\n",
            "S-9787\tIt's no wonder Tom's sleeping <unk> he drinks up to twelve cups of coffee a day.\n",
            "T-9787\tCe n'est pas surprenant que Tom dorme mal , il boit jusqu'à douze tasses de café par jour.\n",
            "H-9787\t-0.5301346778869629\tCe n'est pas étonnant que les dormir de Tom , il boit pour le café un jour.\n",
            "P-9787\t-0.6016 -0.0063 -0.0109 -0.0007 -0.6177 -1.1423 -0.0332 -0.0003 -0.0859 -1.4167 -0.5496 -0.9512 -1.3210 -1.5447 -0.0159 -1.2251 -0.0193 -0.0000\n",
            "S-10808\tThough it was a muggy night , she went to bed with all the windows closed.\n",
            "T-10808\tBien que ce fût une nuit <<unk>> , elle est allée au lit avec toutes les fenêtres fermées.\n",
            "H-10808\t-0.3317604064941406\tBien que c'était une soir , elle est allée au lit , elle est allée au lit les fenêtres fermés.\n",
            "P-10808\t-0.0033 -0.0135 -0.0218 -0.0023 -0.6469 -1.2646 -0.0166 -0.6565 -0.2008 -0.2546 -0.0013 -0.4300 -0.5573 -0.2350 -0.2414 -0.9726 -0.0078 -1.2214 -0.0006 -0.2186 -0.0001\n",
            "S-6263\tYou're the only person I know besides me who would prefer to live in a tent.\n",
            "T-6263\tTu es la seule personne que je connaisse , à part moi , qui préférerais vivre dans une tente.\n",
            "H-6263\t-0.14039607346057892\tTu es la seule personne que je connaisse à part moi qui préférerais vivre dans une tente.\n",
            "P-6263\t-0.5435 -0.0011 -0.0018 -0.0036 -0.0000 -0.0006 -0.0005 -0.0113 -0.2237 -0.0652 -0.0164 -0.5308 -1.0336 -0.0268 -0.0020 -0.0661 -0.0001 -0.0000\n",
            "S-12129\tI thought I was about to be captured so I ran as fast as I could.\n",
            "T-12129\tJe pensais que j'étais sur le point d'être capturée , alors je courus aussi vite que je le pus.\n",
            "H-12129\t-0.18774579465389252\tJe pensais que j'étais sur le point d'être capturé , alors je courus aussi vite que je pouvais.\n",
            "P-12129\t-0.1112 -0.0164 -0.0442 -0.0591 -0.0531 -0.0375 -0.0394 -0.0953 -0.0025 -0.4810 -0.4666 -0.4705 -0.5187 -0.3346 -0.0042 -0.0005 -0.3000 -0.5324 -0.0000\n",
            "S-10886\tPlease don't make me laugh. I did too many sit-ups yesterday and my stomach muscles hurt.\n",
            "T-10886\tS'il te plait ne me fais pas rire. J'ai fait trop <<unk>> hier et mes muscles <<unk>> me font mal.\n",
            "H-10886\t-0.44070544838905334\tJe te prie de ne pas me faire rire , je me prie.\n",
            "P-10886\t-0.7735 -0.4872 -0.0589 -0.0430 -0.0029 -0.0475 -0.3623 -0.0125 -1.5203 -0.3285 -0.4401 -0.8984 -0.8892 -0.3056\n",
            "S-4502\tI don't think it'll rain , but I'll take an umbrella just in case it does.\n",
            "T-4502\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie juste au cas où cela se <<unk>>\n",
            "H-4502\t-0.18971888720989227\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie au cas où.\n",
            "P-4502\t-0.0013 -0.0027 -0.0035 -0.0001 -0.0135 -0.4016 -0.0140 -0.0064 -0.0014 -0.0344 -0.2648 -0.6248 -0.3076 -0.0028 -1.0732 -0.0000 -0.6628 -0.0000\n",
            "S-319\tI thought if I broke up with you , I'd never have to see you again.\n",
            "T-319\tJe pensais que si je <<unk>> avec toi , je n'aurais plus jamais à te revoir.\n",
            "H-319\t-0.33690205216407776\tJe pensais que je me suis rompu avec vous , je ne devrais plus jamais te revoir.\n",
            "P-319\t-0.0521 -0.0873 -0.0875 -0.0606 -0.4331 -0.8581 -1.5689 -0.0440 -0.6619 -0.0101 -0.0119 -0.3247 -1.1659 -0.0622 -0.0113 -0.5181 -0.1065 0.0000\n",
            "S-5700\tIt's still too hard to find a job. And even if you have a job , chances are you're having a tougher time paying the rising costs of everything from <unk> to gas.\n",
            "T-5700\tC'est encore trop difficile de trouver un emploi. Et même quand on en a un , il y a des chances qu'on ait davantage de difficultés à payer le coût de tout , de <<unk>> au gaz.\n",
            "H-5700\t-0.8739908337593079\tIl est encore trop dur pour trouver un travail plus dur , si tu auras un emploi du cours , il est encore trop difficile pour trouver un emploi du cours , généralement si tu as un travail plus dur à trouver , il se sent encore trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur difficile à trouver un travail plus dur à trouver un travail plus difficile de trouver un travail plus trop difficile pour trouver un travail plus difficile de trouver un travail plus trop dur difficile pour trouver un travail plus trop difficile pour trouver un travail plus dur que trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop difficile pour trouver un travail plus trop difficile pour trouver un travail plus trop difficile pour trouver un travail plus difficile de trouver un cours\n",
            "P-5700\t-0.1993 -0.0755 -0.0441 -0.0008 -0.6824 -0.0013 -0.0268 -0.0290 -0.8683 -0.8634 -0.4467 -0.5205 -1.6381 -0.9219 -0.3245 -0.1826 -1.7246 -0.9664 -0.7060 -0.0662 -1.4025 -2.1477 -0.0280 -0.4266 -1.0296 -0.4348 -0.2618 -0.3816 -1.1189 -0.4601 -1.5204 -0.0280 -2.8657 -1.5154 -0.6980 -0.9804 -0.2752 -0.6244 -0.7768 -0.1344 -1.4773 -0.2036 -1.7984 -1.1900 -0.5906 -0.5995 -0.0446 -0.0242 -0.3842 -0.0023 -0.1673 -0.1975 -0.3223 -0.9403 -2.1192 -0.7292 -1.7696 -1.7746 -0.4998 -1.4110 -2.1625 -0.5779 -1.4506 -0.9009 -0.0408 -0.5996 -0.6154 -1.1891 -1.3883 -0.7294 -0.4871 -0.0245 -0.6080 -0.3650 -1.1941 -2.3786 -0.4345 -1.0024 -1.6934 -0.6262 -0.9615 -1.9116 -0.5747 -1.1028 -0.6827 -0.0044 -0.9984 -0.9070 -1.0047 -0.4072 -3.3232 -0.0110 -0.9844 -0.9677 -1.0317 -1.9410 -0.5283 -0.1558 -0.9156 -0.5205 -1.2124 -2.1166 -0.5339 -0.1733 -0.0642 -1.0886 -0.3384 -1.2032 -2.2702 -0.4365 -0.3238 -1.0195 -0.4937 -0.8061 -1.2713 -0.8274 -0.8983 -1.0881 -0.0233 -0.6998 -0.3180 -0.9749 -1.8060 -0.5431 -0.6160 -0.0151 -0.4935 -0.3338 -0.6063 -0.3329 -2.6842 -2.1728 -1.0287 -0.4526 -0.9922 -0.1462 -0.4484 -0.2650 -0.0271 -0.3111 -0.2623 -0.5943 -2.1201 -0.5848 -1.0760 -0.5757 -0.7937 -0.9818 -0.8241 -0.6922 -0.3781 -0.0598 -0.7217 -0.4220 -1.1340 -1.9679 -0.5309 -0.7283 -1.3976 -0.5195 -1.5730 -2.1692 -0.5251 -0.8088 -0.0583 -0.4335 -0.3811 -1.3320 -2.1160 -0.4203 -0.2788 -0.6031 -0.4356 -0.7868 -1.3644 -0.6821 -0.1562 -0.0492 -0.2440 -0.2867 -1.2789 -1.4226 -0.5972 -0.1257 -0.0140 -0.7763 -0.3193 -1.3244 -1.6824 -0.8195 -0.2524 -0.0558 -0.6071 -0.2501 -1.1648 -2.0000 -0.7269 -0.9189 -1.1533 -1.7881 -14.8229\n",
            "S-10885\tIt was bad enough that he usually came to work late , but coming in drunk was the last straw , and I'm going to have to let him go.\n",
            "T-10885\tC'était suffisamment grave qu'il ait l'habitude d'arriver en retard au travail , mais qu'il arrive soûl est un comble , et je vais devoir m'en séparer.\n",
            "H-10885\t-0.9117712378501892\tIl était assez mauvais qu'il arrive , mais je vais devoir le rendre en retard , mais je viens en venir , mais venir le dernier sentiment était la dernière fois , mais je vais devoir le faire , il se passe quand il a eu mauvaise assez assez pour le devoir , il se passe assez mauvais moment qu'il arrive assez mauvais temps qu'il a assez été assez mauvais moment qu'il a assez été mauvais assez assez pour qu'il a assez été assez mauvais cervelle qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été mauvais assez mauvais cervelle qu'il se passe assez assez mauvais qu'il se passe assez assez mauvais moment qu'il va passer assez mauvais moment qu'il a assez été assez assez mauvais moment qu'il a assez été assez mauvais pour qu'il ait assez été mauvais suffisant pour avoir été assez mauvais cervelle qu'il a assez été mauvais assez mauvais moment qu'il a assez été assez mauvais cervelle qu'il va passer\n",
            "P-10885\t-0.8795 -0.7719 -0.2610 -0.0754 -0.2290 -1.7481 -1.5069 -0.3784 -0.2568 -1.2775 -1.4876 -1.0437 -0.9850 -0.4461 -1.3762 -0.3969 -0.1402 -0.9076 -1.4100 -1.6200 -0.3062 -0.9385 -0.8114 -1.8089 -1.4454 -0.0391 -1.2715 -0.4829 -0.8067 -0.1037 -0.1929 -0.8399 -1.7701 -0.1531 -0.2565 -1.6808 -0.4503 -0.4471 -1.9884 -0.1946 -1.9622 -0.0827 -2.2094 -0.2236 -1.3450 -0.4099 -1.2258 -0.3209 -0.8359 -0.7274 -0.7409 -0.5788 -2.0019 -1.6660 -0.9378 -0.9901 -0.8626 -1.0075 -0.8198 -0.2510 -0.8276 -2.0100 -2.1112 -1.3387 -0.0080 -1.9424 -0.7390 -0.2457 -1.2912 -0.5564 -1.3612 -0.0850 -1.8422 -0.4602 -0.1725 -1.1585 -0.3198 -0.9475 -1.4281 -0.3171 -1.7945 -0.5167 -0.3497 -0.5968 -1.0153 -0.8896 -0.0568 -1.6312 -1.0580 -0.3387 -0.4242 -0.3859 -1.6061 -0.2797 -1.8599 -0.4645 -0.1113 -0.4921 -0.9513 -0.9958 -0.1810 -1.7384 -0.4413 -0.2007 -0.4974 -0.9118 -0.9265 -0.0962 -2.3236 -0.2490 -0.1570 -0.7642 -0.7617 -1.0500 -0.3007 -1.8877 -0.3589 -0.2671 -0.3841 -0.7059 -0.7789 -0.1584 -1.7157 -0.3484 -0.1642 -0.3326 -0.2955 -0.9537 -0.0524 -1.9718 -0.4689 -0.2891 -1.1021 -0.4632 -0.6238 -0.8710 -0.0145 -2.1836 -0.1301 -0.7407 -1.7178 -0.8574 -0.5196 -1.8504 -0.2796 -0.9253 -2.1356 -0.5362 -0.9980 -0.2050 -1.6712 -1.7028 -0.7066 -0.3384 -0.8918 -0.0820 -1.3714 -0.3534 -0.4668 -0.4688 -1.1318 -0.8996 -1.1656 -0.2095 -1.8347 -0.4185 -0.2918 -0.5738 -0.4881 -0.6509 -0.4089 -2.2441 -0.1744 -0.2888 -0.6932 -1.3254 -0.7150 -1.7931 -0.7918 -0.6190 -0.3102 -0.6429 -0.1469 -1.6150 -1.1499 -0.4301 -1.1142 -0.5552 -1.0359 -1.3461 -0.0447 -1.4285 -0.4368 -0.2287 -0.7176 -0.5481 -1.3618 -0.1008 -1.5232 -1.9589 -16.5470\n",
            "S-6030\tIn the same amount of time it would take me to correct all the mistakes in your report , I could write a better report myself.\n",
            "T-6030\tDans le même <<unk>> de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi-même.\n",
            "H-6030\t-0.8943619132041931\tDans le même somme de temps que ça me prendrait du meilleur part que je me prenne tous les erreurs , je me prendrais tout le même part que je me prenne tous les erreurs , que le même somme de temps que ça me ferait du plus grand somme de temps que cela me <unk> de le même somme de temps que cela me <unk> de faire part le même somme somme de temps qui me ferait du plus meilleur somme de temps que je me <unk> de faire part le même somme somme de temps que cela me <unk> de plus part de temps me <unk> le même somme de temps que cela me <unk> de le même somme somme de temps que le même somme somme de temps que me <unk> la même somme somme de temps que cela me permet de temps qui me ferait du meilleur somme de temps que cela me <unk> de plus part de temps qui me ferait du plus grand somme de temps que cela me <unk> de plus part de temps que me <unk> le même somme somme de temps que faire le même somme somme de temps que faire\n",
            "P-6030\t-0.4198 -0.3902 -0.0099 -0.0062 -0.0420 -0.0226 -0.0858 -0.8289 -0.3057 -0.5207 -1.5052 -0.3600 -1.0106 -1.1795 -1.3349 -0.7407 -0.8881 -1.1910 -0.3544 -0.1502 -1.0168 -0.8490 -0.1239 -1.4010 -0.1094 -0.7369 -0.1610 -0.9239 -0.6275 -2.0061 -0.1112 -0.3950 -1.2188 -0.0237 -0.0259 -0.8400 -1.6522 -1.7299 -0.0319 -0.0020 -0.7608 -0.0104 -0.4816 -0.8339 -0.6408 -0.7721 -0.5399 -1.1882 -1.5448 -0.3188 -0.1068 -0.0529 -0.0853 -2.1496 -0.1541 -0.4236 -1.2460 -2.6184 -0.5611 -0.0034 -0.7135 -0.0188 -0.1251 -1.8450 -0.1583 -0.2920 -1.3278 -2.1243 -1.1971 -2.2224 -0.0702 -0.0101 -0.8985 -0.3844 -0.0086 -2.4126 -0.4171 -0.8678 -1.5193 -1.0195 -1.0511 -0.1962 -0.1018 -0.0031 -0.1550 -2.2619 -0.8674 -0.6374 -1.7966 -1.7364 -0.7504 -1.8862 -0.1949 -0.0136 -0.7659 -0.3467 -0.0059 -0.1088 -1.8515 -0.3745 -0.6312 -1.2863 -1.4882 -1.0773 -0.9864 -0.0678 -1.9691 -1.5661 -2.2485 -0.0941 -0.8204 -1.1533 -0.0262 -0.2878 -1.8620 -0.1749 -0.3078 -1.4018 -2.4827 -1.4615 -0.0135 -1.5751 -0.5811 -0.0211 -0.1682 -2.1607 -0.0189 -0.0049 -0.3946 -0.3687 -0.0256 -0.4359 -2.1849 -0.7465 -2.6868 -0.0001 -0.0078 -0.4719 -0.5525 -0.0032 -0.0991 -2.4590 -0.2937 -2.1463 -0.2590 -0.7613 -2.4832 -1.0885 -0.8485 -1.0891 -1.1257 -0.8770 -0.2417 -0.0120 -0.1208 -2.1968 -0.3536 -0.5499 -1.9920 -2.0617 -1.3802 -1.5040 -0.5010 -2.3419 -0.8331 -0.6490 -0.9735 -1.2096 -0.8855 -0.0210 -0.1181 -0.0035 -0.1333 -1.8143 -0.3847 -0.5438 -1.1003 -1.4324 -1.1117 -0.6789 -0.1528 -0.9457 -2.3107 -0.7476 -2.0163 -0.0108 -0.1584 -0.8383 -0.4084 -0.0151 -0.1760 -2.0423 -2.6994 -0.1729 -0.0043 -0.1256 -0.6042 -0.0085 -0.3078 -2.2369 -18.7278\n",
            "S-4762\tDid you know that in Japan , if you have a tattoo , you won't be allowed to bathe in many of the hot spring <unk>\n",
            "T-4762\tSaviez-vous qu'au Japon , si vous portez un tatouage , vous ne serez pas autorisé à vous baigner dans beaucoup des <<unk>> <<unk>> ?\n",
            "H-4762\t-0.8568866848945618\tSavais-tu que au Japon , si tu as un tatouage , si tu auras un tatouage , vous ne serez pas autorisé au Japon , tu seras autorisé à le faire dans le Japon , si vous en prenez au Japon , si tu connais cela au Japon , vous ne connaissez que le journal <unk> , si tu le sais au Japon , si tu le connais le mois que le mois <unk> le Japon , vous savais-tu que le Japon , si le mois le savais-tu , vous <unk> que le mois <unk> que le mois <unk> , si tu le saviez-vous au Japon , le Japon , si vous connaissez cela au Japon , vous le savais-tu au Japon , si tu le savais-tu au Japon , si tu le sais au Japon , tu le connais que le matin , tu le connais que le journal <unk> le Japon , si tu le savais-tu au Japon , si vous savais-tu que le Japon se <unk> au Japon , si tu le saviez-vous au Japon , le mois <unk> que le matin au Japon , tu le connais le mois <unk> le Japon , si tu le\n",
            "P-4762\t-0.1411 -0.0180 -0.7747 -0.0000 -0.0026 -0.1060 -0.5136 -0.8302 -0.0493 -0.0000 -0.0342 -0.3086 -0.4027 -0.9260 -0.0096 -0.0001 -0.1629 -0.7676 -0.2285 -0.1479 -0.0261 -0.7493 -0.9010 -0.0108 -0.0274 -1.4368 -0.8707 -0.3046 -0.0583 -0.8767 -0.8884 -1.7351 -0.0282 -0.8867 -0.0214 -1.4684 -0.6303 -1.5294 -1.8109 -0.6690 -0.0024 -0.2260 -0.5794 -0.5306 -1.5087 -1.6501 -0.2538 -0.0006 -0.0154 -0.7284 -0.7703 -0.3777 -0.4151 -1.2168 -1.6700 -0.5359 -0.6030 -1.3413 -0.7384 -0.2011 -1.0841 -1.3554 -0.0011 -0.0666 -0.9878 -1.0339 -0.4064 -0.8519 -1.8746 -0.8341 -2.2376 -0.6257 -1.2949 -0.9420 -1.5260 -0.0097 -0.1981 -1.6286 -2.2153 -0.0433 -0.1307 -0.1090 -1.2495 -0.8446 -1.7409 -0.5993 -0.9486 -1.1548 -0.3819 -1.2816 -2.0307 -0.6207 -0.1633 -1.8075 -0.5146 -1.1721 -0.1811 -1.5039 -0.8093 -1.1018 -1.6357 -0.9794 -0.2754 -1.7304 -0.2280 -0.0001 -0.0335 -1.1025 -0.2742 -1.2243 -1.0266 -0.9833 -1.2809 -0.5035 -0.5150 -0.0000 -0.0281 -1.2432 -1.6744 -0.5031 -0.8803 -0.0004 -0.0917 -0.2745 -0.5294 -0.2357 -1.3126 -1.4965 -0.0005 -0.0545 -0.8595 -0.6587 -0.9213 -1.4457 -1.4512 -0.0008 -0.1196 -1.0489 -1.1188 -0.3662 -1.6728 -0.2397 -1.0517 -1.8545 -1.0930 -1.0229 -0.3136 -2.0531 -0.2528 -1.6894 -0.5409 -1.3431 -0.0079 -0.1857 -1.4287 -0.7640 -0.5051 -1.4371 -0.5368 -0.0005 -0.0347 -1.2802 -0.9907 -1.7033 -0.2209 -0.0947 -0.3357 -1.9014 -1.0626 -0.9268 -0.0004 -0.1840 -1.1918 -0.5800 -0.4783 -1.4508 -0.2426 -0.0001 -0.0425 -0.9016 -0.4491 -1.0868 -1.5717 -0.1397 -1.7557 -1.7679 -0.0052 -0.1137 -1.4076 -0.8640 -0.3596 -2.1652 -0.6337 -1.6924 -1.3177 -0.0537 -0.4548 -0.4861 -1.2687 -0.2714 -23.3635\n",
            "S-11299\tI know that it's highly unlikely that you'd ever want to go out with me , but I still need to ask at least once.\n",
            "T-11299\tJe sais qu'il est très improbable que tu veuilles jamais sortir avec moi , mais je dois quand même demander au moins une fois.\n",
            "H-11299\t-0.665751576423645\tJe sais qu'il est hautement improbable que tu veuilles déjà sortir avec moi , mais je veux déjà sortir avec moi , mais j'ai encore besoin que tu veux encore sortir avec moi , mais il faut que tu sois hautement improbable que vous vouliez qu'il soit hautement improbable que vous vouliez tout plus improbable que tu sois improbable que ce soit hautement improbable que vous vouliez qu'il soit hautement improbable que tu aies peu improbable qu'il soit hautement improbable que vous soyez improbable que c'est hautement improbable que vous <unk> il est hautement improbable que tu improbable que il soit hautement improbable que vous <unk> il est hautement improbable que tu improbable que il soit hautement improbable que vous <unk> il il est hautement improbable que vous <unk> il est hautement improbable que tu aies improbable que ce soit hautement improbable que vous <unk> qu'il est hautement improbable que tu sois peu improbable que c'est hautement improbable que vous vouliez qu'il soit hautement improbable que vous vouliez qu'il soit hautement improbable que tu aies peu improbable que ce soit hautement improbable que tu sois hautement improbable que tu sois hautement improbable que tu improbable que c'est hautement improbable que\n",
            "P-11299\t-0.0003 -0.0000 -0.2111 -0.1642 -0.0179 -0.0000 -0.0012 -1.1521 -0.4904 -0.6913 -0.1014 -0.1146 -0.0058 -0.3465 -0.0011 -0.7039 -0.3880 -2.3548 -0.3953 -0.9957 -0.0215 -0.0830 -0.0316 -1.6131 -0.8924 -0.2614 -0.7227 -0.9316 -0.8338 -1.2901 -0.1879 -0.0799 -0.0215 -0.1958 -0.0328 -1.5001 -1.5000 -0.0770 -0.9103 -0.8482 -0.3401 -0.0001 -0.0066 -0.6751 -1.6301 -1.1010 -0.7222 -0.2526 -0.0000 -0.0014 -0.5986 -0.6302 -1.6002 -1.1032 -0.2225 -0.0271 -1.0387 -0.8713 -0.7226 -0.1848 -1.0070 -0.0145 -0.0640 -0.0013 -0.0632 -0.4982 -1.5187 -1.1566 -0.6009 -0.2643 -0.0003 -0.0914 -0.9433 -1.2979 -0.5555 -0.1314 -1.4627 -0.3900 -0.1163 -0.0001 -0.0779 -0.5753 -1.6877 -1.1366 -0.5147 -1.0957 -0.0444 -0.0018 -0.0588 -0.6427 -1.6816 -0.8164 -0.7549 -0.1395 -0.0003 -0.1204 -1.0174 -1.6122 -0.2233 -0.8056 -0.4358 -0.4810 -0.0003 -0.1295 -0.5930 -0.9423 -1.2546 -0.7377 -0.2122 -0.0001 -0.0681 -1.1117 -0.8626 -0.2329 -1.1395 -0.3467 -0.5694 -0.0002 -0.1240 -0.4679 -1.5242 -1.4199 -1.6652 -0.3247 -0.3584 -0.0006 -0.0608 -0.7673 -1.2529 -0.7902 -1.1192 -0.1768 -0.0003 -0.1128 -0.9114 -0.6978 -0.8675 -0.2382 -1.4291 -0.0348 -0.2577 -0.0048 -0.0469 -0.7801 -1.1176 -1.1873 -1.0230 -0.0837 -0.0002 -0.1062 -0.9684 -1.0765 -1.6110 -0.0462 -0.1071 -1.3576 -0.1960 -0.0024 -0.0658 -0.6383 -1.8940 -1.5416 -0.4761 -0.2499 -0.0005 -0.1091 -0.5193 -1.7366 -0.7289 -0.6890 -0.1810 -0.0004 -0.1049 -0.9145 -1.7514 -0.5752 -0.1096 -0.2179 -1.0905 -0.0192 -0.3056 -0.0004 -0.0485 -1.0430 -1.5781 -0.6131 -0.0001 -0.1474 -0.8735 -1.0120 -0.4292 -0.0001 -0.0781 -1.0942 -0.3304 -0.2352 -0.6621 -0.1565 -0.0033 -0.0597 -23.0141\n",
            "S-9140\tIf you don't eat breakfast , you'll probably be hungry during the morning and won't be as efficient at work as you could be.\n",
            "T-9140\tSi tu ne <<unk>> pas , tu auras probablement faim au cours de la matinée et tu ne seras pas aussi efficace au travail que tu pourrais l'être.\n",
            "H-9140\t-0.5444705486297607\tSi tu ne manges pas le petit-déjeuner , tu seras probablement aussi efficace au travail que vous ne serez pas aussi efficace au travail que vous ne serez probablement plus efficace au travail que tu ne seras pas aussi efficace que vous ne serez pas aussi petite petit-déjeuner , vous ne serez pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , vous ne prenez pas le petit-déjeuner , tu ne seras pas aussi petite déjeuner que vous ne manges pas le petit-déjeuner , tu ne manges pas le petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi efficace que vous ne manges pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne prenez pas le petit-déjeuner , vous ne prenez pas le petit-déjeuner , vous ne serez pas le petit-déjeuner , vous ne seras pas aussi petite petit-déjeuner , vous ne prenez pas\n",
            "P-9140\t-0.0001 -0.6062 -0.0066 -0.2280 -0.0030 -0.2841 -0.0693 -0.0074 -0.8691 -0.4417 -0.0001 -0.9418 -0.0375 -0.5200 -0.0669 -0.5504 -0.8927 -0.0376 -0.0540 -0.0833 -0.0345 -0.0107 -0.3286 -0.0282 -0.5013 -1.1041 -0.0536 -0.1298 -0.6869 -0.2237 -0.0105 -0.4036 -0.2267 -0.5407 -0.6432 -0.1364 -0.1753 -0.0003 -0.0056 -0.0065 -0.5093 -0.6403 -0.1398 -1.2360 -0.2454 -0.1147 -0.9601 -0.5155 -0.0345 -0.4687 -0.2772 -0.0968 -0.3783 -0.8751 -1.7610 -0.4121 -0.0604 -0.4153 -0.5938 -1.0963 -0.0425 -1.1612 -0.1324 -0.2489 -1.0828 -0.1842 -0.1834 -0.0064 -0.5546 -0.6843 -0.7233 -0.1095 -0.4667 -0.6624 -1.3284 -0.0494 -0.2564 -0.0776 -0.1614 -1.0842 -0.1825 -0.1502 -0.0072 -0.3056 -0.6716 -0.3107 -0.7470 -0.4569 -1.3258 -0.9033 -0.0323 -0.5206 -0.1231 -0.6113 -0.6099 -0.3122 -0.9380 -0.1503 -0.1574 -0.0471 -0.9298 -1.1060 -0.1302 -0.2018 -0.0395 -1.0233 -1.0723 -0.9356 -1.2543 -0.4668 -0.1130 -0.7942 -0.0522 -0.8044 -0.1482 -0.3029 -1.1092 -0.0329 -0.7747 -0.0568 -1.1382 -0.0864 -0.2703 -0.5914 -0.3362 -0.9699 -0.0447 -1.1375 -0.1601 -0.3831 -1.0144 -0.2403 -0.1385 -0.0038 -0.3197 -0.5014 -0.4202 -0.8802 -0.4743 -0.4680 -0.8692 -0.0233 -0.8007 -0.1053 -0.2859 -0.9879 -0.0999 -0.1247 -0.0081 -0.6353 -1.4362 -1.1176 -0.5657 -0.3593 -0.5097 -0.0487 -0.9840 -0.1109 -0.2608 -1.1357 -0.2165 -0.1727 -0.0092 -0.6559 -1.3194 -0.4291 -0.6484 -0.3787 -0.2026 -0.6979 -0.1426 -0.1025 -0.0479 -0.7131 -0.5373 -0.2280 -0.9688 -0.4824 -0.1604 -0.0911 -0.6362 -0.4900 -0.4613 -1.1544 -0.0728 -1.0044 -0.1209 -0.4709 -0.5359 -0.2243 -1.2626 -0.0089 -0.1158 -0.7862 -0.3127 -0.4299 -0.4070 -0.1978 -0.8734 -0.1380 -20.5911\n",
            "S-13683\tIf you don't want to put on sunscreen , that's your problem. Just don't come complaining to me when you get a sunburn.\n",
            "T-13683\tSi tu ne veux pas mettre de crème solaire c'est ton problème , mais ne viens pas te plaindre quand <<unk>> des coups de soleil.\n",
            "H-13683\t-0.3102198541164398\tSi vous ne voulez pas me mettre de crème solaire , c'est votre problème.\n",
            "P-13683\t-0.0011 -0.8510 -0.0047 -0.0302 -0.1173 -0.6526 -0.7944 -0.0908 -0.7353 -0.0674 -0.0464 -0.2269 -0.8839 -0.1030 -0.0484\n",
            "S-13070\tDuring hard times , people might not go on a trip , but they might be willing to pay extra for good coffee.\n",
            "T-13070\tQuand les temps sont durs , les gens peuvent ne pas partir en voyage mais ils peuvent être disposés à payer davantage pour du café de bonne qualité.\n",
            "H-13070\t-0.6659844517707825\tLors des moments difficiles , les gens se pourraient <unk> , mais ils ne pourraient peut-être pas être disposés à le procurer un bon café.\n",
            "P-13070\t-1.1245 -0.1720 -1.8897 -0.3850 -0.0005 -0.0560 -0.0004 -0.5172 -1.2297 -1.1572 -0.7780 -0.1902 -0.5368 -0.8200 -1.0510 -0.2130 -0.3742 -2.1222 -1.2951 -0.0112 -1.0372 -1.8286 -0.3581 -0.0372 -0.1305 -0.0001\n",
            "S-10827\tI still have a scar on my left leg from a car accident I was in when I was thirteen years old.\n",
            "T-10827\tJ'ai encore une cicatrice sur ma jambe gauche à la suite d'un accident de voiture dans lequel j'ai été impliqué quand j'avais treize ans.\n",
            "H-10827\t-0.3881998062133789\tJ'ai encore une cicatrice sur mon foyer à la jambe gauche , j'ai eu treize ans.\n",
            "P-10827\t-0.0196 -0.2383 -0.0290 0.0000 -0.0006 -0.4542 -1.6898 -0.4478 -0.3633 -0.0082 -0.0207 -1.4627 -0.2635 -1.1911 -0.0044 -0.3981 -0.0081\n",
            "S-13283\tI never see a library without wishing I had time to go there and stay till I had read everything in it.\n",
            "T-13283\tJe ne vois jamais une bibliothèque sans souhaiter avoir le temps de m'y rendre et d'y rester jusqu'à ce que j'y aie tout lu.\n",
            "H-13283\t-0.6584036946296692\tJe ne vois jamais de la bibliothèque , sans <unk> que je devais y aller et rester là.\n",
            "P-13283\t-0.0273 -0.3658 -0.0324 -0.0001 -0.7974 -0.3032 -0.0006 -0.7175 -1.4608 -0.0349 -2.3083 -0.4873 -1.5329 -0.3907 -0.0490 -0.3536 -1.3084 -2.1974 -0.1421\n",
            "S-4837\tPolice are <unk> people not to pick up <unk> as they search for two prisoners on the run after escaping from jail.\n",
            "T-4837\tLa police <<unk>> les gens à ne pas prendre <<unk>> car ils recherchent deux prisonniers en cavale après leur <<unk>>\n",
            "H-4837\t-0.7876145839691162\tLa police sont <unk> que les gens <unk> de ne pas prendre les <unk> après les deux prisonniers se termine au cours de la prison.\n",
            "P-4837\t-0.0042 -0.0002 -0.3517 -0.3193 -0.5976 -0.0242 -0.0035 -0.5643 -1.2016 -1.0463 -0.3536 -1.1217 -2.2955 -1.3131 -0.4886 -1.8913 -0.9603 -0.7516 -1.3018 -0.4938 -1.3968 -0.1275 -1.2519 -0.5820 -2.0045 -0.0310\n",
            "S-248\tWhen we started out <unk> web pages , we were the only ones doing it in this part of the country.\n",
            "T-248\tQuand nous avons commencé à concevoir des pages web , nous étions les seuls à le faire dans cette partie du pays.\n",
            "H-248\t-0.5569350123405457\tQuand nous avons commencé à la <unk> la toile , nous avons été la seule à le faire dans cette partie du pays.\n",
            "P-248\t-0.3438 -0.0437 -0.2326 -0.1162 -0.5858 -1.1468 -0.4267 -0.8433 -0.4818 -0.0090 -0.0163 -0.8179 -0.2042 -0.8271 -0.0060 -0.2868 -1.5166 -0.4182 -1.9418 -0.3057 -0.0118 -1.0455 -1.7391 0.0000\n",
            "S-11828\tI was looking forward to seeing a scenic view of Mt. Fuji , but unfortunately it was completely hidden behind clouds.\n",
            "T-11828\tJ'avais hâte de voir une vue <<unk>> du mont Fuji mais , malheureusement , il était complètement caché par les nuages.\n",
            "H-11828\t-1.1627466678619385\tJ'étais impatient de voir une beauté en masse vue en doute , mais les <unk> , mais les nuages en <unk>\n",
            "P-11828\t-0.9874 -0.4570 -0.0358 -0.0574 -0.1296 -0.4456 -0.9748 -1.1766 -1.6473 -1.3455 -2.5917 -1.9532 -1.4369 -2.0317 -2.3442 -1.9019 -0.0403 -1.9964 -0.8819 -1.3586 -1.5542 -0.2324\n",
            "S-7610\tI tried to stay inside the house all day , but I ended up going outside and sitting in the garden.\n",
            "T-7610\tJ'ai essayé de rester dans la maison toute la journée , mais j'ai fini par sortir m'asseoir dans le jardin.\n",
            "H-7610\t-0.3343660533428192\tJ'ai essayé de rester à l'intérieur toute la maison , mais j'ai fini par sortir au jardin.\n",
            "P-7610\t-0.0420 -0.0419 -0.0019 -0.0068 -0.9891 -0.0007 -0.0241 -0.0027 -0.0113 -0.4101 -0.0116 -0.0807 -0.1798 -0.1800 -1.5062 -2.5201 -0.0094 -0.0001\n",
            "S-6513\t<unk> told police that the train was <unk> way over the speed limit when it <unk> going around a <unk>\n",
            "T-6513\tDes <<unk>> ont déclaré à la police que le train roulait bien au-dessus de la limite de vitesse lorsqu'il a <<unk>> dans un <<unk>>\n",
            "H-6513\t-0.7337239980697632\tOn a dit à la police que le train était de la <unk> quand il se passe par la <unk>\n",
            "P-6513\t-1.1468 -1.1832 -0.0052 -0.1554 -0.1037 -0.0063 -0.7600 -0.3462 -0.0000 -0.5380 -2.0542 -0.3994 -1.0192 -1.0681 -0.2810 -0.4739 -1.2200 -2.1453 -0.9839 -0.1694 -1.3490\n",
            "S-10561\tI suppose you want to ask me how I was able to make so much money in so little time.\n",
            "T-10561\tJe suppose que vous voulez me demander comment j'ai été en mesure de me faire autant d'argent en si peu de temps.\n",
            "H-10561\t-0.5618663430213928\tJe suppose que tu veux que tu veuilles me demander en tant que je puisse gagner tant d'argent aussi peu de temps.\n",
            "P-10561\t-0.0004 -0.0016 -0.0013 -0.7139 -0.0039 -1.1541 -0.6853 -0.8353 -0.0175 -0.2829 -1.5293 -0.0269 -0.0805 -0.7608 -1.9327 -1.1334 -0.7559 -0.5250 -0.9175 -0.6519 -0.4835 -0.4288 -0.0009\n",
            "| Translated 14928 sentences (118330 tokens) in 37.5s (397.58 sentences/s, 3151.53 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 42.36, 64.9/47.8/36.5/28.4 (BP=1.000, ratio=1.025, syslen=103402, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0KEV7dRlFo3",
        "colab_type": "text"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1gkevAlYJa",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeKee0tlFHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6a2fcfe-f720-4dc3-bb52-b4fd34fd1275"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/ \\\n",
        "     --clip-norm 0.0 --batch-size 128 \\\n",
        "     --arch transformer --max-epoch 15  \\\n",
        "     --save-dir checkpoints/transformer \\\n",
        "     --optimizer adam  --dropout 0.3 \\\n",
        "     --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
        "     --no-epoch-checkpoints --num-workers 3 \\"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=15, max_sentences=128, max_sentences_valid=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(23320, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(25000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion CrossEntropyCriterion\n",
            "| num. model params: 81678336 (num. trained: 81678336)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 128\n",
            "| no existing checkpoint found checkpoints/transformer/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 001:   0% 0/1050 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "| epoch 001 | loss 9.003 | ppl 513.1 | wps 5343 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 1050 | lr 0.000131324 | gnorm 3.207 | clip 0.000 | oom 0.000 | wall 197 | train_wall 192\n",
            "| epoch 001 | valid on 'valid' subset | loss 6.458 | ppl 87.92 | num_updates 1050\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 1 @ 1050 updates) (writing took 16.413809061050415 seconds)\n",
            "| epoch 002 | loss 5.817 | ppl 56.37 | wps 5275 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 2100 | lr 0.000262548 | gnorm 2.142 | clip 0.000 | oom 0.000 | wall 423 | train_wall 386\n",
            "| epoch 002 | valid on 'valid' subset | loss 4.890 | ppl 29.65 | num_updates 2100 | best_loss 4.88993\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 2 @ 2100 updates) (writing took 18.231640100479126 seconds)\n",
            "| epoch 003 | loss 4.585 | ppl 24.01 | wps 5276 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 3150 | lr 0.000393771 | gnorm 1.932 | clip 0.000 | oom 0.000 | wall 651 | train_wall 580\n",
            "| epoch 003 | valid on 'valid' subset | loss 3.882 | ppl 14.74 | num_updates 3150 | best_loss 3.88173\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 3 @ 3150 updates) (writing took 21.922940254211426 seconds)\n",
            "| epoch 004 | loss 3.782 | ppl 13.75 | wps 5274 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 4200 | lr 0.00048795 | gnorm 1.837 | clip 0.000 | oom 0.000 | wall 882 | train_wall 774\n",
            "| epoch 004 | valid on 'valid' subset | loss 3.312 | ppl 9.93 | num_updates 4200 | best_loss 3.31174\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 4 @ 4200 updates) (writing took 17.664696216583252 seconds)\n",
            "| epoch 005 | loss 3.118 | ppl 8.68 | wps 5268 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 5250 | lr 0.000436436 | gnorm 1.744 | clip 0.000 | oom 0.000 | wall 1109 | train_wall 968\n",
            "| epoch 005 | valid on 'valid' subset | loss 2.893 | ppl 7.43 | num_updates 5250 | best_loss 2.893\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 5 @ 5250 updates) (writing took 21.560385942459106 seconds)\n",
            "| epoch 006 | loss 2.612 | ppl 6.11 | wps 5255 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 6300 | lr 0.00039841 | gnorm 1.710 | clip 0.000 | oom 0.000 | wall 1341 | train_wall 1163\n",
            "| epoch 006 | valid on 'valid' subset | loss 2.675 | ppl 6.39 | num_updates 6300 | best_loss 2.67543\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 6 @ 6300 updates) (writing took 19.513208150863647 seconds)\n",
            "| epoch 007 | loss 2.273 | ppl 4.83 | wps 5244 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 7350 | lr 0.000368856 | gnorm 1.693 | clip 0.000 | oom 0.000 | wall 1571 | train_wall 1357\n",
            "| epoch 007 | valid on 'valid' subset | loss 2.570 | ppl 5.94 | num_updates 7350 | best_loss 2.57038\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 7 @ 7350 updates) (writing took 21.822784423828125 seconds)\n",
            "| epoch 008 | loss 2.017 | ppl 4.05 | wps 5255 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 8400 | lr 0.000345033 | gnorm 1.675 | clip 0.000 | oom 0.000 | wall 1803 | train_wall 1552\n",
            "| epoch 008 | valid on 'valid' subset | loss 2.439 | ppl 5.42 | num_updates 8400 | best_loss 2.43937\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 8 @ 8400 updates) (writing took 18.20872473716736 seconds)\n",
            "| epoch 009 | loss 1.817 | ppl 3.52 | wps 5237 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 9450 | lr 0.0003253 | gnorm 1.643 | clip 0.000 | oom 0.000 | wall 2031 | train_wall 1748\n",
            "| epoch 009 | valid on 'valid' subset | loss 2.381 | ppl 5.21 | num_updates 9450 | best_loss 2.3813\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 9 @ 9450 updates) (writing took 17.615625619888306 seconds)\n",
            "| epoch 010 | loss 1.660 | ppl 3.16 | wps 5249 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 10500 | lr 0.000308607 | gnorm 1.633 | clip 0.000 | oom 0.000 | wall 2261 | train_wall 1942\n",
            "| epoch 010 | valid on 'valid' subset | loss 2.337 | ppl 5.05 | num_updates 10500 | best_loss 2.33733\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 10 @ 10500 updates) (writing took 17.901489973068237 seconds)\n",
            "| epoch 011 | loss 1.527 | ppl 2.88 | wps 5246 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 11550 | lr 0.000294245 | gnorm 1.606 | clip 0.000 | oom 0.000 | wall 2489 | train_wall 2137\n",
            "| epoch 011 | valid on 'valid' subset | loss 2.302 | ppl 4.93 | num_updates 11550 | best_loss 2.30205\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 11 @ 11550 updates) (writing took 20.31584668159485 seconds)\n",
            "| epoch 012 | loss 1.420 | ppl 2.68 | wps 5244 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 12600 | lr 0.000281718 | gnorm 1.609 | clip 0.000 | oom 0.000 | wall 2720 | train_wall 2333\n",
            "| epoch 012 | valid on 'valid' subset | loss 2.275 | ppl 4.84 | num_updates 12600 | best_loss 2.27486\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 12 @ 12600 updates) (writing took 20.878103017807007 seconds)\n",
            "| epoch 013 | loss 1.326 | ppl 2.51 | wps 5241 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 13650 | lr 0.000270666 | gnorm 1.600 | clip 0.000 | oom 0.000 | wall 2951 | train_wall 2528\n",
            "| epoch 013 | valid on 'valid' subset | loss 2.292 | ppl 4.9 | num_updates 13650 | best_loss 2.27486\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 13 @ 13650 updates) (writing took 3.677222490310669 seconds)\n",
            "| epoch 014 | loss 1.241 | ppl 2.36 | wps 5245 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 14700 | lr 0.00026082 | gnorm 1.571 | clip 0.000 | oom 0.000 | wall 3165 | train_wall 2723\n",
            "| epoch 014 | valid on 'valid' subset | loss 2.241 | ppl 4.73 | num_updates 14700 | best_loss 2.24101\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 14 @ 14700 updates) (writing took 17.22724723815918 seconds)\n",
            "| epoch 015 | loss 1.170 | ppl 2.25 | wps 5250 | ups 5 | wpb 996.039 | bsz 127.951 | num_updates 15750 | lr 0.000251976 | gnorm 1.566 | clip 0.000 | oom 0.000 | wall 3393 | train_wall 2918\n",
            "| epoch 015 | valid on 'valid' subset | loss 2.263 | ppl 4.8 | num_updates 15750 | best_loss 2.24101\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 15 @ 15750 updates) (writing took 3.612398624420166 seconds)\n",
            "| done training in 3406.3 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaDq40utlqSb",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8tFMAvvlh-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a435cfac-5aef-41c7-e521-378e34596be5"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/transformer/checkpoint_best.pt \\\n",
        "    --batch-size 64 --beam 3 > transformer.out"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0% 0/234 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCMPZrcyl4SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bfbd9bd3-0fbe-44cd-eeeb-e0320107cc46"
      },
      "source": [
        "!grep ^H transformer.out | cut -f3- > transformer.out.sys\n",
        "!grep ^T transformer.out | cut -f2- > transformer.out.ref\n",
        "!fairseq-score --sys transformer.out.sys --ref transformer.out.ref --ignore-case"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='transformer.out.ref', sacrebleu=False, sentence_bleu=False, sys='transformer.out.sys')\n",
            "BLEU4 = 31.42, 56.7/36.9/25.7/18.2 (BP=1.000, ratio=1.028, syslen=103704, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svC90M2Jrz6p",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqPzUXQGl7vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c090ea3-21c3-4d87-abb4-8f95592361d3"
      },
      "source": [
        "!tail -n 100 transformer.out"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H-10523\t-0.660709798336029\tTout à coup de suite , le monde va à l'école à la maison.\n",
            "P-10523\t-0.7438 -1.0061 -0.0801 -1.0465 -1.0056 -0.1050 -0.3252 -0.2197 -0.3327 -0.4542 -1.4129 -0.8987 -1.5607 -0.7186 -0.0007\n",
            "S-4980\tThe only reason Tom went to the party was that he expected Mary to be there.\n",
            "T-4980\tLa seule raison pour laquelle Tom est venu à la fête était qu'il espérait que Marie soit <<unk>>\n",
            "H-4980\t-0.6203224062919617\tLa police était juste là à Tom qu'il était censé arriver.\n",
            "P-4980\t-0.3401 -0.1214 -0.3214 -1.1051 -1.1556 -0.7409 -0.1118 -0.5973 -1.0125 -0.7943 -1.1436 -0.0000\n",
            "S-9787\tIt's no wonder Tom's sleeping <unk> he drinks up to twelve cups of coffee a day.\n",
            "T-9787\tCe n'est pas surprenant que Tom dorme mal , il boit jusqu'à douze tasses de café par jour.\n",
            "H-9787\t-0.8093833327293396\tIl fait un peu chaud pour parler aux <unk>\n",
            "P-9787\t-0.3578 -1.6783 -0.5947 -0.7381 -0.2007 -0.4578 -1.8895 -0.8690 -0.6627 -0.6453\n",
            "S-10808\tThough it was a muggy night , she went to bed with all the windows closed.\n",
            "T-10808\tBien que ce fût une nuit <<unk>> , elle est allée au lit avec toutes les fenêtres fermées.\n",
            "H-10808\t-0.6616648435592651\tAyant été <unk> hier soir , la nuit passée était une nuit ouverte.\n",
            "P-10808\t-0.4413 -1.4698 -0.3995 -1.1076 -0.1399 -0.2089 -0.4159 -0.0387 -0.2791 -0.7498 -0.9866 -1.4479 -1.5708 -0.0074\n",
            "S-6263\tYou're the only person I know besides me who would prefer to live in a tent.\n",
            "T-6263\tTu es la seule personne que je connaisse , à part moi , qui préférerais vivre dans une tente.\n",
            "H-6263\t-0.7880333662033081\tVous êtes la seule personne qui me reste dans un seul endroit que moi à me connaisse.\n",
            "P-6263\t-0.8254 -0.2453 -0.2475 -0.0814 -0.0093 -1.0094 -0.9006 -2.0449 -0.7227 -1.3044 -0.6458 -0.2972 -1.6170 -0.4406 -1.0314 -1.9205 -0.8406 -0.0008\n",
            "S-12129\tI thought I was about to be captured so I ran as fast as I could.\n",
            "T-12129\tJe pensais que j'étais sur le point d'être capturée , alors je courus aussi vite que je le pus.\n",
            "H-12129\t-0.33167165517807007\tJe pensais que je pouvais courir aussi vite que je le pus.\n",
            "P-12129\t-0.4337 -0.2774 -0.3145 -0.2557 -0.1580 -1.5252 -0.1804 -0.0354 -0.0091 -0.2447 -0.7471 -0.1305 -0.0001\n",
            "S-10886\tPlease don't make me laugh. I did too many sit-ups yesterday and my stomach muscles hurt.\n",
            "T-10886\tS'il te plait ne me fais pas rire. J'ai fait trop <<unk>> hier et mes muscles <<unk>> me font mal.\n",
            "H-10886\t-0.8258835673332214\tS'il vous plaît , <unk> pas très fort et j'ai fait mes erreurs.\n",
            "P-10886\t-0.5337 -0.7450 -0.0481 -0.1963 -1.0967 -1.1009 -0.3299 -1.8036 -1.4967 -1.2654 -1.2458 -0.4222 -1.2506 -0.0274\n",
            "S-4502\tI don't think it'll rain , but I'll take an umbrella just in case it does.\n",
            "T-4502\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie juste au cas où cela se <<unk>>\n",
            "H-4502\t-0.46182501316070557\tJe ne pense pas qu'il va pleuvoir , mais je vais pleuvoir au cas où.\n",
            "P-4502\t-0.0066 -0.0584 -0.2840 -0.1253 -0.2741 -1.3927 -0.0360 -0.1932 -0.1483 -0.2496 -1.2573 -0.6574 -2.2362 -0.4315 -0.0384 -0.0003\n",
            "S-319\tI thought if I broke up with you , I'd never have to see you again.\n",
            "T-319\tJe pensais que si je <<unk>> avec toi , je n'aurais plus jamais à te revoir.\n",
            "H-319\t-0.48258328437805176\tJe pensais vous avoir jamais vues alors je ne vous ai jamais vue.\n",
            "P-319\t-0.0335 -0.5502 -1.0198 -0.0725 -0.0380 -1.6131 -0.1964 -0.0122 -0.0938 -1.5284 -0.6084 -0.0283 -0.9612 -0.0005\n",
            "S-5700\tIt's still too hard to find a job. And even if you have a job , chances are you're having a tougher time paying the rising costs of everything from <unk> to gas.\n",
            "T-5700\tC'est encore trop difficile de trouver un emploi. Et même quand on en a un , il y a des chances qu'on ait davantage de difficultés à payer le coût de tout , de <<unk>> au gaz.\n",
            "H-5700\t-1.0537848472595215\tIl est tout ce que tu as à faire si ça est si difficile pour achever le cas que tu le cas de payer un locuteur natif aussi dur que tu le cas de toutes les élections.\n",
            "P-5700\t-1.4825 -0.0534 -1.6712 -0.6681 -0.7027 -0.8978 -0.3382 -0.2706 -0.7659 -0.6900 -1.5570 -0.5545 -0.7828 -0.8549 -1.4300 -1.6014 -0.1464 -1.1634 -1.4872 -0.7246 -0.9573 -1.0655 -1.5182 -1.3628 -1.6308 -1.8146 -0.1982 -2.1685 -1.7147 -0.4958 -1.5129 -0.5545 -0.9654 -1.6976 -1.9517 -0.0259 -2.5477 -0.0194\n",
            "S-10885\tIt was bad enough that he usually came to work late , but coming in drunk was the last straw , and I'm going to have to let him go.\n",
            "T-10885\tC'était suffisamment grave qu'il ait l'habitude d'arriver en retard au travail , mais qu'il arrive soûl est un comble , et je vais devoir m'en séparer.\n",
            "H-10885\t-0.9273435473442078\tComme je me suis levé tard , il était déterminé que je ne pouvais pas le faire car même tôt.\n",
            "P-10885\t-1.5473 -1.0743 -0.9549 -0.6340 -0.1929 -0.5350 -0.3067 -0.2312 -0.7228 -2.1498 -0.6699 -0.0925 -1.4508 -1.5125 -1.0868 -1.6040 -0.8170 -1.5783 -1.0984 -1.1929 -0.0222\n",
            "S-6030\tIn the same amount of time it would take me to correct all the mistakes in your report , I could write a better report myself.\n",
            "T-6030\tDans le même <<unk>> de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi-même.\n",
            "H-6030\t-1.1248655319213867\tDonne-moi tout ce qui me ferait payer , je vous faudra payer dans le mieux que tout le monde spéciale spéciale spéciale spéciale ?\n",
            "P-6030\t-1.0246 -0.3739 -0.6292 -0.5947 -1.1336 -1.0091 -1.6782 -1.3867 -1.5500 -1.8853 -1.4396 -0.6624 -1.6171 -0.5924 -0.9553 -0.4994 -1.1277 -1.3932 -1.2669 -1.2585 -1.2492 -1.2454 -1.5041 -1.9995 -0.0458\n",
            "S-4762\tDid you know that in Japan , if you have a tattoo , you won't be allowed to bathe in many of the hot spring <unk>\n",
            "T-4762\tSaviez-vous qu'au Japon , si vous portez un tatouage , vous ne serez pas autorisé à vous baigner dans beaucoup des <<unk>> <<unk>> ?\n",
            "H-4762\t-0.9143826961517334\tTu as le droit de savoir si tu as été un parcours au Japon , mais il n'y a pas eu autant de dégâts pour atteindre un tremblement de terre.\n",
            "P-4762\t-0.5382 -0.7202 -2.0365 -0.3938 -0.7910 -1.6994 -0.3471 -0.2039 -2.0386 -0.9511 -0.5854 -1.7122 -1.2880 -0.1839 -0.7062 -0.6184 -1.1672 -0.9642 -0.0232 -0.1500 -1.5406 -0.6159 -1.0722 -0.8371 -2.5163 -0.8522 -0.6810 -1.2688 -0.6676 -1.1611 -0.0146\n",
            "S-11299\tI know that it's highly unlikely that you'd ever want to go out with me , but I still need to ask at least once.\n",
            "T-11299\tJe sais qu'il est très improbable que tu veuilles jamais sortir avec moi , mais je dois quand même demander au moins une fois.\n",
            "H-11299\t-0.700485110282898\tJe sais juste si tu as envie de le faire , mais c'est juste que tu ne le sentiras nulle part.\n",
            "P-11299\t-0.3039 -0.3224 -0.6294 -0.3587 -0.4590 -0.4812 -0.6439 -0.5005 -0.3757 -0.8258 -1.3074 -0.0482 -1.0551 -0.9658 -2.5505 -0.9777 -0.8636 -1.6345 -0.4284 -0.3524 -0.3254 -0.0013\n",
            "S-9140\tIf you don't eat breakfast , you'll probably be hungry during the morning and won't be as efficient at work as you could be.\n",
            "T-9140\tSi tu ne <<unk>> pas , tu auras probablement faim au cours de la matinée et tu ne seras pas aussi efficace au travail que tu pourrais l'être.\n",
            "H-9140\t-0.6870004534721375\tSi tu manges ce matin , tu seras probablement en train d'attraper le travail pendant que tu ne <unk>\n",
            "P-9140\t-0.0108 -0.6006 -0.5759 -0.8174 -0.5628 -0.0993 -0.1885 -1.5783 -0.5348 -1.4726 -0.0182 -0.3969 -0.7590 -1.2061 -2.4708 -0.0323 -0.0941 -0.4589 -1.0158 -0.8472\n",
            "S-13683\tIf you don't want to put on sunscreen , that's your problem. Just don't come complaining to me when you get a sunburn.\n",
            "T-13683\tSi tu ne veux pas mettre de crème solaire c'est ton problème , mais ne viens pas te plaindre quand <<unk>> des coups de soleil.\n",
            "H-13683\t-0.6603419184684753\tSi tu ne veux pas sortir , tu me <unk> un mot.\n",
            "P-13683\t-0.0529 -0.5222 -0.0559 -0.4891 -0.0143 -0.7942 -1.0841 -1.5841 -0.6879 -0.9956 -1.1053 -1.1660 -0.0328\n",
            "S-13070\tDuring hard times , people might not go on a trip , but they might be willing to pay extra for good coffee.\n",
            "T-13070\tQuand les temps sont durs , les gens peuvent ne pas partir en voyage mais ils peuvent être disposés à payer davantage pour du café de bonne qualité.\n",
            "H-13070\t-1.0736287832260132\tDans quelques jours , les gens pouvaient faire du bon temps pour ne pas pouvoir pouvoir pouvoir aller en faire des autres.\n",
            "P-13070\t-1.5846 -1.1734 -0.7397 -0.3394 -1.0500 -0.3470 -0.5304 -1.2694 -0.9046 -1.2465 -0.3493 -0.1530 -2.0962 -0.1252 -0.2393 -1.7076 -1.8479 -2.5246 -1.4622 -2.1589 -1.1544 -1.6893 -0.0004\n",
            "S-10827\tI still have a scar on my left leg from a car accident I was in when I was thirteen years old.\n",
            "T-10827\tJ'ai encore une cicatrice sur ma jambe gauche à la suite d'un accident de voiture dans lequel j'ai été impliqué quand j'avais treize ans.\n",
            "H-10827\t-0.630365788936615\tJ'ai déjà failli fermer ma voiture quand j'avais déjà treize ans.\n",
            "P-10827\t-0.0816 -0.3472 -1.6342 -1.5442 -0.8712 -0.2604 -1.0331 -0.0552 -0.1178 -0.9841 -0.6135 -0.0220\n",
            "S-13283\tI never see a library without wishing I had time to go there and stay till I had read everything in it.\n",
            "T-13283\tJe ne vois jamais une bibliothèque sans souhaiter avoir le temps de m'y rendre et d'y rester jusqu'à ce que j'y aie tout lu.\n",
            "H-13283\t-0.9816669225692749\tJe ne suis jamais allé en train de manger une fois à moitié près chaque que je ne le <unk> et écrive toute seule.\n",
            "P-13283\t-0.1020 -2.1396 -0.5721 -0.2136 -0.3987 -1.2620 -0.5354 -0.1069 -0.6928 -2.2996 -0.4596 -2.0309 -1.0423 -1.3497 -1.4253 -1.0784 -0.2993 -1.1470 -1.4534 -1.5965 -0.6375 -1.3927 -1.9754 -0.3288 -0.0020\n",
            "S-4837\tPolice are <unk> people not to pick up <unk> as they search for two prisoners on the run after escaping from jail.\n",
            "T-4837\tLa police <<unk>> les gens à ne pas prendre <<unk>> car ils recherchent deux prisonniers en cavale après leur <<unk>>\n",
            "H-4837\t-0.9933761954307556\tLes sauveteurs <unk> des <unk> pour empêcher le <unk>\n",
            "P-4837\t-0.0305 -1.4717 -0.5789 -1.4667 -1.0294 -1.2115 -0.9469 -0.4143 -2.1355 -0.6485\n",
            "S-248\tWhen we started out <unk> web pages , we were the only ones doing it in this part of the country.\n",
            "T-248\tQuand nous avons commencé à concevoir des pages web , nous étions les seuls à le faire dans cette partie du pays.\n",
            "H-248\t-0.903241753578186\tQuand tout ce document , le travail avait commencé à finir par des <unk>\n",
            "P-248\t-0.1447 -2.3372 -0.2649 -1.2058 -0.7094 -0.6741 -0.7537 -1.2570 -0.5570 -0.0310 -0.6802 -1.4138 -0.8307 -1.5324 -1.1568\n",
            "S-11828\tI was looking forward to seeing a scenic view of Mt. Fuji , but unfortunately it was completely hidden behind clouds.\n",
            "T-11828\tJ'avais hâte de voir une vue <<unk>> du mont Fuji mais , malheureusement , il était complètement caché par les nuages.\n",
            "H-11828\t-0.9316090941429138\tJe me suis senti <unk> , mais la colline était un signe de terre.\n",
            "P-11828\t-0.8209 -0.7979 -0.0329 -1.2455 -1.9950 -1.1061 -0.6722 -0.8096 -0.6141 -0.6909 -1.9364 -1.1147 -0.8930 -1.2329 -0.0120\n",
            "S-7610\tI tried to stay inside the house all day , but I ended up going outside and sitting in the garden.\n",
            "T-7610\tJ'ai essayé de rester dans la maison toute la journée , mais j'ai fini par sortir m'asseoir dans le jardin.\n",
            "H-7610\t-0.627602756023407\tJ'ai commencé à manger à la maison toute la journée , et ai commencé à courir.\n",
            "P-7610\t-0.1237 -1.1673 -0.0213 -1.6257 -1.1764 -0.0660 -0.0027 -1.2326 -0.1613 -0.3873 -0.3713 -1.0623 -1.8064 -0.3087 -0.0776 -1.0788 -0.0001\n",
            "S-6513\t<unk> told police that the train was <unk> way over the speed limit when it <unk> going around a <unk>\n",
            "T-6513\tDes <<unk>> ont déclaré à la police que le train roulait bien au-dessus de la limite de vitesse lorsqu'il a <<unk>> dans un <<unk>>\n",
            "H-6513\t-0.9148429036140442\tL'agent de police a commencé à <unk> le champ par un coup de terre.\n",
            "P-6513\t-1.4606 -0.1617 -0.1078 -1.5685 -1.5429 -0.5536 -0.6208 -1.2335 -1.3733 -0.7645 -1.1945 -0.8457 -0.3013 -1.9897 -0.0043\n",
            "S-10561\tI suppose you want to ask me how I was able to make so much money in so little time.\n",
            "T-10561\tJe suppose que vous voulez me demander comment j'ai été en mesure de me faire autant d'argent en si peu de temps.\n",
            "H-10561\t-0.5682231187820435\tJe suppose que tu me demandais si je pouvais faire ça à la dernière époque.\n",
            "P-10561\t-0.0134 -0.0272 -0.0142 -0.6929 -0.7736 -1.0063 -0.0829 -0.6990 -0.1527 -0.5131 -0.3865 -1.6444 -1.1942 -1.1182 -0.7604 -0.0126\n",
            "| Translated 14928 sentences (118632 tokens) in 66.5s (224.56 sentences/s, 1784.53 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 31.25, 56.1/36.7/25.6/18.1 (BP=1.000, ratio=1.028, syslen=103704, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6GxtbQx3DRI",
        "colab_type": "text"
      },
      "source": [
        "# Plottings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmRrWB_d3WjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_eRRR43jha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(loss_train, loss_val):\n",
        "    epochs = range(1, len(loss_train) + 1)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "    plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_m3yFqK3E6R",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oMxspblr1LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_train_loss = [5.564, 3.341, 2.675, 2.306, 2.069, 1.897, 1.768, 1.665, 1.588, 1.518, 1.456, 1.408, 1.362, 1.324, 1.287]\n",
        "lstm_valid_loss = [3.386, 2.518, 2.231, 2.030, 1.929, 1.858, 1.836, 1.797, 1.762, 1.745, 1.765, 1.732, 1.723, 1.731, 1.747]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VfiYkXe3Y__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "39f919bd-99f4-4a84-c9e7-659a5c930820"
      },
      "source": [
        "plot(lstm_train_loss, lstm_valid_loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_edvx0N35PwP",
        "colab_type": "text"
      },
      "source": [
        "## CNN Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Aty83r3vm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_train_loss = [5.623, 3.201, 2.453, 2.037, 1.780, 1.596, 1.460, 1.352, 1.269, 1.198, 1.134, 1.083, 1.034, 0.996, 0.959]\n",
        "cnn_valid_loss = [3.422, 2.491, 2.186, 2.022, 1.955, 1.904, 1.840, 1.814, 1.783, 1.798, 1.780, 1.779, 1.795, 1.787, 1.768]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uU19kbZ5eGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "bd039269-21e0-4c69-db7b-ec939617fac3"
      },
      "source": [
        "plot(cnn_train_loss, cnn_valid_loss)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PbVad6559Ea",
        "colab_type": "text"
      },
      "source": [
        "## Transformer Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL3r4vUN5ida",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_train_loss = [9.003, 5.817, 4.585, 3.782, 3.118, 2.612, 2.273, 2.017, 1.817, 1.660, 1.527, 1.420, 1.326, 1.241, 1.170]\n",
        "transformer_valid_loss = [6.458, 4.890, 3.882, 3.312, 2.893, 2.675, 2.570, 2.439, 2.381, 2.337, 2.302, 2.275, 2.292, 2.241, 2.263]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnUx6hpT6n5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c5656c3a-bca1-46ad-c45f-4c94d1e210fa"
      },
      "source": [
        "plot(transformer_train_loss, transformer_valid_loss)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAFNCAYAAAAtnkrkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3hUVf7H8fdJIZUaQg1dehKCBERRBFRUQBZZEZBgW0VdV8WyLqvrylZ3rdhd9GejCWIFUUGRKqj0gKCotFBCCEkgJIGU8/vjJjFAIEOYm0n5vJ5nnkzu3Hu+ZybPymfPOXOusdYiIiIiIt7h5+sOiIiIiFQnClciIiIiXqRwJSIiIuJFClciIiIiXqRwJSIiIuJFClciIiIiXqRwJVIDGGM+Ncbc4O1zfckYs90Yc6kL7S4yxtxS+HyMMWa+J+eWo05LY0ymMca/vH09TdvWGHOOt9sVEc8oXIlUUoX/8BY9Cowx2SV+H3MmbVlrr7TWvuXtcysjY8wEY8ySUo43NMYcM8ZEe9qWtXaatXagl/p1XBi01u601oZba/O90b6IVB4KVyKVVOE/vOHW2nBgJ3BViWPTis4zxgT4rpeV0lTgAmNMmxOOjwISrbUbfdAnEalBFK5EqhhjTD9jTJIx5k/GmH3AG8aY+saYucaYFGNMWuHzqBLXlJzqutEYs8wY82ThuduMMVeW89w2xpglxpjDxpgvjDEvGmOmnqLfnvTxH8aY5YXtzTfGNCzx+lhjzA5jTKox5uFTfT7W2iRgITD2hJeuB94uqx8n9PlGY8yyEr9fZozZYozJMMa8AJgSr7Uzxiws7N8BY8w0Y0y9wtemAC2BOYUjjw8aY1oXTt8FFJ7TzBjzsTHmoDHmJ2PMrSXanmiMmWWMebvws9lkjIk/1WdwwnuoW3hdSuHn9xdjjF/ha+cYYxYXvp8DxpiZhceNMeYZY8x+Y8whY0zimYz4idR0ClciVVMToAHQChiH87/lNwp/bwlkAy+c5vrzgB+AhsDjwP8ZY0w5zp0OfAtEABM5OdCU5EkfrwNuAhoBtYAHAIwxXYCXC9tvVliv1EBU6K2SfTHGdATiCvt7pp9VURsNgfeBv+B8Fj8DfUqeAjxW2L/OQAuczwRr7ViOH318vJQS7wBJhddfA/zbGDOgxOtDC8+pB3zsSZ8LPQ/UBdoCF+OEzJsKX/sHMB+oj/N5Pl94fCDQF+hQeO21QKqH9URqPIUrkaqpAHjUWnvUWpttrU211r5nrc2y1h4G/oXzD+mp7LDWvlq43uctoCnQ+EzONca0BHoCf7XWHrPWLsP5R79UHvbxDWvtj9babGAWTiACJ2zMtdYusdYeBR4p/AxO5YPCPl5Q+Pv1wKfW2pRyfFZFBgGbrLWzrbW5wCRgX4n395O1dkHh3yQFeNrDdjHGtMAJan+y1uZYa9cBrxX2u8gya+28wr/DFKCbB+3640yH/tlae9haux14il+DZy5OyGxWWHdZieO1gU6AsdZuttbu9eS9iIjClUhVlWKtzSn6xRgTaoz5X+G0zyFgCVDPnPqbaCVDQVbh0/AzPLcZcLDEMYBdp+qwh33cV+J5Vok+NSvZtrX2CKcZSSns07vA9YWjbGOAt8+gH6U5sQ+25O/GmMbGmHeMMbsL252KM8LliaLP8nCJYzuA5iV+P/GzCTZlr7drCAQWtlVauw/ijLh9WzjVeHPhe1uIMzL2IrDfGDPZGFPHw/ciUuMpXIlUTfaE3+8HOgLnWWvr4EzpQIk1QS7YCzQwxoSWONbiNOefTR/3lmy7sGZEGde8hTOddRnOKMycs+zHiX0wHP9+/43zd4kpbDfhhDZP/JuVtAfns6xd4lhLYHcZfSrLAX4dnTqpXWvtPmvtrdbaZsBtwEumcAsHa+1z1toeQBec6cE/nmVfRGoMhSuR6qE2ztqhdGNMA+BRtwtaa3cAq4CJxphaxpjzgatc6uNsYIgx5kJjTC3g75T936+lQDowGXjHWnvsLPvxCdDVGDO8cMTobpy1b0VqA5lAhjGmOSeHkWScdU8nsdbuAr4GHjPGBBtjYoHf4Yx+lVvhFOIs4F/GmNrGmFbAfUXtGmNGlFjMn4YTAAuMMT2NMecZYwKBI0AOp5+GFZESFK5EqodJQAjOSMVK4LMKqjsGOB9niu6fwEzg6CnOLXcfrbWbgDtxFqTvxQkCSWVcY3GmAlsV/jyrflhrDwAjgP/gvN/2wPISp/wNOBfIwAli75/QxGPAX4wx6caYB0opMRpojTOK9QHOmrovPOlbGe7CCUi/AMtwPsPXC1/rCXxjjMnEWS93j7X2F6AO8CrO57wD5/0+4YW+iNQIxvnvj4jI2Sv8Kv8Wa63rI2ciIpWVRq5EpNwKp4/aGWP8jDFXAL8BPvR1v0REfEk7O4vI2WiCM/0VgTNNd4e1dq1vuyQi4luaFhQRERHxIk0LioiIiHiRwpWIiIiIF1WqNVcNGza0rVu39nU3RERERMq0evXqA9bayBOPV6pw1bp1a1atWuXrboiIiIiUyRizo7TjmhYUERER8SKFKxEREREvUrgSERER8aJKteZKRESkpsjNzSUpKYmcnBxfd0XKEBwcTFRUFIGBgR6dr3AlIiLiA0lJSdSuXZvWrVtjjPF1d+QUrLWkpqaSlJREmzZtPLpG04IiIiI+kJOTQ0REhIJVJWeMISIi4oxGGBWuREREfETBqmo407+Tq+HKGHOPMWajMWaTMWa8m7VERETEc6mpqcTFxREXF0eTJk1o3rx58e/Hjh077bWrVq3i7rvvLrPGBRdc4JW+Llq0iCFDhnilrYrg2porY0w0cCvQCzgGfGaMmWut/cmtmiIiIuKZiIgI1q1bB8DEiRMJDw/ngQceKH49Ly+PgIDSY0J8fDzx8fFl1vj666+909kqxs2Rq87AN9baLGttHrAYGO5ivdOy1vLp1k/5/KfPfdUFERGRSu3GG2/k9ttv57zzzuPBBx/k22+/5fzzz6d79+5ccMEF/PDDD8DxI0kTJ07k5ptvpl+/frRt25bnnnuuuL3w8PDi8/v168c111xDp06dGDNmDNZaAObNm0enTp3o0aMHd999d5kjVAcPHmTYsGHExsbSu3dvNmzYAMDixYuLR966d+/O4cOH2bt3L3379iUuLo7o6GiWLl3q9c+sNG5+W3Aj8C9jTASQDQwCTrq3jTFmHDAOoGXLlq51xhjDwwsfJsAvgMvPudy1OiIiIlVZUlISX3/9Nf7+/hw6dIilS5cSEBDAF198wUMPPcR777130jVbtmzhq6++4vDhw3Ts2JE77rjjpG0L1q5dy6ZNm2jWrBl9+vRh+fLlxMfHc9ttt7FkyRLatGnD6NGjy+zfo48+Svfu3fnwww9ZuHAh119/PevWrePJJ5/kxRdfpE+fPmRmZhIcHMzkyZO5/PLLefjhh8nPzycrK8trn9PpuBaurLWbjTH/BeYDR4B1QH4p500GJgPEx8dbt/oDkBCbwP3z7+eHAz/QsWFHN0uJiIh4bPxn41m3b51X24xrEsekKyad8XUjRozA398fgIyMDG644Qa2bt2KMYbc3NxSrxk8eDBBQUEEBQXRqFEjkpOTiYqKOu6cXr16FR+Li4tj+/bthIeH07Zt2+ItDkaPHs3kyZNP279ly5YVB7wBAwaQmprKoUOH6NOnD/fddx9jxoxh+PDhREVF0bNnT26++WZyc3MZNmwYcXFxZ/x5lIerC9qttf9nre1hre0LpAE/ulmvLKOiR+Fn/JiWOM2X3RAREam0wsLCip8/8sgj9O/fn40bNzJnzpxTbkcQFBRU/Nzf35+8vLxynXM2JkyYwGuvvUZ2djZ9+vRhy5Yt9O3blyVLltC8eXNuvPFG3n77ba/WPBVXNxE1xjSy1u43xrTEWW/V2816ZWlWuxmXtLmEqRum8rd+f9NXYEVEpFIozwhTRcjIyKB58+YAvPnmm15vv2PHjvzyyy9s376d1q1bM3PmzDKvueiii5g2bRqPPPIIixYtomHDhtSpU4eff/6ZmJgYYmJi+O6779iyZQshISFERUVx6623cvToUdasWcP111/v9fdxIrf3uXrPGPM9MAe401qb7nK9Mo2NHcu29G18vatmfoNBRETEUw8++CB//vOf6d69u9dHmgBCQkJ46aWXuOKKK+jRowe1a9embt26p71m4sSJrF69mtjYWCZMmMBbb70FwKRJk4iOjiY2NpbAwECuvPJKFi1aRLdu3ejevTszZ87knnvu8fp7KI0pWq1fGcTHx9tVq05a8+5VmccyafxkY66PvZ6Xh7zsai0REZFT2bx5M507d/Z1N3wuMzOT8PBwrLXceeedtG/fnnvvvdfX3TpJaX8vY8xqa+1Je1LUuB3aw2uFM6zTMGZumsmx/NNvkiYiIiLuevXVV4mLi6Nr165kZGRw2223+bpLZ63GhStwpgbTctKYt3Wer7siIiJSo917772sW7eO77//nmnTphEaGurrLp21GhmuLm17KY3CGjF1w1Rfd0VERESqmRoZrgL8AhgdPZo5P84hPcfna+xFRESkGqmR4QqcDUWP5R/j3U3v+rorIiIiUo3U2HDVo2kPOjXsxNRETQ2KiIiI99TYcGWMISEmgSU7lrAjfYevuyMiIlLpFd2Iec+ePVxzzTWlntOvXz/K2lZp0qRJx93nb9CgQaSnn/0ynYkTJ/Lkk0+edTtnq8aGK4AxsWMAmJ443cc9ERERqTqaNWvG7Nmzy339ieFq3rx51KtXzxtdqxRqdLhqXa81F7W8iCkbplCZNlMVERFx24QJE3jxxReLfy8a9cnMzOSSSy7h3HPPJSYmho8++uika7dv3050dDQA2dnZjBo1is6dO3P11VeTnZ1dfN4dd9xBfHw8Xbt25dFHHwXgueeeY8+ePfTv35/+/fsD0Lp1aw4cOADA008/TXR0NNHR0UyaNKm4XufOnbn11lvp2rUrAwcOPK5OadatW0fv3r2JjY3l6quvJi0trbh+ly5diI2NZdSoUQAsXryYuLg44uLi6N69O4cPHy7XZ1rMWltpHj169LAV7X+r/meZiF29Z3WF1xYRkZrr+++/92n9NWvW2L59+xb/3rlzZ7tz506bm5trMzIyrLXWpqSk2Hbt2tmCggJrrbVhYWHWWmu3bdtmu3btaq219qmnnrI33XSTtdba9evXW39/f/vdd99Za61NTU211lqbl5dnL774Yrt+/XprrbWtWrWyKSkpxbWLfl+1apWNjo62mZmZ9vDhw7ZLly52zZo1dtu2bdbf39+uXbvWWmvtiBEj7JQpU056T48++qh94oknrLXWxsTE2EWLFllrrX3kkUfsPffcY621tmnTpjYnJ8daa21aWpq11tohQ4bYZcuWWWutPXz4sM3NzT2p7dL+XsAqW0qecfXGzVXBiC4juOvTu5i6YSrnNj3X190REZEaaPx4WLfOu23GxcGk09wPunv37uzfv589e/aQkpJC/fr1adGiBbm5uTz00EMsWbIEPz8/du/eTXJyMk2aNCm1nSVLlnD33XcDEBsbS2xsbPFrs2bNYvLkyeTl5bF3716+//77414/0bJly7j66qsJCwsDYPjw4SxdupShQ4fSpk0b4uLiAOjRowfbt28/ZTsZGRmkp6dz8cUXA3DDDTcwYsSI4j6OGTOGYcOGMWzYMAD69OnDfffdx5gxYxg+fDhRUVGn/uA8UKOnBQHqh9RnSIchTE+cTl6B929KKSIiUlmNGDGC2bNnM3PmTEaOHAnAtGnTSElJYfXq1axbt47GjRuTk5Nzxm1v27aNJ598ki+//JINGzYwePDgcrVTJCgoqPi5v79/uW8k/cknn3DnnXeyZs0aevbsSV5eHhMmTOC1114jOzubPn36sGXLlnL3E9DIFUBCTALvb36fL3/5ksvPudzX3RERkRrmdCNMbho5ciS33norBw4cYPHixYAz6tOoUSMCAwP56quv2LHj9N+o79u3L9OnT2fAgAFs3LiRDRs2AHDo0CHCwsKoW7cuycnJfPrpp/Tr1w+A2rVrc/jwYRo2bHhcWxdddBE33ngjEyZMwFrLBx98wJQpU874fdWtW5f69euzdOlSLrroIqZMmcLFF19MQUEBu3bton///lx44YW88847ZGZmkpqaSkxMDDExMXz33Xds2bKFTp06nXHdIgpXwKD2g6gXXI+piVMVrkREpMbo2rUrhw8fpnnz5jRt2hSAMWPGcNVVVxETE0N8fHyZIeOOO+7gpptuonPnznTu3JkePXoA0K1bN7p3706nTp1o0aIFffr0Kb5m3LhxXHHFFTRr1oyvvvqq+Pi5557LjTfeSK9evQC45ZZb6N69+2mnAE/lrbfe4vbbbycrK4u2bdvyxhtvkJ+fT0JCAhkZGVhrufvuu6lXrx6PPPIIX331FX5+fnTt2pUrr7zyjOuVZGwl+pZcfHy8LWtvDLfcNuc2piZOJfmBZMJrhfukDyIiUnNs3ryZzp07+7ob4qHS/l7GmNXW2vgTz63xa66KjO02lqzcLD7c8qGvuyIiIiJVmMJVoQtaXEDreq2ZukG3wxEREZHyU7gq5Gf8GBMzhgW/LGBf5j5fd0dERESqKIWrEhJiEyiwBcxInOHrroiISA1QmdY9y6md6d9J4aqETg07Ed8snqmJmhoUERF3BQcHk5qaqoBVyVlrSU1NJTg42ONrtBXDCRJiEhj/+Xi+T/meLpFdfN0dERGppqKiokhKSiIlJcXXXZEyBAcHn9Gu7QpXJxgVPYr759/PtA3T+Ncl//J1d0REpJoKDAykTZs2vu6GuMDVaUFjzL3GmE3GmI3GmBnGGM/H1HykcXhjBrYbyNTEqRTYAl93R0RERKoY18KVMaY5cDcQb62NBvyBUW7V86aE2AR2Zuxk2c5lvu6KiIiIVDFuL2gPAEKMMQFAKLDH5Xpe8ZuOvyEsMEx7XomIiMgZcy1cWWt3A08CO4G9QIa1dr5b9bwprFYYv+3yW2ZtmkVOXvnv4C0iIiI1j5vTgvWB3wBtgGZAmDEmoZTzxhljVhljVlWmb0wkxCSQcTSDT378xNddERERkSrEzWnBS4Ft1toUa20u8D5wwYknWWsnW2vjrbXxkZGRLnbnzAxoM4Cm4U2155WIiIicETfD1U6gtzEm1BhjgEuAzS7W8yp/P3+ui7mOT378hNSsVF93R0RERKoIN9dcfQPMBtYAiYW1JrtVzw0JsQnkFuTy7vfv+rorIiIiUkW4+m1Ba+2j1tpO1tpoa+1Ya+1RN+t5W7fG3ega2VXfGhQRERGP6d6Cp2GMISE2geW7lvNL2i++7o6IiIhUAQpXZRgTMwaAaRum+bgnIiIiUhUoXJWhRd0W9Gvdj6mJU3XnchERESmTwpUHEmIS+DH1R1btWeXrroiIiEglp3DlgWu6XEOQfxBTNkzxdVdERESkklO48kDd4LoM7TiUdza+Q25+rq+7IyIiIpWYwpWHEmITSMlKYcEvC3zdFREREanEFK48dMU5VxAREqE9r0REROS0FK48VMu/FiO7juTDLR9y+OhhX3dHREREKimFqzOQEJtAdl42729+39ddERERkUpK4eoM9I7qTbv67ZiaqKlBERERKZ3C1Rkouh3Ol798ye5Du33dHREREamEFK7O0JiYMVgsMzbO8HVXREREpBJSuDpD7SPac17z8/StQRERESmVwlU5jI0dy/rk9SQmJ/q6KyIiIlLJKFyVw7VdryXAL0CjVyIiInIShatyiAyL5IpzrmBa4jQKbIGvuyMiIiKViMJVOSXEJLD78G4Wb1/s666IiIhIJaJwVU5DOw6ldq3aTNkwxdddERERkUpE4aqcQgJDuKbLNcz+fjbZudm+7o6IiIhUEgpXZyEhNoHDxw4z58c5vu6KiIiIVBIKV2ehX+t+RNWJ0rcGRUREpJjC1VnwM35cF30dn/70KSlHUnzdHREREakEXAtXxpiOxph1JR6HjDHj3arnKwmxCeQV5DFr0yxfd0VEREQqAdfClbX2B2ttnLU2DugBZAEfuFXPV2Iax9CtcTemJmpqUERERCpuWvAS4Gdr7Y4KqlehEmITWJm0kq2pW33dFREREfGxigpXo4AZFVSrwo2OHo3BMC1xmq+7IiIiIj7mergyxtQChgLvnuL1ccaYVcaYVSkpVXNRePM6zRnQZgBTN0zFWuvr7oiIiIgPVcTI1ZXAGmttcmkvWmsnW2vjrbXxkZGRFdAdd4yNHcvPaT/zze5vfN0VERER8aGKCFejqcZTgkWu7nw1IQEhTFmv2+GIiIjUZK6GK2NMGHAZ8L6bdSqDOkF1+E2n3zBz00yO5R/zdXdERETER1wNV9baI9baCGtthpt1KouxsWNJzU7l858+93VXRERExEe0Q7sXXdb2MiJDI5myQVODIiIiNZXClRcF+gcyKnoUH//wMRk5NWKwTkRERE6gcOVlY2PHcjT/KO9tfs/XXREREREfULjysvhm8XSI6KCpQRERkRpK4crLjDEkxCSwaPsidmbs9HV3REREpIIpXLlgTOwYAGYkVvvtvUREROQEClcuaFu/LX1a9GHKhim6HY6IiEgNo3DlkoTYBDalbGJ98npfd0VEREQqkMKVS0Z0GUGgXyBTN0z1dVdERESkAilcuSQiNILBHQYzPXE6+QX5vu6OiIiIVJAaFa5SUmDPnoqrlxCTwN7MvSzctrDiioqIiIhP1ZhwlZcHF10Eo0Y5zyvC4A6DqRtUl6mJmhoUERGpKWpMuAoIgL/8BZYuhX/+s2JqBgcEM6LLCN7f/D5Hjh2pmKIiIiLiUzUmXAEkJMD118M//gGLF1dMzbHdxpJ5LJOPf/i4YgqKiIiIT9WocAXw4ovQrh2MGQMHDrhf78KWF9KybkvdDkdERKSGqHHhKjwcZs50FrfffDO4vcenn/FjTMwY5v88n+TMZHeLiYiIiM/VuHAF0L07PPEEzJkDzz/vfr2E2ATybT4zN810v5iIiIj4VI0MVwB33QVXXQV//COsXeturS6RXTi36bmaGhQREakBamy4MgZefx0iI2HkSDh82N16CTEJrNqzii0HtrhbSERERHyqxoYrgIYNYdo0+Pln+MMf3K01OmY0fsaPaRumuVtIREREfKpGhyuAiy+GRx6Bt9+GKS7O2jUJb8JlbS9jauJUrNur6EVERMRnany4Amdz0b594Y47YOtW9+okxCawPX07y3ctd6+IiIiI+JSr4coYU88YM9sYs8UYs9kYc76b9corIMCZHgwKcm6Pc/SoO3WGdRpGaGAoUzfodjgiIiLVldsjV88Cn1lrOwHdgM0u1yu3qCh44w1YswYmTHCnRnitcIZ3Hs6sTbM4mudSghMRERGfci1cGWPqAn2B/wOw1h6z1qa7Vc8bhg51tmiYNAnmznWnRkJMAmk5aczbOs+dAiIiIuJTbo5ctQFSgDeMMWuNMa8ZY8JcrOcVjz8OcXFw442we7f327+k7SU0DmvM1ERNDYqIiFRHboarAOBc4GVrbXfgCHDShJsxZpwxZpUxZlVKSoqL3fFMcDC88w7k5Dj3H8zP9277AX4BXBdzHXN/nEtadpp3GxcRERGfczNcJQFJ1tpvCn+fjRO2jmOtnWytjbfWxkdGRrrYHc917Ojc4HnxYvj3v73ffkJsAsfyjzH7+9neb1xERER8yrVwZa3dB+wyxnQsPHQJ8L1b9bzt+uudkauJE2HpUu+23b1Jdzo37Kzb4YiIiFRDbn9b8C5gmjFmAxAHuDAO5A5j4OWXoW1buO46OHjQm20bxsaOZenOpWxP3+69hkVERMTnXA1X1tp1hVN+sdbaYdbaKrXIqHZtZ/1VcjLcfDN4c2P162KuA2B64nTvNSoiIiI+px3ay9CjB/z3v/DRR/DSS95rt1W9VvRt1ZfX1rxGRk6G9xoWERERn1K48sD48TBoENx/P6xf7712J148kaRDSQyZMYSs3CzvNSwiIiI+o3DlAWPgzTehQQMYORKOHPFOu/3b9Gfa8Gks37mca2Zdw7H8Y95pWERERHxG4cpDkZHO/Qd//NHZxd1bRnQdwf+G/I9Pf/qUGz68gfwCL2+sJSIiIhVK4eoM9O8PDz/s3INwuhfXod/a41Yev/Rx3tn4DnfOuxPrzZXzIiIiUqECfN2BqubRR+Grr+D22+G886BdO++0+8c+fyQtJ43Hlj1G/eD6PHbpY95pWERERCqURq7OUECAM2rl7w+jRsExLy6T+teAf3F7j9v5z/L/8Pjyx73XsIiIiFQYhatyaNkSXn8dVq2Chx7yXrvGGF4Y9AKjokfxpy/+xKurX/Ve4yIiIlIhFK7K6eqr4fe/h6eegk8/9V67/n7+vD3sbQa1H8Rtc29j1qZZ3mtcREREXKdwdRaefBJiYpz7EO7Z4712A/0DeXfEu1zY8kIS3k/gs58+817jIiIi4iqFq7MQEgIzZ0JWFowdC/le3EUhNDCUOaPnEN0omuEzh7N853LvNS4iIiKuUbg6S507w3PPwcKFzm1yvKlucF0+S/iMFnVbMHj6YNbtW+fdAiIiIuJ1CldecPPNzjcH//pX+Ppr77bdKKwRC8YuoE5QHS6fejlbU7d6t4CIiIh4lcKVFxgDr7zifItw9GhIS/Nu+y3rtmTB2AVYa7l0yqUkHUrybgERERHxGoUrL6lbF955x1nYfsst4O1N1js27MhnCZ+RnpPOZVMuI+VIincLiIiIiFd4FK6MMWHGGL/C5x2MMUONMYHudq3q6dUL/v1veP99+N//vN/+uU3PZc7oOWxP386V067k0NFD3i8iIiIiZ8XTkaslQLAxpjkwHxgLvOlWp6qy+++Hyy+H8eMhMdH77fdt1Zf3rn2P9cnrGTpjKNm52d4vIiIiIuXmabgy1tosYDjwkrV2BNDVvW5VXX5+8NZbUK8ejBwJR454v8ag9oN4e9jbLNmxhGtnX0tufq73i4iIiEi5eByujDHnA2OATwqP+bvTpaqvcWOYOhW2bHFGsNwwOmY0Lw1+ibk/zuWmj26iwBa4U0hERETOiKfhajzwZ+ADa+0mY0xb4Cv3ulX1XXopTJgAr73mbDTqhtvjb+ffA4bhLr4AACAASURBVP7NtMRp3P3p3Vhvr6IXERGRMxbgyUnW2sXAYoDChe0HrLV3u9mx6uBvf4OvvoJx46BnT2jb1vs1Jlw4gbScNJ74+gnqB9fnHwP+4f0iIiIi4jFPvy043RhTxxgTBmwEvjfG/NHdrlV9gYEwY4azD9bo0ZDrwtIoYwz/vfS/3NL9Fv659J88veJp7xcRERERj3k6LdjFWnsIGAZ8CrTB+caglKF1a2dq8Ntv4S9/caeGMYZXhrzCiC4juH/+/by+9nV3ComIiEiZPJoWBAIL97UaBrxgrc01xpS5wMcYsx04DOQDedba+HL3tAq75hq47TZ4/HEYMMDZqsHb/P38mTp8KoeOHuLWObdSN6guv+3yW+8XEhERkdPydOTqf8B2IAxYYoxpBXi6g2V/a21cTQ1WRZ55Brp2heuvh3373KlRy78W7137Hr2jenPd+9ex4OcF7hQSERGRU/IoXFlrn7PWNrfWDrKOHUB/l/tWrYSEON8aPHwYxo6FApd2TgirFcbc0XPp1LATw2YOY8WuFe4UEhERkVJ5uqC9rjHmaWPMqsLHUzijWGWxwHxjzGpjzLiz6mk10LUrTJoEX3wBTzzhXp36IfWZnzCfZrWbMWj6IBKTXdgqXkRERErl6bTg6zhrp64tfBwC3vDguguttecCVwJ3GmP6nniCMWZcUWhLSan+NyO+9VYYMQIefhhWrnSvTuPwxiwYu4CwwDAGTh3ITwd/cq+YiIiIFDOebDxpjFlnrY0r61gZbUwEMq21T57qnPj4eLtq1SpPm6yy0tMhLs7ZomHtWudWOW7ZnLKZi964iNpBtVl20zKa12nuXjEREZEaxBizurQ15Z6OXGUbYy4s0Vgf4LR3DDbGhBljahc9Bwbi7JFV49WrB++8A7t2wU03ubP/VZHOkZ35LOEzUrNSGTh1IKlZqe4VExEREY/D1e3Ai8aY7YXbK7wA3FbGNY2BZcaY9cC3wCfW2s/K3dNqpndvePJJ+PBDuOoqZ6G7W+KbxfPx6I/5+eDPXDntSg4fdbGYiIhIDefptwXXW2u7AbFArLW2OzCgjGt+sdZ2K3x0tdb+ywv9rVbGj4dXX3UWuPftC3v2uFerX+t+vDviXdbsXcOwmcPIyctxr5iIiEgN5unIFQDW2kOFO7UD3OdCf2qcW26BOXNg61ZnNGvTJvdqXdXxKt4c9iYLty1k9HujySvIc6+YiIhIDXVG4eoExmu9qOGuvBKWLHHWXvXp49zs2S0JsQk8f+XzfLjlQ3738e8osC5tuCUiIlJDnU24KvtrhuKxc891tmZo1sy5Pc706e7V+kOvP/D3fn/n7fVvc+9n9+LJN0ZFRETEM6e9t6Ax5jClhygDhLjSoxqsVStYvhyuvhrGjIEdO2DCBGfLBm/7S9+/kJaTxjMrn6FBSAMe7feo94uIiIjUQKcNV9ba2hXVEXHUrw+ffw433ggPPeQErBdegABPb7HtIWMMTw18ivScdCYunki94Hrc0/se7xYRERGpgbz8T7Z4Q1AQTJvmjGT997+QlOTsixUe7t06xhgmXzWZjKMZjP98PPVD6nN9t+u9W0RERKSGOZs1V+IiPz/4z3/gpZfg00+hXz/Yt8/7dQL8Apg+fDqXtr2Umz+6mY+2fOT9IiIiIjWIwlUld8cdzkajmzfD+efDli3erxEUEMQHIz8gvlk8186+lmdXPqtvEYqIiJSTwlUVcNVVsGgRZGXBBRfA0qXerxFeK5x5Y+ZxWdvLGP/5eK6YegW7D+32fiEREZFqTuGqiujZE1asgMhIuOwymDXL+zUahDRgzug5vDL4FZbvWk7MyzHM2uRCIRERkWpM4aoKadsWvv4aevSAkSOdexN6e4sqYwy3xd/G2tvW0j6iPSNnj2TsB2PJyMnwbiEREZFqSuGqiomIcO5FeM018Mc/wt13Q36+9+t0iOjAspuW8ejFjzIjcQaxr8SyePti7xcSERGpZhSuqqCQEJg5E+67z9kD65prnPVY3hboH8jEfhNZfvNyavnXov9b/XlwwYMczTvq/WIiIiLVhMJVFeXnB089Bc8+Cx99BAMGQEqKO7XOizqPdbetY1yPcTzx9ROc99p5bNy/0Z1iIiIiVZzCVRV3990wezasX+9s1bB1qzt1wmqF8cqQV/h41MfszdxL/OR4nlnxjLZsEBEROYHCVTUwfDgsXAjp6U7AWrHCvVpXdbyKxDsSGdhuIPfNv4+BUwaSdCjJvYIiIiJVjMJVNVEUqurVc6YIP/jAvVqNwhrx0aiPePWqV1mZtJKYl2N4Z+M77hUUERGpQhSuqpH27Z2A1a0b/Pa38Nxz7tUyxnDLubew7vZ1dGrYidHvjWbM+2NIz0l3r6iIiEgVoHBVzURGOlOEQ4fCPfc43ygscHFZ1DkNzmHpTUv5e7+/M3PjTGJfjuWrbV+5V1BERKSSU7iqhkJD4b334A9/gGeecTYczclxr16AXwCPXPwIK363gpDAEC55+xIemP+AtmwQEZEaSeGqmvL3d6YFn3zS+TbhpZdCaqq7NXs278macWu4Pf52nlrxFD1f7UlicqK7RUVERCoZhatqzBi4/35nw9HvvnNu+vzLL+7WDKsVxkuDX+KT6z5h/5H9xL8az1NfP6UtG0REpMZwPVwZY/yNMWuNMXPdriWlu/Za55Y5KSnQuzd8+637NQe1H0TiHYkMaj+IBxY8wCVvX8LOjJ3uFxYREfGxihi5ugfYXAF15DQuusi56XNYGPTrB3PmuF8zMiyS9699n/8b+n+s2rOK2JdjmZ443f3CIiIiPuRquDLGRAGDgdfcrCOe6dTJ2aqhSxcYNgxeftn9msYYbu5+M+tvX0/XRl0Z8/4YRr83mrTsNPeLi4iI+IDbI1eTgAcBLbipJJo0gUWL4Mor4fe/hz/9yd2tGoq0rd+WxTcu5p/9/8ns72cT+0osX/7ypfuFRUREKphr4coYMwTYb61dXcZ544wxq4wxq1LcuvOwHCc8HD78EG67DR5/HMaMgaMVsGtCgF8AD/d9mJW/W0l4rXAunXIp931+Hzl5Lu4TISIiUsHcHLnqAww1xmwH3gEGGGOmnniStXaytTbeWhsfGRnpYnekpIAAZ1rwscfgnXdg4EBIq6CZuh7NerB63Gru7Hknz6x8hvjJ8azft75iiouIiLjMtXBlrf2ztTbKWtsaGAUstNYmuFVPzpwxMGECTJvmrMXq0wd27KiY2qGBobww6AU+HfMpqdmp9Hy1J48vf5z8gvyK6YCIiIhLtM+VcN118PnnsGcP9OoF//sfHDtWMbWvOOcKEu9I5KqOV/GnL/7EgLcHsCO9ghKeiIiICyokXFlrF1lrh1RELSmf/v1h+XJo2xZuvx3OOceZNqyItVgNQxsye8Rs3vzNm6zdu5bYV2KZsn4K1lr3i4uIiHiZRq6kWNeuzl5Yn38OLVo43yY85xx48UV3700IzpYNN8TdwPrb1xPbOJbrP7yekbNHcjD7oLuFRUREvEzhSo5jjLO4fdkyWLAAWrd2bgDdrh08/7z7IatN/TYsumERj13yGB9u+ZCYl2N4f/P7GsUSEZEqQ+FKSmWMc7PnJUvgyy+dEay773amDZ99FrKz3avt7+fPhAsn8M0t3xAREsFvZ/2WS6dcqptAi4hIlaBwJadlDAwYAIsXw1dfQYcOMH68E7KeeQaystyr3b1pd9bctoYXB73Iun3riPtfHL//5PccyDrgXlEREZGzpHAlHuvXz9ndfdEi6NwZ7rvPCVlPPQVHjrhTM8AvgN/3/D1b79rKnT3vZPLqybR/vj3PffMcufm57hQVERE5CwpXcsYuvhgWLnSmDKOj4YEHnJD1xBPuhawGIQ147srnWH/7eno268k9n91Dt1e6Mf/n+e4UFBERKSeFKym3iy6CL75wFr936wYPPugsgP/vfyEz052aXRt15fOEz/lo1Eccyz/G5VMvZ+iMoWxN3epOQRERkTOkcCVnrU8fmD/f2cahRw9n1/fWreE//4HDh71fzxjD0I5D2fT7TTx+6eMs2r6Iri915U8L/sSho4e8X1BEROQMKFyJ15x/Pnz2mXMrnV694M9/dkLWv/8Nh1zIPEEBQfyxzx/58a4fGRs7lie+foIOz3fg9bWvU2ALvF9QRETEAwpX4nW9e8O8efDNN07gevhhJ2T985+QkeH9ek3Cm/B/v/k/vr31W9o1aMfvPv4dvV7txfKdy71fTEREpAwKV+KaXr1g7lz47ju48EJ45BEnZP3975Ce7v168c3iWXbTMqYPn07ykWQufONCrnvvOnZl7PJ+MRERkVNQuBLXxcfDxx/DqlXQty88+qgTsiZO9H7IMsYwOmY0W+7cwl/7/pUPtnxAxxc68vfFfycr18VNuURERAopXEmF6dEDPvoI1qxxbhT9t79Bq1bw179CWpp3a4XVCuNv/f/Glju3cFXHq3h00aN0frEzszbN0q10RETEVQpXUuG6d4cPPoB165xb7PzjH85I1iOPwEEv36e5Vb1WzLxmJotvXEyDkAaMnD2Si9+8mLV713q3kIiISCGFK/GZbt3gvfdg/XrnZtH//KcTsh5+GFJTvVurb6u+rLp1FZOHTGbzgc30mNyDcXPGsf/Ifu8WEhGRGk/hSnwuNhbefRcSE+HKK+Gxx5yQ9ec/wwEv3kbQ38+fW3vcyta7tnJv73t5Y90btH++PU+veJpj+ce8V0hERGo0hSupNKKjYeZM2LgRhgxxdnpv3RpuucW5cXSBl7auqhdcj6cuf4qNd2zkwpYXcv/8+4l5OYZ5W+d5p4CIiNRoCldS6XTpAjNmwKZNcO21TuDq18+5f+Ff/gI//OCdOh0bduST6z7hk+s+wWAYPH0wg6YNYsuBLd4pICIiNZLClVRanTvD66/Dvn0wdSp06uRMGXbqBOedBy+84J1pw0HtB7Hhjg08PfBpvt71NTEvx3Df5/eRnuPCZlwiIlLtmcr0tfT4+Hi7atUqX3dDKrG9e2H6dJgyxVkIHxAAgwbB2LHOVGJw8Nm1v//Ifh5Z+AivrnmViNAI/jXgX/yu++/w9/P3zhsQEZFqwxiz2lobf9JxhSupqjZscELWtGlO6KpXz5lGHDvWuZm0MeVve+3etYz/fDxLdiyhW+NuPHvFs1zc+mLvdV5ERKq8U4UrTQtKlRUbC088Abt2wfz5zsjV1Klw0UVwzjnOTvA//VS+trs37c6iGxYx65pZpOWk0e+tflz77rXsSN/h3TchIiLVjkaupFrJzIT333dGtL78Eqx1bh49diyMHAkNGpx5m9m52Tz59ZM8tuwxLJbfx/+eu867i9b1Wnu9/yIiUnVU+LSgMSYYWAIEAQHAbGvto6e7RuFKvCkp6df1WRs3QmCgM7o1dqyzTiso6Mza25Wxi4cXPsz0xOlYLMM7D+fe3vdyftT5mLOZgxQRkSrJF+HKAGHW2kxjTCCwDLjHWrvyVNcoXIkbrHUWv7/9thO2kpOdEayRI52g1bv3ma3P2pWxixe+fYHJayaTnpNOr+a9GH/eeK7pcg2B/oHuvREREalUfLqg3RgTihOu7rDWfnOq8xSuxG15ebBggTOa9eGHkJ3trM8aOxYSEpy9tDx15NgR3lr/FpNWTmLrwa1E1YniDz3/wLge46gfUt+9NyEiIpWCT8KVMcYfWA2cA7xorf3T6c5XuJKKdOiQc2/DKVPgq6+cYxde6AStESOgvof5qMAWMG/rPCatnMSX274kNDCUG7vdyD2976FDRAf33oCIiPiUr0eu6gEfAHdZazee8No4YBxAy5Yte+zYoW9jScXbudPZ0mHKFNi82VmPddVVTtC64gqoVcuzdjYkb2DSyklMS5zGsfxjDG4/mHt738uANgO0LktEpJrx+T5Xxpi/AlnW2idPdY5GrsTXrIXVq52QNWMGpKRAw4YwapQTtHr29Gx9VnJmMi+vepmXvnuJlKwUYhrFML73eK6LuY7ggLPc6VRERCoFXyxojwRyrbXpxpgQYD7wX2vt3FNdo3AllUluLnz+uRO0PvoIjh6Fjh2dbxp27w5xcc6teAJPs4Y9Jy+HGYkzeGblMyTuT6RRWCPuiL+DO+LvoHF444p7MyIi4nW+CFexwFuAP85mpbOstX8/3TUKV1JZpafD7NnO1OHKlZCT4xwPCoLoaCdsFQWu2FgIDz/+emstC7ctZNI3k5j741xq+dfiupjruLf3vcQ2jq34NyQiImfN59OCnlC4kqogLw9++AHWrYO1a3/9efCg87ox0L79r2GrKHg1auS8/mPqjzy78lneXP8mWblZDGgzgHt738ug9oPwM7ppgohIVaFwJeIia53b8JwYuEp+P6Np0+NHuNp0TmfBwcm8uOp5kg4l0b5Be+457x5ujLuRsFphvnszIiLiEYUrER9IS3OCVlHYWrvW+TZifr7zeu3a0K1bAWEtf+LHWjPZFvQedVvs4bZeN/GHXn+gRd0Wvn0DIiJySgpXIpVETo5zO56SgWv9esjKcl43/nnYyERMk/XEdYfbBvdi9GVdqFPHt/0WEZHjKVyJVGL5+fDTT78GrhWrslm1Oo+s9NrF5zRukUmfnqGce65f8dRi06ZnduseERHxHoUrkSrGWti6PZOnP1zIu19u5eC2lgTs70negdbF5zRq5GwH0aSJ82jc+NefRc8bNfJ8E1QREfGcwpVIFZZfkM/cH+fyzMpnWPzDGoJTe9PLfxyRhwaSvLMOycnODakPHSr9+gYNjg9cpYWwxo2dIHa6fbtERORXClci1cTavWuZ9M0kZiTOIK8gj4HtBjK041AGtx9Mo6BWJCfDvn0UB67Snu/bB5mZpbcfEXHq8FXyWKNGEBBQse9dRKQyUbgSqWb2Ht7Ly6teZsbGGfx08CcAohtFM6T9EAZ3GEzvqN4E+J06/WRlnT58lTx25MjJ1xvjBLETQ1iTJtCixa+PZs0UwkSkelK4EqnGfkz9kbk/zuWTrZ+wZMcS8gryaBDSgCvOuYIh7Ydw+TmX0yCkQbnbz8z8NWyVFciKvvVYxM/PCVglA1eLFtCy5a/PIyOd80REqhKFK5EaIiMngwW/LGDuj3OZt3UeKVkp+Bk/+rTow+D2gxnSYQhdIrtgXPqa4aFDzoaqRY+dO4//fdeuX28fVKRWrZPD14kBrG5dfTNSRCoXhSuRGqjAFvDd7u+KR7XW7lsLQOt6rYuDVr/W/QgOCK6wPlkLBw6cPoDt3v3rRqtFwsNPDlwnhrCQkAp7GyIiClciAkmHkpi3dR6fbP2EL375gqzcLEIDQ7m07aUMbj+Ywe0H07xOc193k/x82Lv39AEsOfnk6yIiTg5cUVFQr56zG/6Jj5AQjYaJSPkpXInIcXLycli0fVHxqNb29O0AxDWJY0j7IQzpMISezXtW2ptJHz0KSUmnD2Dp6advw8/v+LAVHl56CCvrUXRdUJDCmkhNonAlIqdkreX7lO/5ZOsnzP1xLst3LafAFhAZGsmg9oMY3H4wA9sNpG5wXV939YwcPuxMMWZkOM8PH3YW5xc9P92j5HlHj3pWLyDAszAWFgahoc7IWWjo8Y9THQsMVHATqWwUrkTEYwezD/L5T58zd+tcPt36KWk5aQT4BXBRy4sY0sEZ1eoQ0cHX3awwubmeBzFPzsvNPfM++Pt7FsLKcywkBIKDnZG34GBtnSHiKYUrESmXvII8ViatLJ4+3Lh/IwDnNDineE+tvq36Ustf99jxVG6us2VFdrbzs+SjtGNnerzoWHlCHDhBLjj4+MB14qO0454eK+vcoCCnDyKVncKViHjFjvQdxdOHC7ct5Gj+UWrXqs1l7S5jSPshXNn+SpqEN/F1NwUnXJUMX6cLaEePOltkFD1O/P1Ux0o7Xt5QV5K/v7NFR2V4BAZCQQHk5R3/yM8v/7GzuT4/3wmgYWHOIzz8+J9lPQ8N1b5y3qJwJSJed+TYERZuW1gctnYf3g1Au/rtOL/F+Zwf5TxiGsecdrd4qV4KCjwPZ6cKbLm5cOyYdx8nbu9RWQQEOA9//1+fn+6Yn5/zuR054jwyMz1fF1gkNPT0QaysoHbiMX9/529W1iMvz3vnne6cwEBYvNidv1dJClci4iprLeuT17Pg5wWsSFrBiqQV7MvcB0BYYBg9m/csDlvntzifhqENfdxjqWny88sf2k4MOZ4GobLO9fPzzhcV8vKcUcjMzONDV8mfp3pe1usFBWffv/IKCHCCUmmP070WGgoffeR+/xSuRKRCWWvZkbGDFbtWFIetdfvWkVeQBzhrtkqGrehG0RrdEqlkrHVGEssKZwUFZQceT0JRyXMCAir/N2QVrkTE57Jys1i9Z3Vx2FqxawXJR5zdQMMCw+jVvFdx2Ood1VujWyJSqSlciUilY61le/r24qD1ddLXrN+3nnzrLI7pENHhuNGtrpFd8ffT18hEpHKo8HBljGkBvA00Biww2Vr77OmuUbgSkSPHjrBqz6rjRrdSslIAqF2r9kmjWw1CGvi4xyJSU/kiXDUFmlpr1xhjagOrgWHW2u9PdY3ClYicyFrLL2m/FAetFUkr2JC8oXh0q2NEx+O+mdglsotGt0SkQvh8WtAY8xHwgrV2wanOUbgSEU8cOXaE7/Z8d9xi+QNZBwBndOu8qPOKw1bvqN7UD6nv4x6LSHXk03BljGkNLAGirbWHTnWewpWIlIe1lp/Tfj4ubG1I3kCBdb5D3qpuK6IbRRPTKIaYxjFEN4qmU8NO2lVeRM6Kz8KVMSYcWAz8y1r7fimvjwPGAbRs2bLHjh07XO2PiNQMmccy+W73d3yz+xs2JG9g4/6NbDmwhdwCZ/vwAL8AOkR0IKZRTHHwim4UTZv6bfAz2r5aRMrmk3BljAkE5gKfW2ufLut8jVyJiJuO5R9ja+pWEvcnsnH/RhL3J5KYnMi29G3F54QFhtG1UVeiI6OLR7liGsXQKKwRprJvuiMiFcoXC9oN8BZw0Fo73pNrFK5ExBcyj2Wyaf+mXwNXYfjaf2R/8TkNQxseN8oV0ziGrpFdqR1U24c9FxFf8kW4uhBYCiQCRZvnP2StnXeqaxSuRKQy2X9kvxO4kn8NXBv3b+RI7pHic1rXa33ctGJMoxg6Nuyo9VwiNYDPvy3oCYUrEansCmwBO9J3FE8pbkxxwtcPqT8U39onwC+AjhEdnWnFEtOLreu11noukWpE4UpExEXH8o/xw4EfTppa3J6+vficovVcHSM60q5+O9o1aFf8MzI0Umu6RKoYhSsRER84dPQQ36d8f9zU4taDW0k6lHTcebVr1aZt/ba/Bq4S4atF3Ra6qbVIJaRwJSJSieTk5bAtbRs/p/3Mzwd/dn4WPt+Wvo1j+ceKzw3wC6B1vdYnha52DdrRtn5bQgNDffhORGquU4Ur/V8hEREfCA4IpnNkZzpHdj7ptfyCfHYf3v1r6CoRvlYmrSTjaMZx5zer3ez40FXieYOQBppuFKlgClciIpWMv58/Leu2pGXdlvRv0/+416y1HMw+WOqI1/yf57Pn8J7jzq8bVLfU0NWuQTua126u+zCKuEDhSkSkCjHGEBEaQURoBL2a9zrp9azcrFKnG9ftW8eHWz4s3qEeoJZ/LdrUa0Ob+m1oWaclLeq2KA51Leq0IKpOFEEBQRX59kSqBYUrEZFqJDQwlK6NutK1UdeTXssvyGfXoV0nTTduT9/O6j2rSclKOemaJuFNaFGnRXHgKg5fhUGsUVgjbS8hcgItaBcREQCyc7NJOpTEzoyd7Dq0y/mZsYudh3ayM8N5ZOVmHXdNLf9aRNWJOjl8FQWyui2oE1THR+9IxF1a0C4iIqcVEhhC+4j2tI9oX+rr1lrSctJ+DV0lQtjOjJ0s3rGY3Yd2k2/zj7uublDdX0e76hw/8tWiTgua12muHe2lWlG4EhERjxhjaBDSgAYhDYhrElfqOXkFeew9vPe40FU0+rUrYxffJH1Danbq8e1iaFq7afFoV1SdKJqGN6VJeBMahzemSXgTmoQ3oWFoQ01BSpWgaUEREalQR44dKZ5+PG4KssRUZHZe9knX+Rt/GoU1Kg5bjcMb0ySsSfHvJR91gupoCwpxnaYFRUSkUgirFUbHhh3p2LBjqa9ba8k8lknykWT2Ze476VF0PHF/Ivsy9xXf07GkIP+gUkNXk/AmNA5rfNzvIYEhbr9lqWEUrkREpFIxxlA7qDa1g2pzToNzTntugS0gLTvtpOBV8vFL2i+sSFpBypEULCfP1tQJqnPK4FV0LDIsksjQSAUx8YjClYiIVFl+xq9436/Stp8oKa8gj5QjKaWOghU9NiRvYP7P80/aBb9IWGBYcdAq/lny+Qk/w2uFa3qyBlK4EhGRGiHAL4CmtZvStHbTMs/NycshOfPX4JWSlULKkRTnZ+HzfZn7SExOJCUrhZy8nFLbCfIPKjWMNQxtWGogqxdcT4v2qwGFKxERkRMEBwTTql4rWtVrVea51lqO5B75NXyd+LPE862pW0nJSiHzWGapbfkb/9KD1wkhLCLEGa2LCInQLvqVkMKViIjIWTDGEF4rnPBa4bSp38aja3LycjiQdaDMQLZu3zpSjqSQlpN2yrbCAsOKg1bxz5LPC382CGlQ/LxucF2NkLlI4UpERKSCBQcEE1Uniqg6UR6dn5ufS2p2anH4Ss1KJTU79defJZ7vSN9BanYqadlppS7gB2eErH5I/VJDWKnHQp1wFhwQ7M2PodpSuBIREankAv0Di7+96Kn8gnzSc9KLg9fB7IPHB7ISwWxnxk7W7ltLalZqqXuMFQkNDD3lqFi94HrUC65H3aC6xc+LjwXXrVG78CtciYiIVEP+fv7F36QkwvPrsnOzjwtfB7MPnnKkbNe+XaRmpZKWk0aBLThtuyEBIaWGrnpBpRw74bx6wfWq1KiZwpWIiIgUCwkMISrQ8ylL+HXj14yj2lyu7gAAB05JREFUGaTnpJ/0yMgpcfyo8zMlK4WtB7eSkZNBWk5aqZvBllTLv9bJ4ayUUbKi44PaD/LZNhgKVyIiInJWSm78eiahrIi1luy87FMHspLHSwS4Hek7ip8fzT9a3F5oYChHHjrizbd4RhSuRERExKeMMYQGhhIaGEqz2s3K1UZOXk5xIDvVVhcVxbVwZYx5HRgC7LfWRrtVR0RERCQ4IJjg8GAahzf2dVdwc5OLN4ErXGxfROT/27vfUL3LOo7j70+b0KYxra1lbnWixsJMnZwHltADrZAUF/RAxcJqEEnoCim1oEcRUlG2kmJZbuCYD5aaBI2NKSq0/qy1/4uEWnbsrJ0hs1Zhap8e/K7Z7bZzjuHv7LrOzucFh/t3X2f8zuf+cu573/v6Xee6IyKaM2XNle3HgWem6vwRERERLcr2rBERERE9qt5cSfqUpG2Sto2NjdWOExEREfGqVG+ubK+2PWx7eMGCBbXjRERERLwq1ZuriIiIiNPJlDVXktYDW4GlkkYkrZiqnxURERHRiinb58r29VN17oiIiIhW5bJgRERERI/SXEVERET0SLZrZ3iJpDHgT7VznGLzgcO1QzQuNZpY6jO51Ghiqc/kUqOJzdT6vNX2CVsdNNVczUSSttkerp2jZanRxFKfyaVGE0t9JpcaTSz1eblcFoyIiIjoUZqriIiIiB6luapvde0A00BqNLHUZ3Kp0cRSn8mlRhNLfQZkzVVEREREjzJzFREREdGjNFeVSFos6VFJ+yTtlbSydqYWSZol6beSflo7S4sknS1pg6TfSdov6T21M7VE0ufK82uPpPWSXls7U22SfiTpkKQ9A2Ovl7RZ0pPl9pyaGWsapz5fL8+xXZIelHR2zYy1naxGA9+7VZIlza+RrRVprup5AbjV9vnApcBnJJ1fOVOLVgL7a4do2LeBjbbfCVxEavUSSecBtwDDti8AZgHX1U3VhDXAlceN3Q5ssb0E2FLuz1RrOLE+m4ELbF8I/B6441SHaswaTqwRkhYDHwSeOtWBWpPmqhLbo7a3l+O/0/2neF7dVG2RtAi4CrindpYWSZoHvA/4IYDtf9s+UjdVc2YDcyTNBuYCf6mcpzrbjwPPHDe8HFhbjtcCHz6loRpysvrY3mT7hXL3F8CiUx6sIeP8DgF8C/gCMOMXc6e5aoCkIWAZ8Mu6SZpzF90T9T+1gzTqbcAYcG+5dHqPpDNrh2qF7aeBb9C9ix4FnrW9qW6qZi20PVqODwILa4Zp3CeBn9UO0RpJy4Gnbe+snaUFaa4qk3QW8GPgs7b/VjtPKyRdDRyy/ZvaWRo2G7gE+J7tZcA/mNmXc16mrBtaTteEvhk4U9JH66Zqn7s/IZ/xMw8nI+lLdEs61tXO0hJJc4EvAl+unaUVaa4qknQGXWO1zvYDtfM05jLgGkkHgPuByyXdVzdSc0aAEdvHZjw30DVb0Xk/8EfbY7afBx4A3ls5U6v+KulcgHJ7qHKe5kj6OHA1cIOzh9Hx3k73JmZnec1eBGyX9KaqqSpKc1WJJNGtldlv+5u187TG9h22F9keoluE/IjtzDoMsH0Q+LOkpWXoCmBfxUiteQq4VNLc8ny7giz4H8/DwI3l+EbgJxWzNEfSlXRLFK6x/c/aeVpje7ftN9oeKq/ZI8Al5TVqRkpzVc9lwMfoZmR2lK8P1Q4V087NwDpJu4CLga9WztOMMqO3AdgO7KZ7vZvxu0hLWg9sBZZKGpG0ArgT+ICkJ+lm/O6smbGmcerzXeB1wObyWv39qiErG6dGMSA7tEdERET0KDNXERERET1KcxURERHRozRXERERET1KcxURERHRozRXERERET1KcxURTZP04sB2JTsk9bYLvaQhSXv6Ol9EBHQfnxER0bJ/2b64doiIiFcqM1cRMS1JOiDpa5J2S/qVpHeU8SFJj0jaJWmLpLeU8YWSHpS0s3wd+yicWZJ+IGmvpE2S5pR/f4ukfeU891d6mBExDaW5iojWzTnusuC1A9971va76XbQvquMfQdYa/tCug/YXVXGVwGP2b6I7jMY95bxJcDdtt8FHAE+UsZvB5aV83x6qh5cRJx+skN7RDRN0lHbZ51k/ABwue0/lA9BP2j7DZIOA+fafr6Mj9qeL2kMWGT7uYFzDAGbbS8p928DzrD9FUkbgaPAQ8BDto9O8UONiNNEZq4iYjrzOMf/j+cGjl/kf2tRrwLuppvl+rWkrFGNiFckzVVETGfXDtxuLcc/B64rxzcAT5TjLcBNAJJmSZo33kklvQZYbPtR4DZgHnDC7FlExMnknVhEtG6OpB0D9zfaPrYdwzmSdtHNPl1fxm4G7pX0eWAM+EQZXwmslrSCbobqJmB0nJ85C7ivNGACVtk+0tsjiojTWtZcRcS0VNZcDds+XDtLRMSgXBaMiIiI6FFmriIiIiJ6lJmriIiIiB6luYqIiIjoUZqriIiIiB6luYqIiIjoUZqriIiIiB6luYqIiIjo0X8BCzyT5wnHECUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGQuo6AL6rH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}