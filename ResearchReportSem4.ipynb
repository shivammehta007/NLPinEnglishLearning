{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResearchReport.ipynb",
      "provenance": [],
      "mount_file_id": "1B9xn20f36VNC_nP1c9iKu5VHJcsk34jZ",
      "authorship_tag": "ABX9TyOH1qBMVgMfV6AYJ51a5pc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/ResearchReportSem4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7ouSKNHTUa",
        "colab_type": "text"
      },
      "source": [
        "# Installing FairSeq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_aCj_e36puh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "08a5f701-6c39-48ce-a40e-2e5c6797299e"
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/f6/21311ebe1af3e2e6d959cc5ce69e09cae6b6267bbd6db294fcd8744316f6/sacrebleu-1.4.8-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.38.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2015435 sha256=90e45295ff89a841caed49773a99f753ce9d365c46440f206cd493147300400e\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: mecab-python3, portalocker, sacrebleu, fairseq\n",
            "Successfully installed fairseq-0.9.0 mecab-python3-0.996.5 portalocker-1.7.0 sacrebleu-1.4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaAyiCOHWUp",
        "colab_type": "text"
      },
      "source": [
        "# Download dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Xcx3pz5z-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osxgoL5mHd44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "abb981e8-af25-4060-9e2b-fc2e855a9fbc"
      },
      "source": [
        "!kaggle datasets list --user devicharith"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                             title                                  size  lastUpdated          downloadCount  \n",
            "----------------------------------------------  -------------------------------------  ----  -------------------  -------------  \n",
            "devicharith/language-translation-englishfrench  Language Translation (English-French)   4MB  2020-04-08 11:35:32             12  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-g0ThfHeCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5dc3b969-6163-4278-c473-143a3bac969a"
      },
      "source": [
        "!kaggle datasets download devicharith/language-translation-englishfrench"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading language-translation-englishfrench.zip to /content\n",
            "\r  0% 0.00/3.51M [00:00<?, ?B/s]\n",
            "\r100% 3.51M/3.51M [00:00<00:00, 118MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THU0ViA1HunH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4d64f2c4-24ac-4590-e03b-21f119d146dd"
      },
      "source": [
        "!unzip language-translation-englishfrench.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  language-translation-englishfrench.zip\n",
            "  inflating: eng_-french.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcIUf4dnIBAm",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTgsZjVqH08y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_auml_TIhua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"eng_-french.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbeQFlxoIh6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "573a6fb6-da12-4deb-9190-1b7160c4c2c5"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "      <th>French words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English words/sentences French words/sentences\n",
              "0                     Hi.                 Salut!\n",
              "1                    Run!                Cours !\n",
              "2                    Run!               Courez !\n",
              "3                    Who?                  Qui ?\n",
              "4                    Wow!             Ça alors !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqPM4vcyIiKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4eb7a84-faee-48cf-9775-cd39be16180e"
      },
      "source": [
        "trainset, testset = train_test_split(dataset, test_size=0.15, random_state=1234)\n",
        "trainset, validset = train_test_split(trainset, test_size=0.1, random_state=1234)\n",
        "trainset.shape, testset.shape, validset.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((134349, 2), (26344, 2), (14928, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQqmpb9_Jubp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeGhmp95L4HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write(dataframe, dataset_name, folder=\"dataset\"):\n",
        "    en = dataframe[\"English words/sentences\"]\n",
        "    french = dataframe[\"French words/sentences\"]\n",
        "    en.to_csv('{}/{}.en'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
        "    french.to_csv('{}/{}.fr'.format(folder, dataset_name), index=False, header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcUeqNXQJzMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write(trainset, \"train\")\n",
        "write(testset, \"valid\")\n",
        "write(validset, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmleyKnrhSht",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agiCKuON6JF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "fe60e5de-3c97-438c-c361-4590b837224e"
      },
      "source": [
        "!fairseq-preprocess --source-lang en --target-lang fr \\\n",
        "     --trainpref dataset/train --testpref dataset/test --validpref dataset/valid\\\n",
        "     --destdir preprocessed_data --seed 1234 --nwordssrc 25000 --nwordstgt 25000"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='preprocessed_data', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=25000, nwordstgt=25000, only_source=False, optimizer='nag', padding_factor=8, seed=1234, source_lang='en', srcdict=None, target_lang='fr', task='translation', tensorboard_logdir='', testpref='dataset/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='dataset/train', user_dir=None, validpref='dataset/valid', workers=1)\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/train.en: 134349 sents, 969752 tokens, 0.0% replaced by <unk>\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/valid.en: 26344 sents, 190333 tokens, 0.871% replaced by <unk>\n",
            "| [en] Dictionary: 23319 types\n",
            "| [en] dataset/test.en: 14928 sents, 107474 tokens, 0.929% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/train.fr: 134349 sents, 1045841 tokens, 1.19% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/valid.fr: 26344 sents, 205246 tokens, 2.44% replaced by <unk>\n",
            "| [fr] Dictionary: 24999 types\n",
            "| [fr] dataset/test.fr: 14928 sents, 115853 tokens, 2.5% replaced by <unk>\n",
            "| Wrote preprocessed data to preprocessed_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9jMDRJahZz8",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB1puWhkhdOx",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebeOxkAhnvE",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcN5ahW59MPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09ff8546-3931-4432-b45d-876027103037"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/  --clip-norm 5 --batch-size 64 \\\n",
        "                      --save-dir checkpoints/lstm --arch lstm --max-epoch 15 --encoder-hidden-size 258 \\\n",
        "                      --encoder-layers 2  --decoder-hidden-size 258 --decoder-layers 2 --optimizer adam --lr 0.001  \\\n",
        "                      --dropout 0.3 --encoder-bidirectional --encoder-embed-dim 300 \\\n",
        "                      --decoder-embed-dim 300 --no-epoch-checkpoints --decoder-out-embed-dim 300 \\\n",
        "                      --num-workers 3"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', arch='lstm', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=5.0, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, decoder_embed_dim=300, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=258, decoder_layers=2, decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_bidirectional=True, encoder_dropout_in=0.3, encoder_dropout_out=0.3, encoder_embed_dim=300, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=258, encoder_layers=2, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/lstm', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (embed_tokens): Embedding(23320, 300, padding_idx=1)\n",
            "    (lstm): LSTM(300, 258, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (embed_tokens): Embedding(25000, 300, padding_idx=1)\n",
            "    (encoder_hidden_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (encoder_cell_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(558, 258)\n",
            "      (1): LSTMCell(258, 258)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=258, out_features=516, bias=False)\n",
            "      (output_proj): Linear(in_features=774, out_features=258, bias=False)\n",
            "    )\n",
            "    (additional_fc): Linear(in_features=258, out_features=300, bias=True)\n",
            "    (fc_out): Linear(in_features=300, out_features=25000, bias=True)\n",
            "  )\n",
            ")\n",
            "| model lstm, criterion CrossEntropyCriterion\n",
            "| num. model params: 26834548 (num. trained: 26834548)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| no existing checkpoint found checkpoints/lstm/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 001:   2% 43/2100 [00:04<02:34, 13.27it/s, loss=11.095, ppl=2186.77, wps=6260, ups=12, wpb=526.023, bsz=64.000, num_updates=43, lr=0.001, gnorm=3.126, clip=0.140, oom=0.000, wall=5, train_wall=4]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [16] is 16 which does not match the computed number of elements 19. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (19,).\n",
            "| epoch 001:   6% 129/2100 [00:10<02:32, 12.93it/s, loss=9.780, ppl=878.94, wps=6139, ups=12, wpb=502.171, bsz=64.000, num_updates=129, lr=0.001, gnorm=2.171, clip=0.062, oom=0.000, wall=12, train_wall=10]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [19] is 19 which does not match the computed number of elements 45. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (45,).\n",
            "| epoch 001 | loss 5.564 | ppl 47.32 | wps 6236 | ups 13 | wpb 498.020 | bsz 63.976 | num_updates 2100 | lr 0.001 | gnorm 1.162 | clip 0.004 | oom 0.000 | wall 169 | train_wall 160\n",
            "| epoch 001 | valid on 'valid' subset:  99% 409/412 [00:09<00:00, 27.93it/s]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [45] is 45 which does not match the computed number of elements 47. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (47,).\n",
            "| epoch 001 | valid on 'valid' subset | loss 3.386 | ppl 10.45 | num_updates 2100\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 1 @ 2100 updates) (writing took 1.9189412593841553 seconds)\n",
            "| epoch 002 | loss 3.341 | ppl 10.14 | wps 6208 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 4200 | lr 0.001 | gnorm 1.019 | clip 0.000 | oom 0.000 | wall 350 | train_wall 320\n",
            "| epoch 002 | valid on 'valid' subset | loss 2.518 | ppl 5.73 | num_updates 4200 | best_loss 2.51799\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 2 @ 4200 updates) (writing took 1.9292066097259521 seconds)\n",
            "| epoch 003 | loss 2.675 | ppl 6.38 | wps 6219 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 6300 | lr 0.001 | gnorm 0.966 | clip 0.000 | oom 0.000 | wall 530 | train_wall 480\n",
            "| epoch 003 | valid on 'valid' subset | loss 2.231 | ppl 4.7 | num_updates 6300 | best_loss 2.23115\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 3 @ 6300 updates) (writing took 1.888312578201294 seconds)\n",
            "| epoch 004 | loss 2.306 | ppl 4.95 | wps 6212 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 8400 | lr 0.001 | gnorm 0.925 | clip 0.000 | oom 0.000 | wall 710 | train_wall 639\n",
            "| epoch 004 | valid on 'valid' subset | loss 2.030 | ppl 4.08 | num_updates 8400 | best_loss 2.02976\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 4 @ 8400 updates) (writing took 1.8792660236358643 seconds)\n",
            "| epoch 005 | loss 2.069 | ppl 4.2 | wps 6199 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 10500 | lr 0.001 | gnorm 0.895 | clip 0.000 | oom 0.000 | wall 891 | train_wall 800\n",
            "| epoch 005 | valid on 'valid' subset | loss 1.929 | ppl 3.81 | num_updates 10500 | best_loss 1.92928\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 5 @ 10500 updates) (writing took 1.8253521919250488 seconds)\n",
            "| epoch 006 | loss 1.897 | ppl 3.72 | wps 6206 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 12600 | lr 0.001 | gnorm 0.870 | clip 0.000 | oom 0.000 | wall 1071 | train_wall 960\n",
            "| epoch 006 | valid on 'valid' subset | loss 1.858 | ppl 3.63 | num_updates 12600 | best_loss 1.85847\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 6 @ 12600 updates) (writing took 2.1757524013519287 seconds)\n",
            "| epoch 007 | loss 1.768 | ppl 3.41 | wps 6207 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 14700 | lr 0.001 | gnorm 0.850 | clip 0.000 | oom 0.000 | wall 1252 | train_wall 1120\n",
            "| epoch 007 | valid on 'valid' subset | loss 1.836 | ppl 3.57 | num_updates 14700 | best_loss 1.83627\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 7 @ 14700 updates) (writing took 1.9732916355133057 seconds)\n",
            "| epoch 008 | loss 1.665 | ppl 3.17 | wps 6208 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 16800 | lr 0.001 | gnorm 0.832 | clip 0.000 | oom 0.000 | wall 1432 | train_wall 1280\n",
            "| epoch 008 | valid on 'valid' subset | loss 1.797 | ppl 3.47 | num_updates 16800 | best_loss 1.7965\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 8 @ 16800 updates) (writing took 1.9086954593658447 seconds)\n",
            "| epoch 009 | loss 1.588 | ppl 3.01 | wps 6213 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 18900 | lr 0.001 | gnorm 0.820 | clip 0.000 | oom 0.000 | wall 1612 | train_wall 1439\n",
            "| epoch 009 | valid on 'valid' subset | loss 1.762 | ppl 3.39 | num_updates 18900 | best_loss 1.76191\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 9 @ 18900 updates) (writing took 1.8824272155761719 seconds)\n",
            "| epoch 010 | loss 1.518 | ppl 2.86 | wps 6215 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 21000 | lr 0.001 | gnorm 0.808 | clip 0.000 | oom 0.000 | wall 1793 | train_wall 1599\n",
            "| epoch 010 | valid on 'valid' subset | loss 1.745 | ppl 3.35 | num_updates 21000 | best_loss 1.74522\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 10 @ 21000 updates) (writing took 1.8681750297546387 seconds)\n",
            "| epoch 011 | loss 1.456 | ppl 2.74 | wps 6189 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 23100 | lr 0.001 | gnorm 0.796 | clip 0.000 | oom 0.000 | wall 1973 | train_wall 1759\n",
            "| epoch 011 | valid on 'valid' subset | loss 1.765 | ppl 3.4 | num_updates 23100 | best_loss 1.74522\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 11 @ 23100 updates) (writing took 0.7003545761108398 seconds)\n",
            "| epoch 012 | loss 1.408 | ppl 2.65 | wps 6199 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 25200 | lr 0.001 | gnorm 0.784 | clip 0.000 | oom 0.000 | wall 2153 | train_wall 1920\n",
            "| epoch 012 | valid on 'valid' subset | loss 1.732 | ppl 3.32 | num_updates 25200 | best_loss 1.732\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 12 @ 25200 updates) (writing took 1.995889663696289 seconds)\n",
            "| epoch 013 | loss 1.362 | ppl 2.57 | wps 6201 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 27300 | lr 0.001 | gnorm 0.777 | clip 0.000 | oom 0.000 | wall 2334 | train_wall 2080\n",
            "| epoch 013 | valid on 'valid' subset | loss 1.723 | ppl 3.3 | num_updates 27300 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 13 @ 27300 updates) (writing took 1.8983409404754639 seconds)\n",
            "| epoch 014 | loss 1.324 | ppl 2.5 | wps 6172 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 29400 | lr 0.001 | gnorm 0.768 | clip 0.000 | oom 0.000 | wall 2515 | train_wall 2241\n",
            "| epoch 014 | valid on 'valid' subset | loss 1.731 | ppl 3.32 | num_updates 29400 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 14 @ 29400 updates) (writing took 0.6554539203643799 seconds)\n",
            "| epoch 015 | loss 1.287 | ppl 2.44 | wps 6194 | ups 12 | wpb 498.020 | bsz 63.976 | num_updates 31500 | lr 0.001 | gnorm 0.761 | clip 0.000 | oom 0.000 | wall 2695 | train_wall 2401\n",
            "| epoch 015 | valid on 'valid' subset | loss 1.747 | ppl 3.36 | num_updates 31500 | best_loss 1.72332\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 15 @ 31500 updates) (writing took 0.6092920303344727 seconds)\n",
            "| done training in 2704.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6tiPCl6hhtF",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzHkHB4KF2ZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "f719f642-26df-47d9-e12d-05e1a45df8eb"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/lstm/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > lstm_last.out"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2% 5/234 [00:01<00:46,  4.91it/s, wps=1228]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [3] is 3 which does not match the computed number of elements 4. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (4,).\n",
            " 10% 23/234 [00:04<00:50,  4.19it/s, wps=1363]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [4] is 4 which does not match the computed number of elements 5. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (5,).\n",
            " 25% 58/234 [00:13<00:49,  3.57it/s, wps=1375]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [5] is 5 which does not match the computed number of elements 6. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (6,).\n",
            " 44% 102/234 [00:27<00:42,  3.10it/s, wps=1370]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [6] is 6 which does not match the computed number of elements 7. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (7,).\n",
            " 62% 145/234 [00:41<00:29,  2.98it/s, wps=1404]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [7] is 7 which does not match the computed number of elements 8. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (8,).\n",
            " 76% 177/234 [00:51<00:20,  2.81it/s, wps=1435]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [8] is 8 which does not match the computed number of elements 9. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (9,).\n",
            " 85% 199/234 [01:00<00:15,  2.23it/s, wps=1447]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [9] is 9 which does not match the computed number of elements 10. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (10,).\n",
            " 91% 212/234 [01:06<00:09,  2.25it/s, wps=1449]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [10] is 10 which does not match the computed number of elements 11. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (11,).\n",
            " 94% 220/234 [01:09<00:06,  2.15it/s, wps=1457]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [11] is 11 which does not match the computed number of elements 12. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (12,).\n",
            " 96% 225/234 [01:12<00:04,  2.13it/s, wps=1463]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [12] is 12 which does not match the computed number of elements 13. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (13,).\n",
            " 97% 227/234 [01:13<00:03,  1.92it/s, wps=1463]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [13] is 13 which does not match the computed number of elements 14. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (14,).\n",
            " 98% 229/234 [01:14<00:02,  1.94it/s, wps=1466]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [14] is 14 which does not match the computed number of elements 15. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (15,).\n",
            " 99% 231/234 [01:15<00:01,  1.84it/s, wps=1469]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [15] is 15 which does not match the computed number of elements 17. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (17,).\n",
            " 99% 232/234 [01:15<00:01,  1.80it/s, wps=1471]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [17] is 17 which does not match the computed number of elements 21. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (21,).\n",
            "100% 233/234 [01:16<00:00,  1.69it/s, wps=1474]/pytorch/aten/src/ATen/native/RangeFactories.cpp:170: UserWarning: The number of elements in the out tensor of shape [21] is 21 which does not match the computed number of elements 34. Note that this may occur as a result of rounding error. The out tensor will be resized to a tensor of shape (34,).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GneTJgbugZIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a3f10f2-b862-49b5-9f6d-45118618e3f7"
      },
      "source": [
        "!grep ^H lstm_last.out | cut -f3- > lstm_last.out.sys\n",
        "!grep ^T lstm_last.out | cut -f2- > lstm_last.out.ref\n",
        "!fairseq-score --sys lstm_last.out.sys --ref lstm_last.out.ref --ignore-case"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='lstm_last.out.ref', sacrebleu=False, sentence_bleu=False, sys='lstm_last.out.sys')\n",
            "BLEU4 = 43.91, 68.4/50.6/39.0/30.7 (BP=0.973, ratio=0.974, syslen=98285, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9W9AziYrZsi",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSH2elWIrpvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef154b31-7ce6-4647-cd55-1141f622a7db"
      },
      "source": [
        "!tail -n 100 lstm_last.out"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H-10523\t-0.3092171549797058\tÀ la fin d'un travail , tout le monde est pressé de rentrer chez lui.\n",
            "P-10523\t-1.3950 -0.1059 -0.0389 -0.3817 -0.4143 -0.5681 -0.3332 -0.1484 -0.0013 -0.0155 -0.0579 -0.1914 -0.2386 -0.5319 -0.5254 0.0000\n",
            "S-4980\tThe only reason Tom went to the party was that he expected Mary to be there.\n",
            "T-4980\tLa seule raison pour laquelle Tom est venu à la fête était qu'il espérait que Marie soit <<unk>>\n",
            "H-4980\t-0.36923620104789734\tLa seule raison raison , Tom est allé à la fête qu'il espérait que Marie soit là.\n",
            "P-4980\t-0.0055 -0.0055 -0.0000 -0.7288 -0.9190 -0.0065 -0.8930 -0.5125 -0.1288 -0.0066 -0.0221 -0.5502 -1.6436 -0.0478 -0.6189 -0.4241 -0.1333 -0.0000\n",
            "S-9787\tIt's no wonder Tom's sleeping <unk> he drinks up to twelve cups of coffee a day.\n",
            "T-9787\tCe n'est pas surprenant que Tom dorme mal , il boit jusqu'à douze tasses de café par jour.\n",
            "H-9787\t-0.595396101474762\tCe n'est pas étonnant que Tom dorme un verre de café à condition douze jours.\n",
            "P-9787\t-0.2619 -0.0016 -0.0111 -0.8643 -0.0453 -0.0035 -1.3139 -0.5687 -0.4753 -0.0994 -0.0056 -1.4583 -2.2704 -1.0734 -1.0735 0.0000\n",
            "S-10808\tThough it was a muggy night , she went to bed with all the windows closed.\n",
            "T-10808\tBien que ce fût une nuit <<unk>> , elle est allée au lit avec toutes les fenêtres fermées.\n",
            "H-10808\t-0.2141532450914383\tBien que c'était une nuit lourd , elle est allée au lit avec toutes les fenêtres fermés.\n",
            "P-10808\t-0.0361 -0.0635 -0.7582 -0.0529 -0.3345 -0.8319 -0.0129 -0.0132 -0.7616 -0.2122 -0.2624 -0.0312 -0.0201 -0.3298 -0.0021 -0.0000 -0.1321 0.0000\n",
            "S-6263\tYou're the only person I know besides me who would prefer to live in a tent.\n",
            "T-6263\tTu es la seule personne que je connaisse , à part moi , qui préférerais vivre dans une tente.\n",
            "H-6263\t-0.09729104489088058\tTu es la seule personne que je connaisse à part moi qui préférerais vivre dans une tente.\n",
            "P-6263\t-0.5059 -0.0016 -0.0014 -0.0111 -0.0002 -0.1163 -0.0150 -0.0434 -0.1019 -0.0354 -0.0183 -0.4467 -0.4497 -0.0023 -0.0007 -0.0013 -0.0001 0.0000\n",
            "S-12129\tI thought I was about to be captured so I ran as fast as I could.\n",
            "T-12129\tJe pensais que j'étais sur le point d'être capturée , alors je courus aussi vite que je le pus.\n",
            "H-12129\t-0.2322169840335846\tJe pensais que j'étais sur le point d'être capturé , alors j'ai couru aussi vite que je le pouvais.\n",
            "P-12129\t-0.0155 -0.0567 -0.1101 -0.1975 -0.3868 -0.0450 -0.0094 -0.0035 -0.8702 -0.6841 -0.5043 -0.4823 -0.0010 -0.0631 -0.0461 -0.0018 -0.4928 -0.6121 -0.0620 0.0000\n",
            "S-10886\tPlease don't make me laugh. I did too many sit-ups yesterday and my stomach muscles hurt.\n",
            "T-10886\tS'il te plait ne me fais pas rire. J'ai fait trop <<unk>> hier et mes muscles <<unk>> me font mal.\n",
            "H-10886\t-0.3982280492782593\tJe te prie de ne pas me faire rire. J'ai fait trop <unk> , hier et mes muscles <unk> mal.\n",
            "P-10886\t-1.9304 -0.4362 -0.0376 -0.0007 -0.0072 -0.1315 -0.0190 -0.1566 -0.2404 -0.4613 -0.0026 -0.0376 -0.0118 -0.6816 -0.2040 -0.7573 -0.5343 -0.0004 -1.2893 -1.4231 0.0000\n",
            "S-4502\tI don't think it'll rain , but I'll take an umbrella just in case it does.\n",
            "T-4502\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie juste au cas où cela se <<unk>>\n",
            "H-4502\t-0.14987024664878845\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie.\n",
            "P-4502\t-0.0075 -0.0183 -0.0283 -0.0220 -0.0105 -0.6233 -0.0369 -0.0416 -0.0031 -0.0709 -0.5187 -0.0726 -0.0116 -0.7518 -0.0310\n",
            "S-319\tI thought if I broke up with you , I'd never have to see you again.\n",
            "T-319\tJe pensais que si je <<unk>> avec toi , je n'aurais plus jamais à te revoir.\n",
            "H-319\t-0.26804235577583313\tJe pensais si je me suis rompu avec vous , je ne devrais jamais vous revoir.\n",
            "P-319\t-0.0978 -0.1009 -1.0724 -0.0216 -0.3855 -0.0640 -1.2317 -0.0491 -0.4439 -0.0027 -0.0087 -0.0505 -0.4893 -0.0175 -0.5025 -0.0186 0.0000\n",
            "S-5700\tIt's still too hard to find a job. And even if you have a job , chances are you're having a tougher time paying the rising costs of everything from <unk> to gas.\n",
            "T-5700\tC'est encore trop difficile de trouver un emploi. Et même quand on en a un , il y a des chances qu'on ait davantage de difficultés à payer le coût de tout , de <<unk>> au gaz.\n",
            "H-5700\t-0.5950573086738586\tC'est encore trop difficile pour trouver un boulot , même si tu as un boulot , mais vous avez un boulot de temps en <unk>\n",
            "P-5700\t-0.6696 -0.3579 -0.0034 -0.0711 -0.3546 -0.0152 -0.0116 -0.4062 -0.1495 -0.6232 -0.0018 -0.3676 -0.3742 -0.0874 -0.5002 -0.3835 -1.3831 -0.9486 -0.7781 -0.9716 -1.2587 -0.8680 -1.2014 -1.1214 -0.9728 -1.5910\n",
            "S-10885\tIt was bad enough that he usually came to work late , but coming in drunk was the last straw , and I'm going to have to let him go.\n",
            "T-10885\tC'était suffisamment grave qu'il ait l'habitude d'arriver en retard au travail , mais qu'il arrive soûl est un comble , et je vais devoir m'en séparer.\n",
            "H-10885\t-0.7556149959564209\tIl fut assez mauvais qu'il est venu en retard , mais je ne vais pas étudier en retard , mais je vais le laisser partir.\n",
            "P-10885\t-0.4331 -1.0967 -0.6396 -0.0999 -0.5489 -1.4321 -0.6969 -1.4414 -0.2558 -0.3193 -0.0098 -0.5201 -2.8518 -0.3701 -0.3946 -1.4623 -0.9579 -0.4928 -0.4849 -0.0516 -0.1213 -0.1185 -2.7304 -0.7580 -1.3581 -0.0000\n",
            "S-6030\tIn the same amount of time it would take me to correct all the mistakes in your report , I could write a better report myself.\n",
            "T-6030\tDans le même <<unk>> de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi-même.\n",
            "H-6030\t-0.3895154893398285\tDans la même fois qu'il <unk> à corriger toutes les erreurs , je pourrais écrire un meilleur rapport dans la tête.\n",
            "P-6030\t-0.2692 -0.0210 -0.0001 -0.4367 -1.0404 -0.5308 -1.0081 -0.0836 -0.6450 -0.0011 -0.0018 -0.2731 -0.3790 -0.8338 -0.0742 -0.1003 -0.1233 -0.0491 -1.0774 -1.3290 -0.2924 0.0000\n",
            "S-4762\tDid you know that in Japan , if you have a tattoo , you won't be allowed to bathe in many of the hot spring <unk>\n",
            "T-4762\tSaviez-vous qu'au Japon , si vous portez un tatouage , vous ne serez pas autorisé à vous baigner dans beaucoup des <<unk>> <<unk>> ?\n",
            "H-4762\t-0.4053613841533661\tSavais-tu cela au Japon , si tu as un tatouage , si tu auras un tatouage , vous ne serez pas autorisé à <unk> dans beaucoup de printemps , si vous ne serez pas <unk>\n",
            "P-4762\t-0.8459 -1.2334 -0.1052 -0.0001 -0.0101 -0.0218 -0.5053 -0.5076 -0.0049 -0.0013 -0.1564 -0.5802 -0.4033 -0.9214 -0.0618 -0.0072 -0.9607 -0.8450 -0.0342 -0.0052 -0.0018 -0.9527 -0.0074 -0.5582 -0.5079 -0.0973 -0.2682 -0.0814 -0.4304 -1.6255 -1.0822 -0.2639 -0.0013 -0.0030 -0.8837 -0.6174\n",
            "S-11299\tI know that it's highly unlikely that you'd ever want to go out with me , but I still need to ask at least once.\n",
            "T-11299\tJe sais qu'il est très improbable que tu veuilles jamais sortir avec moi , mais je dois quand même demander au moins une fois.\n",
            "H-11299\t-0.17841894924640656\tJe sais qu'il est hautement improbable que vous vouliez jamais sortir avec moi , mais j'ai encore besoin de demander au moins une fois.\n",
            "P-11299\t-0.0051 -0.0065 -0.0806 -0.0087 -0.0029 -0.0112 -0.0022 -0.9706 -0.9093 -0.0667 -0.4964 -0.0400 -0.0009 -0.0297 -0.0035 -0.5183 -0.1709 -0.0007 -0.0542 -0.1245 -0.6647 -0.0282 -0.2014 -0.0633 -0.0000\n",
            "S-9140\tIf you don't eat breakfast , you'll probably be hungry during the morning and won't be as efficient at work as you could be.\n",
            "T-9140\tSi tu ne <<unk>> pas , tu auras probablement faim au cours de la matinée et tu ne seras pas aussi efficace au travail que tu pourrais l'être.\n",
            "H-9140\t-0.31194645166397095\tSi vous ne prenez pas le petit déjeuner , tu auras probablement faim pendant le matin et vous ne serez pas aussi efficace au travail.\n",
            "P-9140\t-0.0009 -0.6038 -0.0634 -0.5961 -0.0168 -0.2219 -0.4228 -0.0223 -0.0029 -0.6202 -0.8722 -0.1905 -0.0008 -0.2572 -0.3038 -0.3397 -0.0290 -1.8965 -0.5640 -0.0237 -0.0732 -0.1440 -0.0577 -0.2077 -0.5791 -0.0002\n",
            "S-13683\tIf you don't want to put on sunscreen , that's your problem. Just don't come complaining to me when you get a sunburn.\n",
            "T-13683\tSi tu ne veux pas mettre de crème solaire c'est ton problème , mais ne viens pas te plaindre quand <<unk>> des coups de soleil.\n",
            "H-13683\t-0.4888012707233429\tSi tu ne veux pas mettre de crème solaire , c'est ton problème , juste quand tu auras un coup de soleil.\n",
            "P-13683\t-0.0022 -0.4659 -0.0278 -0.0118 -0.0020 -1.5814 -0.0823 -0.0091 -0.0709 -0.0006 -0.2368 -1.7831 -0.2228 -0.5850 -2.1608 -1.3105 -0.5808 -1.3423 -0.0111 -0.0370 -0.0188 -0.6993 -0.0000\n",
            "S-13070\tDuring hard times , people might not go on a trip , but they might be willing to pay extra for good coffee.\n",
            "T-13070\tQuand les temps sont durs , les gens peuvent ne pas partir en voyage mais ils peuvent être disposés à payer davantage pour du café de bonne qualité.\n",
            "H-13070\t-0.5406641960144043\tPendant la fois de nombreuses fois , les gens ne sont pas en voyage , mais ils se sont <unk> pour payer un bon café.\n",
            "P-13070\t-0.3247 -2.0434 -0.9499 -0.5002 -1.9616 -0.0350 -0.2147 -0.0394 -0.0261 -0.7406 -0.4989 -0.0275 -1.1779 -0.1581 -0.1574 -0.0087 -1.5190 -1.0986 -0.3093 -1.1500 -0.5654 -0.0909 -0.4264 -0.0246 -0.0089 0.0000\n",
            "S-10827\tI still have a scar on my left leg from a car accident I was in when I was thirteen years old.\n",
            "T-10827\tJ'ai encore une cicatrice sur ma jambe gauche à la suite d'un accident de voiture dans lequel j'ai été impliqué quand j'avais treize ans.\n",
            "H-10827\t-0.21723514795303345\tJ'ai encore une cicatrice sur la jambe gauche.\n",
            "P-10827\t-0.0997 -0.3617 -0.0402 -0.0003 -0.0472 -0.6609 -0.0083 -0.7345 -0.0023\n",
            "S-13283\tI never see a library without wishing I had time to go there and stay till I had read everything in it.\n",
            "T-13283\tJe ne vois jamais une bibliothèque sans souhaiter avoir le temps de m'y rendre et d'y rester jusqu'à ce que j'y aie tout lu.\n",
            "H-13283\t-0.607856273651123\tJe ne vois jamais une bibliothèque sans <unk> que j'ai eu le temps d'y aller , et de rester jusqu'à ce que je devais tout lire.\n",
            "P-13283\t-0.0580 -0.2052 -0.0098 -0.0065 -0.1251 -0.0392 -0.0001 -0.2895 -2.1609 -0.9237 -1.3194 -0.3525 -0.0249 -1.1650 -0.4096 -1.0684 -0.0732 -2.0596 -1.0305 -1.1899 -0.0642 -0.6192 -1.2240 -1.6706 -0.2727 -0.0503 0.0000\n",
            "S-4837\tPolice are <unk> people not to pick up <unk> as they search for two prisoners on the run after escaping from jail.\n",
            "T-4837\tLa police <<unk>> les gens à ne pas prendre <<unk>> car ils recherchent deux prisonniers en cavale après leur <<unk>>\n",
            "H-4837\t-0.7455959320068359\tLa police est <unk> aux gens de ne pas prendre les <unk> , comme ils <unk> deux prisonniers sur les <unk> de la prison.\n",
            "P-4837\t-0.0012 -0.0045 -0.2320 -0.1504 -1.1384 -0.0169 -0.8876 -0.0699 -0.0110 -0.9242 -0.7453 -1.6871 -1.4357 -2.4364 -1.6325 -0.7937 -0.3436 -0.0498 -0.6521 -0.9672 -1.1275 -1.3427 -1.2668 -0.7232 0.0000\n",
            "S-248\tWhen we started out <unk> web pages , we were the only ones doing it in this part of the country.\n",
            "T-248\tQuand nous avons commencé à concevoir des pages web , nous étions les seuls à le faire dans cette partie du pays.\n",
            "H-248\t-0.5113544464111328\tLorsque nous avons commencé à <unk> <unk> , nous étions les seules à le faire dans cette partie de la campagne.\n",
            "P-248\t-0.4518 -0.0127 -0.2498 -0.2641 -0.3244 -1.3885 -2.1747 -0.2284 -0.1176 -0.3180 -0.5396 -0.1679 -1.2175 -1.0514 -0.2821 -1.1703 -0.1228 -0.0311 -0.6858 -0.2932 -0.1582 0.0000\n",
            "S-11828\tI was looking forward to seeing a scenic view of Mt. Fuji , but unfortunately it was completely hidden behind clouds.\n",
            "T-11828\tJ'avais hâte de voir une vue <<unk>> du mont Fuji mais , malheureusement , il était complètement caché par les nuages.\n",
            "H-11828\t-0.3577519357204437\tJ'étais impatiente de voir une vue malade du Mont Fuji , mais tout le monde fut complètement caché derrière les nuages.\n",
            "P-11828\t-0.5648 -0.1464 -0.0625 -0.1662 -0.0198 -0.1668 -2.5503 -0.3932 -0.2556 -0.0340 -0.0202 -0.0174 -1.4024 -0.2092 -0.1026 -0.9909 -0.2960 -0.4603 -0.0005 -0.0065 -0.0048 0.0000\n",
            "S-7610\tI tried to stay inside the house all day , but I ended up going outside and sitting in the garden.\n",
            "T-7610\tJ'ai essayé de rester dans la maison toute la journée , mais j'ai fini par sortir m'asseoir dans le jardin.\n",
            "H-7610\t-0.26735180616378784\tJ'ai essayé de rester à l'intérieur toute la maison , mais j'ai fini par sortir dans le jardin.\n",
            "P-7610\t-0.1674 -0.0489 -0.0659 -0.0205 -0.2618 -0.0071 -0.1632 -0.0007 -0.2752 -0.0721 -0.0035 -0.1481 -1.4353 -0.5221 -0.1740 -1.1388 -0.4853 -0.0900 -0.0000\n",
            "S-6513\t<unk> told police that the train was <unk> way over the speed limit when it <unk> going around a <unk>\n",
            "T-6513\tDes <<unk>> ont déclaré à la police que le train roulait bien au-dessus de la limite de vitesse lorsqu'il a <<unk>> dans un <<unk>>\n",
            "H-6513\t-0.6118181347846985\tOn dit que le train a dit que le train était <unk> à la vitesse de la vitesse quand ça <unk> autour d'un <unk>\n",
            "P-6513\t-1.3675 -0.0101 -0.0647 -0.1152 -0.0000 -1.3611 -0.5313 -0.0266 -0.0625 -0.0000 -0.5073 -1.4963 -1.9704 -0.5262 -0.2134 -0.9079 -0.4953 -0.1088 -1.4005 -1.9786 -0.3412 -0.5282 -0.8028 -0.4793 -0.0001\n",
            "S-10561\tI suppose you want to ask me how I was able to make so much money in so little time.\n",
            "T-10561\tJe suppose que vous voulez me demander comment j'ai été en mesure de me faire autant d'argent en si peu de temps.\n",
            "H-10561\t-0.3360203504562378\tJe suppose que tu veux me demander comment j'ai été en mesure de faire autant d'argent dans un peu de temps.\n",
            "P-10561\t-0.0055 -0.0223 -0.0027 -0.6870 -0.0366 -0.0006 -0.0042 -0.0061 -1.3233 -0.0723 -0.7302 -0.0031 -0.0276 -0.9772 -0.2762 -0.1041 -0.8012 -0.3717 -0.5554 -1.3743 -0.0108 0.0000\n",
            "| Translated 14928 sentences (113213 tokens) in 25.7s (580.01 sentences/s, 4398.79 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 43.71, 67.9/50.4/38.8/30.6 (BP=0.973, ratio=0.974, syslen=98285, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsL5n7FHhq12",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5EquxnviIHo",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXVAic1uhIDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3093c4f4-63b9-49c1-f1ab-e85972bd7453"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/ \\\n",
        "     --lr 0.001 --clip-norm 0.1 --dropout 0.3 --max-epoch 15 --optimizer adam\\\n",
        "     --arch fconv_iwslt_de_en --save-dir checkpoints/fconv --batch-size 128 --no-epoch-checkpoints \\\n",
        "     --encoder-embed-dim 300 --decoder-embed-dim 300 --decoder-out-embed-dim 300 --num-workers 3"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, arch='fconv_iwslt_de_en', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='cross_entropy', curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=300, decoder_embed_path=None, decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_embed_dim=300, encoder_embed_path=None, encoder_layers='[(256, 3)] * 4', fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_sentences=128, max_sentences_valid=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/fconv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (embed_tokens): Embedding(23320, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "      (3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (embed_tokens): Embedding(25000, 300, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "    (fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "      (2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "        (out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "    (fc3): Linear(in_features=300, out_features=25000, bias=True)\n",
            "  )\n",
            ")\n",
            "| model fconv_iwslt_de_en, criterion CrossEntropyCriterion\n",
            "| num. model params: 26193640 (num. trained: 26193640)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 128\n",
            "| no existing checkpoint found checkpoints/fconv/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 001 | loss 5.623 | ppl 49.3 | wps 11121 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 1050 | lr 0.001 | gnorm 1.162 | clip 1.000 | oom 0.000 | wall 96 | train_wall 90\n",
            "| epoch 001 | valid on 'valid' subset | loss 3.422 | ppl 10.72 | num_updates 1050\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 1 @ 1050 updates) (writing took 1.7692511081695557 seconds)\n",
            "| epoch 002 | loss 3.201 | ppl 9.19 | wps 11046 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 2100 | lr 0.001 | gnorm 0.877 | clip 1.000 | oom 0.000 | wall 199 | train_wall 180\n",
            "| epoch 002 | valid on 'valid' subset | loss 2.491 | ppl 5.62 | num_updates 2100 | best_loss 2.49082\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 2 @ 2100 updates) (writing took 1.7268321514129639 seconds)\n",
            "| epoch 003 | loss 2.453 | ppl 5.47 | wps 11070 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 3150 | lr 0.001 | gnorm 0.818 | clip 1.000 | oom 0.000 | wall 301 | train_wall 270\n",
            "| epoch 003 | valid on 'valid' subset | loss 2.186 | ppl 4.55 | num_updates 3150 | best_loss 2.18624\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 3 @ 3150 updates) (writing took 1.799570083618164 seconds)\n",
            "| epoch 004 | loss 2.037 | ppl 4.1 | wps 11061 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 4200 | lr 0.001 | gnorm 0.770 | clip 1.000 | oom 0.000 | wall 404 | train_wall 360\n",
            "| epoch 004 | valid on 'valid' subset | loss 2.022 | ppl 4.06 | num_updates 4200 | best_loss 2.02162\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 4 @ 4200 updates) (writing took 1.7686834335327148 seconds)\n",
            "| epoch 005 | loss 1.780 | ppl 3.43 | wps 11057 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 5250 | lr 0.001 | gnorm 0.734 | clip 1.000 | oom 0.000 | wall 506 | train_wall 450\n",
            "| epoch 005 | valid on 'valid' subset | loss 1.955 | ppl 3.88 | num_updates 5250 | best_loss 1.95471\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 5 @ 5250 updates) (writing took 1.7555854320526123 seconds)\n",
            "| epoch 006 | loss 1.596 | ppl 3.02 | wps 11042 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 6300 | lr 0.001 | gnorm 0.702 | clip 1.000 | oom 0.000 | wall 609 | train_wall 540\n",
            "| epoch 006 | valid on 'valid' subset | loss 1.904 | ppl 3.74 | num_updates 6300 | best_loss 1.90355\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 6 @ 6300 updates) (writing took 1.7877705097198486 seconds)\n",
            "| epoch 007 | loss 1.460 | ppl 2.75 | wps 10987 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 7350 | lr 0.001 | gnorm 0.674 | clip 1.000 | oom 0.000 | wall 712 | train_wall 631\n",
            "| epoch 007 | valid on 'valid' subset | loss 1.840 | ppl 3.58 | num_updates 7350 | best_loss 1.83965\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 7 @ 7350 updates) (writing took 1.701216220855713 seconds)\n",
            "| epoch 008 | loss 1.352 | ppl 2.55 | wps 10982 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 8400 | lr 0.001 | gnorm 0.649 | clip 1.000 | oom 0.000 | wall 815 | train_wall 722\n",
            "| epoch 008 | valid on 'valid' subset | loss 1.814 | ppl 3.52 | num_updates 8400 | best_loss 1.81382\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 8 @ 8400 updates) (writing took 1.7253515720367432 seconds)\n",
            "| epoch 009 | loss 1.269 | ppl 2.41 | wps 11071 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 9450 | lr 0.001 | gnorm 0.629 | clip 1.000 | oom 0.000 | wall 918 | train_wall 811\n",
            "| epoch 009 | valid on 'valid' subset | loss 1.783 | ppl 3.44 | num_updates 9450 | best_loss 1.78253\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 9 @ 9450 updates) (writing took 1.7163794040679932 seconds)\n",
            "| epoch 010 | loss 1.198 | ppl 2.29 | wps 11044 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 10500 | lr 0.001 | gnorm 0.610 | clip 1.000 | oom 0.000 | wall 1021 | train_wall 902\n",
            "| epoch 010 | valid on 'valid' subset | loss 1.798 | ppl 3.48 | num_updates 10500 | best_loss 1.78253\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 10 @ 10500 updates) (writing took 0.6765499114990234 seconds)\n",
            "| epoch 011 | loss 1.134 | ppl 2.2 | wps 11052 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 11550 | lr 0.001 | gnorm 0.595 | clip 1.000 | oom 0.000 | wall 1122 | train_wall 992\n",
            "| epoch 011 | valid on 'valid' subset | loss 1.780 | ppl 3.43 | num_updates 11550 | best_loss 1.78029\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 11 @ 11550 updates) (writing took 1.6620514392852783 seconds)\n",
            "| epoch 012 | loss 1.083 | ppl 2.12 | wps 11009 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 12600 | lr 0.001 | gnorm 0.584 | clip 1.000 | oom 0.000 | wall 1225 | train_wall 1082\n",
            "| epoch 012 | valid on 'valid' subset | loss 1.779 | ppl 3.43 | num_updates 12600 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 12 @ 12600 updates) (writing took 1.7584583759307861 seconds)\n",
            "| epoch 013 | loss 1.034 | ppl 2.05 | wps 11057 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 13650 | lr 0.001 | gnorm 0.570 | clip 1.000 | oom 0.000 | wall 1327 | train_wall 1173\n",
            "| epoch 013 | valid on 'valid' subset | loss 1.795 | ppl 3.47 | num_updates 13650 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 13 @ 13650 updates) (writing took 0.6580889225006104 seconds)\n",
            "| epoch 014 | loss 0.996 | ppl 1.99 | wps 11046 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 14700 | lr 0.001 | gnorm 0.559 | clip 1.000 | oom 0.000 | wall 1429 | train_wall 1263\n",
            "| epoch 014 | valid on 'valid' subset | loss 1.787 | ppl 3.45 | num_updates 14700 | best_loss 1.77887\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_last.pt (epoch 14 @ 14700 updates) (writing took 0.6093218326568604 seconds)\n",
            "| epoch 015 | loss 0.959 | ppl 1.94 | wps 11056 | ups 11 | wpb 996.039 | bsz 127.951 | num_updates 15750 | lr 0.001 | gnorm 0.551 | clip 1.000 | oom 0.000 | wall 1531 | train_wall 1353\n",
            "| epoch 015 | valid on 'valid' subset | loss 1.768 | ppl 3.41 | num_updates 15750 | best_loss 1.76801\n",
            "| saved checkpoint checkpoints/fconv/checkpoint_best.pt (epoch 15 @ 15750 updates) (writing took 1.8601069450378418 seconds)\n",
            "| done training in 1537.1 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IyqxKTQlWLW",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa-ow3Y-iGUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a203011c-f18e-4055-b622-38d23ab475c8"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/fconv/checkpoint_last.pt \\\n",
        "    --batch-size 64 --beam 3 > conv_out.out"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R88p7yf2kg19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "55bccdee-a06f-448f-f437-ca82241b6185"
      },
      "source": [
        "!grep ^H conv_out.out | cut -f3- > conv_out.out.sys\n",
        "!grep ^T conv_out.out | cut -f2- > conv_out.out.ref\n",
        "!fairseq-score --sys conv_out.out.sys --ref conv_out.out.ref --ignore-case"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='conv_out.out.ref', sacrebleu=False, sentence_bleu=False, sys='conv_out.out.sys')\n",
            "BLEU4 = 42.56, 65.4/48.1/36.7/28.4 (BP=1.000, ratio=1.025, syslen=103402, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_kmbcq2rsWF",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tJq8k7NruSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072b34dc-5ad3-49c1-a008-4c2726292b3a"
      },
      "source": [
        "!tail -n 100 conv_out.out"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H-10523\t-0.36244139075279236\tÀ la fin de travail , tout le monde est pressé , tout le monde est pressé de rentrer à la maison.\n",
            "P-10523\t-1.2422 -0.1022 -0.0002 -0.1192 -0.6157 -0.0312 -0.6946 -0.0011 -0.0007 -0.5180 -0.5462 -0.3651 -0.3874 -0.0029 -0.0007 -0.8464 -0.6123 -0.2805 -0.7026 -0.7963 -0.1867 -0.2839 -0.0000\n",
            "S-4980\tThe only reason Tom went to the party was that he expected Mary to be there.\n",
            "T-4980\tLa seule raison pour laquelle Tom est venu à la fête était qu'il espérait que Marie soit <<unk>>\n",
            "H-4980\t-0.5381535291671753\tLa seule raison pour que Tom est allé à la fête était là que Mary s'y soit allé.\n",
            "P-4980\t-0.1132 -0.1080 -0.0003 -0.7223 -0.9748 -0.0002 -0.7911 -0.0597 -0.1847 -0.0571 -0.0705 -1.3654 -0.6302 -1.7399 -0.5191 -1.0186 -1.2311 -0.6382 -0.0005\n",
            "S-9787\tIt's no wonder Tom's sleeping <unk> he drinks up to twelve cups of coffee a day.\n",
            "T-9787\tCe n'est pas surprenant que Tom dorme mal , il boit jusqu'à douze tasses de café par jour.\n",
            "H-9787\t-0.5301346778869629\tCe n'est pas étonnant que les dormir de Tom , il boit pour le café un jour.\n",
            "P-9787\t-0.6016 -0.0063 -0.0109 -0.0007 -0.6177 -1.1423 -0.0332 -0.0003 -0.0859 -1.4167 -0.5496 -0.9512 -1.3210 -1.5447 -0.0159 -1.2251 -0.0193 -0.0000\n",
            "S-10808\tThough it was a muggy night , she went to bed with all the windows closed.\n",
            "T-10808\tBien que ce fût une nuit <<unk>> , elle est allée au lit avec toutes les fenêtres fermées.\n",
            "H-10808\t-0.3317604064941406\tBien que c'était une soir , elle est allée au lit , elle est allée au lit les fenêtres fermés.\n",
            "P-10808\t-0.0033 -0.0135 -0.0218 -0.0023 -0.6469 -1.2646 -0.0166 -0.6565 -0.2008 -0.2546 -0.0013 -0.4300 -0.5573 -0.2350 -0.2414 -0.9726 -0.0078 -1.2214 -0.0006 -0.2186 -0.0001\n",
            "S-6263\tYou're the only person I know besides me who would prefer to live in a tent.\n",
            "T-6263\tTu es la seule personne que je connaisse , à part moi , qui préférerais vivre dans une tente.\n",
            "H-6263\t-0.14039607346057892\tTu es la seule personne que je connaisse à part moi qui préférerais vivre dans une tente.\n",
            "P-6263\t-0.5435 -0.0011 -0.0018 -0.0036 -0.0000 -0.0006 -0.0005 -0.0113 -0.2237 -0.0652 -0.0164 -0.5308 -1.0336 -0.0268 -0.0020 -0.0661 -0.0001 -0.0000\n",
            "S-12129\tI thought I was about to be captured so I ran as fast as I could.\n",
            "T-12129\tJe pensais que j'étais sur le point d'être capturée , alors je courus aussi vite que je le pus.\n",
            "H-12129\t-0.18774579465389252\tJe pensais que j'étais sur le point d'être capturé , alors je courus aussi vite que je pouvais.\n",
            "P-12129\t-0.1112 -0.0164 -0.0442 -0.0591 -0.0531 -0.0375 -0.0394 -0.0953 -0.0025 -0.4810 -0.4666 -0.4705 -0.5187 -0.3346 -0.0042 -0.0005 -0.3000 -0.5324 -0.0000\n",
            "S-10886\tPlease don't make me laugh. I did too many sit-ups yesterday and my stomach muscles hurt.\n",
            "T-10886\tS'il te plait ne me fais pas rire. J'ai fait trop <<unk>> hier et mes muscles <<unk>> me font mal.\n",
            "H-10886\t-0.44070544838905334\tJe te prie de ne pas me faire rire , je me prie.\n",
            "P-10886\t-0.7735 -0.4872 -0.0589 -0.0430 -0.0029 -0.0475 -0.3623 -0.0125 -1.5203 -0.3285 -0.4401 -0.8984 -0.8892 -0.3056\n",
            "S-4502\tI don't think it'll rain , but I'll take an umbrella just in case it does.\n",
            "T-4502\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie juste au cas où cela se <<unk>>\n",
            "H-4502\t-0.18971888720989227\tJe ne pense pas qu'il va pleuvoir , mais je vais prendre un parapluie au cas où.\n",
            "P-4502\t-0.0013 -0.0027 -0.0035 -0.0001 -0.0135 -0.4016 -0.0140 -0.0064 -0.0014 -0.0344 -0.2648 -0.6248 -0.3076 -0.0028 -1.0732 -0.0000 -0.6628 -0.0000\n",
            "S-319\tI thought if I broke up with you , I'd never have to see you again.\n",
            "T-319\tJe pensais que si je <<unk>> avec toi , je n'aurais plus jamais à te revoir.\n",
            "H-319\t-0.33690205216407776\tJe pensais que je me suis rompu avec vous , je ne devrais plus jamais te revoir.\n",
            "P-319\t-0.0521 -0.0873 -0.0875 -0.0606 -0.4331 -0.8581 -1.5689 -0.0440 -0.6619 -0.0101 -0.0119 -0.3247 -1.1659 -0.0622 -0.0113 -0.5181 -0.1065 0.0000\n",
            "S-5700\tIt's still too hard to find a job. And even if you have a job , chances are you're having a tougher time paying the rising costs of everything from <unk> to gas.\n",
            "T-5700\tC'est encore trop difficile de trouver un emploi. Et même quand on en a un , il y a des chances qu'on ait davantage de difficultés à payer le coût de tout , de <<unk>> au gaz.\n",
            "H-5700\t-0.8739908337593079\tIl est encore trop dur pour trouver un travail plus dur , si tu auras un emploi du cours , il est encore trop difficile pour trouver un emploi du cours , généralement si tu as un travail plus dur à trouver , il se sent encore trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur difficile à trouver un travail plus dur à trouver un travail plus difficile de trouver un travail plus trop difficile pour trouver un travail plus difficile de trouver un travail plus trop dur difficile pour trouver un travail plus trop difficile pour trouver un travail plus dur que trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop dur pour trouver un travail plus difficile de trouver un travail plus trop difficile pour trouver un travail plus trop difficile pour trouver un travail plus trop difficile pour trouver un travail plus difficile de trouver un cours\n",
            "P-5700\t-0.1993 -0.0755 -0.0441 -0.0008 -0.6824 -0.0013 -0.0268 -0.0290 -0.8683 -0.8634 -0.4467 -0.5205 -1.6381 -0.9219 -0.3245 -0.1826 -1.7246 -0.9664 -0.7060 -0.0662 -1.4025 -2.1477 -0.0280 -0.4266 -1.0296 -0.4348 -0.2618 -0.3816 -1.1189 -0.4601 -1.5204 -0.0280 -2.8657 -1.5154 -0.6980 -0.9804 -0.2752 -0.6244 -0.7768 -0.1344 -1.4773 -0.2036 -1.7984 -1.1900 -0.5906 -0.5995 -0.0446 -0.0242 -0.3842 -0.0023 -0.1673 -0.1975 -0.3223 -0.9403 -2.1192 -0.7292 -1.7696 -1.7746 -0.4998 -1.4110 -2.1625 -0.5779 -1.4506 -0.9009 -0.0408 -0.5996 -0.6154 -1.1891 -1.3883 -0.7294 -0.4871 -0.0245 -0.6080 -0.3650 -1.1941 -2.3786 -0.4345 -1.0024 -1.6934 -0.6262 -0.9615 -1.9116 -0.5747 -1.1028 -0.6827 -0.0044 -0.9984 -0.9070 -1.0047 -0.4072 -3.3232 -0.0110 -0.9844 -0.9677 -1.0317 -1.9410 -0.5283 -0.1558 -0.9156 -0.5205 -1.2124 -2.1166 -0.5339 -0.1733 -0.0642 -1.0886 -0.3384 -1.2032 -2.2702 -0.4365 -0.3238 -1.0195 -0.4937 -0.8061 -1.2713 -0.8274 -0.8983 -1.0881 -0.0233 -0.6998 -0.3180 -0.9749 -1.8060 -0.5431 -0.6160 -0.0151 -0.4935 -0.3338 -0.6063 -0.3329 -2.6842 -2.1728 -1.0287 -0.4526 -0.9922 -0.1462 -0.4484 -0.2650 -0.0271 -0.3111 -0.2623 -0.5943 -2.1201 -0.5848 -1.0760 -0.5757 -0.7937 -0.9818 -0.8241 -0.6922 -0.3781 -0.0598 -0.7217 -0.4220 -1.1340 -1.9679 -0.5309 -0.7283 -1.3976 -0.5195 -1.5730 -2.1692 -0.5251 -0.8088 -0.0583 -0.4335 -0.3811 -1.3320 -2.1160 -0.4203 -0.2788 -0.6031 -0.4356 -0.7868 -1.3644 -0.6821 -0.1562 -0.0492 -0.2440 -0.2867 -1.2789 -1.4226 -0.5972 -0.1257 -0.0140 -0.7763 -0.3193 -1.3244 -1.6824 -0.8195 -0.2524 -0.0558 -0.6071 -0.2501 -1.1648 -2.0000 -0.7269 -0.9189 -1.1533 -1.7881 -14.8229\n",
            "S-10885\tIt was bad enough that he usually came to work late , but coming in drunk was the last straw , and I'm going to have to let him go.\n",
            "T-10885\tC'était suffisamment grave qu'il ait l'habitude d'arriver en retard au travail , mais qu'il arrive soûl est un comble , et je vais devoir m'en séparer.\n",
            "H-10885\t-0.9117712378501892\tIl était assez mauvais qu'il arrive , mais je vais devoir le rendre en retard , mais je viens en venir , mais venir le dernier sentiment était la dernière fois , mais je vais devoir le faire , il se passe quand il a eu mauvaise assez assez pour le devoir , il se passe assez mauvais moment qu'il arrive assez mauvais temps qu'il a assez été assez mauvais moment qu'il a assez été mauvais assez assez pour qu'il a assez été assez mauvais cervelle qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été assez mauvais moment qu'il a assez été mauvais assez mauvais cervelle qu'il se passe assez assez mauvais qu'il se passe assez assez mauvais moment qu'il va passer assez mauvais moment qu'il a assez été assez assez mauvais moment qu'il a assez été assez mauvais pour qu'il ait assez été mauvais suffisant pour avoir été assez mauvais cervelle qu'il a assez été mauvais assez mauvais moment qu'il a assez été assez mauvais cervelle qu'il va passer\n",
            "P-10885\t-0.8795 -0.7719 -0.2610 -0.0754 -0.2290 -1.7481 -1.5069 -0.3784 -0.2568 -1.2775 -1.4876 -1.0437 -0.9850 -0.4461 -1.3762 -0.3969 -0.1402 -0.9076 -1.4100 -1.6200 -0.3062 -0.9385 -0.8114 -1.8089 -1.4454 -0.0391 -1.2715 -0.4829 -0.8067 -0.1037 -0.1929 -0.8399 -1.7701 -0.1531 -0.2565 -1.6808 -0.4503 -0.4471 -1.9884 -0.1946 -1.9622 -0.0827 -2.2094 -0.2236 -1.3450 -0.4099 -1.2258 -0.3209 -0.8359 -0.7274 -0.7409 -0.5788 -2.0019 -1.6660 -0.9378 -0.9901 -0.8626 -1.0075 -0.8198 -0.2510 -0.8276 -2.0100 -2.1112 -1.3387 -0.0080 -1.9424 -0.7390 -0.2457 -1.2912 -0.5564 -1.3612 -0.0850 -1.8422 -0.4602 -0.1725 -1.1585 -0.3198 -0.9475 -1.4281 -0.3171 -1.7945 -0.5167 -0.3497 -0.5968 -1.0153 -0.8896 -0.0568 -1.6312 -1.0580 -0.3387 -0.4242 -0.3859 -1.6061 -0.2797 -1.8599 -0.4645 -0.1113 -0.4921 -0.9513 -0.9958 -0.1810 -1.7384 -0.4413 -0.2007 -0.4974 -0.9118 -0.9265 -0.0962 -2.3236 -0.2490 -0.1570 -0.7642 -0.7617 -1.0500 -0.3007 -1.8877 -0.3589 -0.2671 -0.3841 -0.7059 -0.7789 -0.1584 -1.7157 -0.3484 -0.1642 -0.3326 -0.2955 -0.9537 -0.0524 -1.9718 -0.4689 -0.2891 -1.1021 -0.4632 -0.6238 -0.8710 -0.0145 -2.1836 -0.1301 -0.7407 -1.7178 -0.8574 -0.5196 -1.8504 -0.2796 -0.9253 -2.1356 -0.5362 -0.9980 -0.2050 -1.6712 -1.7028 -0.7066 -0.3384 -0.8918 -0.0820 -1.3714 -0.3534 -0.4668 -0.4688 -1.1318 -0.8996 -1.1656 -0.2095 -1.8347 -0.4185 -0.2918 -0.5738 -0.4881 -0.6509 -0.4089 -2.2441 -0.1744 -0.2888 -0.6932 -1.3254 -0.7150 -1.7931 -0.7918 -0.6190 -0.3102 -0.6429 -0.1469 -1.6150 -1.1499 -0.4301 -1.1142 -0.5552 -1.0359 -1.3461 -0.0447 -1.4285 -0.4368 -0.2287 -0.7176 -0.5481 -1.3618 -0.1008 -1.5232 -1.9589 -16.5470\n",
            "S-6030\tIn the same amount of time it would take me to correct all the mistakes in your report , I could write a better report myself.\n",
            "T-6030\tDans le même <<unk>> de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi-même.\n",
            "H-6030\t-0.8943619132041931\tDans le même somme de temps que ça me prendrait du meilleur part que je me prenne tous les erreurs , je me prendrais tout le même part que je me prenne tous les erreurs , que le même somme de temps que ça me ferait du plus grand somme de temps que cela me <unk> de le même somme de temps que cela me <unk> de faire part le même somme somme de temps qui me ferait du plus meilleur somme de temps que je me <unk> de faire part le même somme somme de temps que cela me <unk> de plus part de temps me <unk> le même somme de temps que cela me <unk> de le même somme somme de temps que le même somme somme de temps que me <unk> la même somme somme de temps que cela me permet de temps qui me ferait du meilleur somme de temps que cela me <unk> de plus part de temps qui me ferait du plus grand somme de temps que cela me <unk> de plus part de temps que me <unk> le même somme somme de temps que faire le même somme somme de temps que faire\n",
            "P-6030\t-0.4198 -0.3902 -0.0099 -0.0062 -0.0420 -0.0226 -0.0858 -0.8289 -0.3057 -0.5207 -1.5052 -0.3600 -1.0106 -1.1795 -1.3349 -0.7407 -0.8881 -1.1910 -0.3544 -0.1502 -1.0168 -0.8490 -0.1239 -1.4010 -0.1094 -0.7369 -0.1610 -0.9239 -0.6275 -2.0061 -0.1112 -0.3950 -1.2188 -0.0237 -0.0259 -0.8400 -1.6522 -1.7299 -0.0319 -0.0020 -0.7608 -0.0104 -0.4816 -0.8339 -0.6408 -0.7721 -0.5399 -1.1882 -1.5448 -0.3188 -0.1068 -0.0529 -0.0853 -2.1496 -0.1541 -0.4236 -1.2460 -2.6184 -0.5611 -0.0034 -0.7135 -0.0188 -0.1251 -1.8450 -0.1583 -0.2920 -1.3278 -2.1243 -1.1971 -2.2224 -0.0702 -0.0101 -0.8985 -0.3844 -0.0086 -2.4126 -0.4171 -0.8678 -1.5193 -1.0195 -1.0511 -0.1962 -0.1018 -0.0031 -0.1550 -2.2619 -0.8674 -0.6374 -1.7966 -1.7364 -0.7504 -1.8862 -0.1949 -0.0136 -0.7659 -0.3467 -0.0059 -0.1088 -1.8515 -0.3745 -0.6312 -1.2863 -1.4882 -1.0773 -0.9864 -0.0678 -1.9691 -1.5661 -2.2485 -0.0941 -0.8204 -1.1533 -0.0262 -0.2878 -1.8620 -0.1749 -0.3078 -1.4018 -2.4827 -1.4615 -0.0135 -1.5751 -0.5811 -0.0211 -0.1682 -2.1607 -0.0189 -0.0049 -0.3946 -0.3687 -0.0256 -0.4359 -2.1849 -0.7465 -2.6868 -0.0001 -0.0078 -0.4719 -0.5525 -0.0032 -0.0991 -2.4590 -0.2937 -2.1463 -0.2590 -0.7613 -2.4832 -1.0885 -0.8485 -1.0891 -1.1257 -0.8770 -0.2417 -0.0120 -0.1208 -2.1968 -0.3536 -0.5499 -1.9920 -2.0617 -1.3802 -1.5040 -0.5010 -2.3419 -0.8331 -0.6490 -0.9735 -1.2096 -0.8855 -0.0210 -0.1181 -0.0035 -0.1333 -1.8143 -0.3847 -0.5438 -1.1003 -1.4324 -1.1117 -0.6789 -0.1528 -0.9457 -2.3107 -0.7476 -2.0163 -0.0108 -0.1584 -0.8383 -0.4084 -0.0151 -0.1760 -2.0423 -2.6994 -0.1729 -0.0043 -0.1256 -0.6042 -0.0085 -0.3078 -2.2369 -18.7278\n",
            "S-4762\tDid you know that in Japan , if you have a tattoo , you won't be allowed to bathe in many of the hot spring <unk>\n",
            "T-4762\tSaviez-vous qu'au Japon , si vous portez un tatouage , vous ne serez pas autorisé à vous baigner dans beaucoup des <<unk>> <<unk>> ?\n",
            "H-4762\t-0.8568866848945618\tSavais-tu que au Japon , si tu as un tatouage , si tu auras un tatouage , vous ne serez pas autorisé au Japon , tu seras autorisé à le faire dans le Japon , si vous en prenez au Japon , si tu connais cela au Japon , vous ne connaissez que le journal <unk> , si tu le sais au Japon , si tu le connais le mois que le mois <unk> le Japon , vous savais-tu que le Japon , si le mois le savais-tu , vous <unk> que le mois <unk> que le mois <unk> , si tu le saviez-vous au Japon , le Japon , si vous connaissez cela au Japon , vous le savais-tu au Japon , si tu le savais-tu au Japon , si tu le sais au Japon , tu le connais que le matin , tu le connais que le journal <unk> le Japon , si tu le savais-tu au Japon , si vous savais-tu que le Japon se <unk> au Japon , si tu le saviez-vous au Japon , le mois <unk> que le matin au Japon , tu le connais le mois <unk> le Japon , si tu le\n",
            "P-4762\t-0.1411 -0.0180 -0.7747 -0.0000 -0.0026 -0.1060 -0.5136 -0.8302 -0.0493 -0.0000 -0.0342 -0.3086 -0.4027 -0.9260 -0.0096 -0.0001 -0.1629 -0.7676 -0.2285 -0.1479 -0.0261 -0.7493 -0.9010 -0.0108 -0.0274 -1.4368 -0.8707 -0.3046 -0.0583 -0.8767 -0.8884 -1.7351 -0.0282 -0.8867 -0.0214 -1.4684 -0.6303 -1.5294 -1.8109 -0.6690 -0.0024 -0.2260 -0.5794 -0.5306 -1.5087 -1.6501 -0.2538 -0.0006 -0.0154 -0.7284 -0.7703 -0.3777 -0.4151 -1.2168 -1.6700 -0.5359 -0.6030 -1.3413 -0.7384 -0.2011 -1.0841 -1.3554 -0.0011 -0.0666 -0.9878 -1.0339 -0.4064 -0.8519 -1.8746 -0.8341 -2.2376 -0.6257 -1.2949 -0.9420 -1.5260 -0.0097 -0.1981 -1.6286 -2.2153 -0.0433 -0.1307 -0.1090 -1.2495 -0.8446 -1.7409 -0.5993 -0.9486 -1.1548 -0.3819 -1.2816 -2.0307 -0.6207 -0.1633 -1.8075 -0.5146 -1.1721 -0.1811 -1.5039 -0.8093 -1.1018 -1.6357 -0.9794 -0.2754 -1.7304 -0.2280 -0.0001 -0.0335 -1.1025 -0.2742 -1.2243 -1.0266 -0.9833 -1.2809 -0.5035 -0.5150 -0.0000 -0.0281 -1.2432 -1.6744 -0.5031 -0.8803 -0.0004 -0.0917 -0.2745 -0.5294 -0.2357 -1.3126 -1.4965 -0.0005 -0.0545 -0.8595 -0.6587 -0.9213 -1.4457 -1.4512 -0.0008 -0.1196 -1.0489 -1.1188 -0.3662 -1.6728 -0.2397 -1.0517 -1.8545 -1.0930 -1.0229 -0.3136 -2.0531 -0.2528 -1.6894 -0.5409 -1.3431 -0.0079 -0.1857 -1.4287 -0.7640 -0.5051 -1.4371 -0.5368 -0.0005 -0.0347 -1.2802 -0.9907 -1.7033 -0.2209 -0.0947 -0.3357 -1.9014 -1.0626 -0.9268 -0.0004 -0.1840 -1.1918 -0.5800 -0.4783 -1.4508 -0.2426 -0.0001 -0.0425 -0.9016 -0.4491 -1.0868 -1.5717 -0.1397 -1.7557 -1.7679 -0.0052 -0.1137 -1.4076 -0.8640 -0.3596 -2.1652 -0.6337 -1.6924 -1.3177 -0.0537 -0.4548 -0.4861 -1.2687 -0.2714 -23.3635\n",
            "S-11299\tI know that it's highly unlikely that you'd ever want to go out with me , but I still need to ask at least once.\n",
            "T-11299\tJe sais qu'il est très improbable que tu veuilles jamais sortir avec moi , mais je dois quand même demander au moins une fois.\n",
            "H-11299\t-0.665751576423645\tJe sais qu'il est hautement improbable que tu veuilles déjà sortir avec moi , mais je veux déjà sortir avec moi , mais j'ai encore besoin que tu veux encore sortir avec moi , mais il faut que tu sois hautement improbable que vous vouliez qu'il soit hautement improbable que vous vouliez tout plus improbable que tu sois improbable que ce soit hautement improbable que vous vouliez qu'il soit hautement improbable que tu aies peu improbable qu'il soit hautement improbable que vous soyez improbable que c'est hautement improbable que vous <unk> il est hautement improbable que tu improbable que il soit hautement improbable que vous <unk> il est hautement improbable que tu improbable que il soit hautement improbable que vous <unk> il il est hautement improbable que vous <unk> il est hautement improbable que tu aies improbable que ce soit hautement improbable que vous <unk> qu'il est hautement improbable que tu sois peu improbable que c'est hautement improbable que vous vouliez qu'il soit hautement improbable que vous vouliez qu'il soit hautement improbable que tu aies peu improbable que ce soit hautement improbable que tu sois hautement improbable que tu sois hautement improbable que tu improbable que c'est hautement improbable que\n",
            "P-11299\t-0.0003 -0.0000 -0.2111 -0.1642 -0.0179 -0.0000 -0.0012 -1.1521 -0.4904 -0.6913 -0.1014 -0.1146 -0.0058 -0.3465 -0.0011 -0.7039 -0.3880 -2.3548 -0.3953 -0.9957 -0.0215 -0.0830 -0.0316 -1.6131 -0.8924 -0.2614 -0.7227 -0.9316 -0.8338 -1.2901 -0.1879 -0.0799 -0.0215 -0.1958 -0.0328 -1.5001 -1.5000 -0.0770 -0.9103 -0.8482 -0.3401 -0.0001 -0.0066 -0.6751 -1.6301 -1.1010 -0.7222 -0.2526 -0.0000 -0.0014 -0.5986 -0.6302 -1.6002 -1.1032 -0.2225 -0.0271 -1.0387 -0.8713 -0.7226 -0.1848 -1.0070 -0.0145 -0.0640 -0.0013 -0.0632 -0.4982 -1.5187 -1.1566 -0.6009 -0.2643 -0.0003 -0.0914 -0.9433 -1.2979 -0.5555 -0.1314 -1.4627 -0.3900 -0.1163 -0.0001 -0.0779 -0.5753 -1.6877 -1.1366 -0.5147 -1.0957 -0.0444 -0.0018 -0.0588 -0.6427 -1.6816 -0.8164 -0.7549 -0.1395 -0.0003 -0.1204 -1.0174 -1.6122 -0.2233 -0.8056 -0.4358 -0.4810 -0.0003 -0.1295 -0.5930 -0.9423 -1.2546 -0.7377 -0.2122 -0.0001 -0.0681 -1.1117 -0.8626 -0.2329 -1.1395 -0.3467 -0.5694 -0.0002 -0.1240 -0.4679 -1.5242 -1.4199 -1.6652 -0.3247 -0.3584 -0.0006 -0.0608 -0.7673 -1.2529 -0.7902 -1.1192 -0.1768 -0.0003 -0.1128 -0.9114 -0.6978 -0.8675 -0.2382 -1.4291 -0.0348 -0.2577 -0.0048 -0.0469 -0.7801 -1.1176 -1.1873 -1.0230 -0.0837 -0.0002 -0.1062 -0.9684 -1.0765 -1.6110 -0.0462 -0.1071 -1.3576 -0.1960 -0.0024 -0.0658 -0.6383 -1.8940 -1.5416 -0.4761 -0.2499 -0.0005 -0.1091 -0.5193 -1.7366 -0.7289 -0.6890 -0.1810 -0.0004 -0.1049 -0.9145 -1.7514 -0.5752 -0.1096 -0.2179 -1.0905 -0.0192 -0.3056 -0.0004 -0.0485 -1.0430 -1.5781 -0.6131 -0.0001 -0.1474 -0.8735 -1.0120 -0.4292 -0.0001 -0.0781 -1.0942 -0.3304 -0.2352 -0.6621 -0.1565 -0.0033 -0.0597 -23.0141\n",
            "S-9140\tIf you don't eat breakfast , you'll probably be hungry during the morning and won't be as efficient at work as you could be.\n",
            "T-9140\tSi tu ne <<unk>> pas , tu auras probablement faim au cours de la matinée et tu ne seras pas aussi efficace au travail que tu pourrais l'être.\n",
            "H-9140\t-0.5444705486297607\tSi tu ne manges pas le petit-déjeuner , tu seras probablement aussi efficace au travail que vous ne serez pas aussi efficace au travail que vous ne serez probablement plus efficace au travail que tu ne seras pas aussi efficace que vous ne serez pas aussi petite petit-déjeuner , vous ne serez pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , vous ne prenez pas le petit-déjeuner , tu ne seras pas aussi petite déjeuner que vous ne manges pas le petit-déjeuner , tu ne manges pas le petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne <unk> pas le petit-déjeuner , tu ne seras pas aussi efficace que vous ne manges pas le petit-déjeuner , tu ne seras pas aussi petite petit-déjeuner , vous ne prenez pas le petit-déjeuner , vous ne prenez pas le petit-déjeuner , vous ne serez pas le petit-déjeuner , vous ne seras pas aussi petite petit-déjeuner , vous ne prenez pas\n",
            "P-9140\t-0.0001 -0.6062 -0.0066 -0.2280 -0.0030 -0.2841 -0.0693 -0.0074 -0.8691 -0.4417 -0.0001 -0.9418 -0.0375 -0.5200 -0.0669 -0.5504 -0.8927 -0.0376 -0.0540 -0.0833 -0.0345 -0.0107 -0.3286 -0.0282 -0.5013 -1.1041 -0.0536 -0.1298 -0.6869 -0.2237 -0.0105 -0.4036 -0.2267 -0.5407 -0.6432 -0.1364 -0.1753 -0.0003 -0.0056 -0.0065 -0.5093 -0.6403 -0.1398 -1.2360 -0.2454 -0.1147 -0.9601 -0.5155 -0.0345 -0.4687 -0.2772 -0.0968 -0.3783 -0.8751 -1.7610 -0.4121 -0.0604 -0.4153 -0.5938 -1.0963 -0.0425 -1.1612 -0.1324 -0.2489 -1.0828 -0.1842 -0.1834 -0.0064 -0.5546 -0.6843 -0.7233 -0.1095 -0.4667 -0.6624 -1.3284 -0.0494 -0.2564 -0.0776 -0.1614 -1.0842 -0.1825 -0.1502 -0.0072 -0.3056 -0.6716 -0.3107 -0.7470 -0.4569 -1.3258 -0.9033 -0.0323 -0.5206 -0.1231 -0.6113 -0.6099 -0.3122 -0.9380 -0.1503 -0.1574 -0.0471 -0.9298 -1.1060 -0.1302 -0.2018 -0.0395 -1.0233 -1.0723 -0.9356 -1.2543 -0.4668 -0.1130 -0.7942 -0.0522 -0.8044 -0.1482 -0.3029 -1.1092 -0.0329 -0.7747 -0.0568 -1.1382 -0.0864 -0.2703 -0.5914 -0.3362 -0.9699 -0.0447 -1.1375 -0.1601 -0.3831 -1.0144 -0.2403 -0.1385 -0.0038 -0.3197 -0.5014 -0.4202 -0.8802 -0.4743 -0.4680 -0.8692 -0.0233 -0.8007 -0.1053 -0.2859 -0.9879 -0.0999 -0.1247 -0.0081 -0.6353 -1.4362 -1.1176 -0.5657 -0.3593 -0.5097 -0.0487 -0.9840 -0.1109 -0.2608 -1.1357 -0.2165 -0.1727 -0.0092 -0.6559 -1.3194 -0.4291 -0.6484 -0.3787 -0.2026 -0.6979 -0.1426 -0.1025 -0.0479 -0.7131 -0.5373 -0.2280 -0.9688 -0.4824 -0.1604 -0.0911 -0.6362 -0.4900 -0.4613 -1.1544 -0.0728 -1.0044 -0.1209 -0.4709 -0.5359 -0.2243 -1.2626 -0.0089 -0.1158 -0.7862 -0.3127 -0.4299 -0.4070 -0.1978 -0.8734 -0.1380 -20.5911\n",
            "S-13683\tIf you don't want to put on sunscreen , that's your problem. Just don't come complaining to me when you get a sunburn.\n",
            "T-13683\tSi tu ne veux pas mettre de crème solaire c'est ton problème , mais ne viens pas te plaindre quand <<unk>> des coups de soleil.\n",
            "H-13683\t-0.3102198541164398\tSi vous ne voulez pas me mettre de crème solaire , c'est votre problème.\n",
            "P-13683\t-0.0011 -0.8510 -0.0047 -0.0302 -0.1173 -0.6526 -0.7944 -0.0908 -0.7353 -0.0674 -0.0464 -0.2269 -0.8839 -0.1030 -0.0484\n",
            "S-13070\tDuring hard times , people might not go on a trip , but they might be willing to pay extra for good coffee.\n",
            "T-13070\tQuand les temps sont durs , les gens peuvent ne pas partir en voyage mais ils peuvent être disposés à payer davantage pour du café de bonne qualité.\n",
            "H-13070\t-0.6659844517707825\tLors des moments difficiles , les gens se pourraient <unk> , mais ils ne pourraient peut-être pas être disposés à le procurer un bon café.\n",
            "P-13070\t-1.1245 -0.1720 -1.8897 -0.3850 -0.0005 -0.0560 -0.0004 -0.5172 -1.2297 -1.1572 -0.7780 -0.1902 -0.5368 -0.8200 -1.0510 -0.2130 -0.3742 -2.1222 -1.2951 -0.0112 -1.0372 -1.8286 -0.3581 -0.0372 -0.1305 -0.0001\n",
            "S-10827\tI still have a scar on my left leg from a car accident I was in when I was thirteen years old.\n",
            "T-10827\tJ'ai encore une cicatrice sur ma jambe gauche à la suite d'un accident de voiture dans lequel j'ai été impliqué quand j'avais treize ans.\n",
            "H-10827\t-0.3881998062133789\tJ'ai encore une cicatrice sur mon foyer à la jambe gauche , j'ai eu treize ans.\n",
            "P-10827\t-0.0196 -0.2383 -0.0290 0.0000 -0.0006 -0.4542 -1.6898 -0.4478 -0.3633 -0.0082 -0.0207 -1.4627 -0.2635 -1.1911 -0.0044 -0.3981 -0.0081\n",
            "S-13283\tI never see a library without wishing I had time to go there and stay till I had read everything in it.\n",
            "T-13283\tJe ne vois jamais une bibliothèque sans souhaiter avoir le temps de m'y rendre et d'y rester jusqu'à ce que j'y aie tout lu.\n",
            "H-13283\t-0.6584036946296692\tJe ne vois jamais de la bibliothèque , sans <unk> que je devais y aller et rester là.\n",
            "P-13283\t-0.0273 -0.3658 -0.0324 -0.0001 -0.7974 -0.3032 -0.0006 -0.7175 -1.4608 -0.0349 -2.3083 -0.4873 -1.5329 -0.3907 -0.0490 -0.3536 -1.3084 -2.1974 -0.1421\n",
            "S-4837\tPolice are <unk> people not to pick up <unk> as they search for two prisoners on the run after escaping from jail.\n",
            "T-4837\tLa police <<unk>> les gens à ne pas prendre <<unk>> car ils recherchent deux prisonniers en cavale après leur <<unk>>\n",
            "H-4837\t-0.7876145839691162\tLa police sont <unk> que les gens <unk> de ne pas prendre les <unk> après les deux prisonniers se termine au cours de la prison.\n",
            "P-4837\t-0.0042 -0.0002 -0.3517 -0.3193 -0.5976 -0.0242 -0.0035 -0.5643 -1.2016 -1.0463 -0.3536 -1.1217 -2.2955 -1.3131 -0.4886 -1.8913 -0.9603 -0.7516 -1.3018 -0.4938 -1.3968 -0.1275 -1.2519 -0.5820 -2.0045 -0.0310\n",
            "S-248\tWhen we started out <unk> web pages , we were the only ones doing it in this part of the country.\n",
            "T-248\tQuand nous avons commencé à concevoir des pages web , nous étions les seuls à le faire dans cette partie du pays.\n",
            "H-248\t-0.5569350123405457\tQuand nous avons commencé à la <unk> la toile , nous avons été la seule à le faire dans cette partie du pays.\n",
            "P-248\t-0.3438 -0.0437 -0.2326 -0.1162 -0.5858 -1.1468 -0.4267 -0.8433 -0.4818 -0.0090 -0.0163 -0.8179 -0.2042 -0.8271 -0.0060 -0.2868 -1.5166 -0.4182 -1.9418 -0.3057 -0.0118 -1.0455 -1.7391 0.0000\n",
            "S-11828\tI was looking forward to seeing a scenic view of Mt. Fuji , but unfortunately it was completely hidden behind clouds.\n",
            "T-11828\tJ'avais hâte de voir une vue <<unk>> du mont Fuji mais , malheureusement , il était complètement caché par les nuages.\n",
            "H-11828\t-1.1627466678619385\tJ'étais impatient de voir une beauté en masse vue en doute , mais les <unk> , mais les nuages en <unk>\n",
            "P-11828\t-0.9874 -0.4570 -0.0358 -0.0574 -0.1296 -0.4456 -0.9748 -1.1766 -1.6473 -1.3455 -2.5917 -1.9532 -1.4369 -2.0317 -2.3442 -1.9019 -0.0403 -1.9964 -0.8819 -1.3586 -1.5542 -0.2324\n",
            "S-7610\tI tried to stay inside the house all day , but I ended up going outside and sitting in the garden.\n",
            "T-7610\tJ'ai essayé de rester dans la maison toute la journée , mais j'ai fini par sortir m'asseoir dans le jardin.\n",
            "H-7610\t-0.3343660533428192\tJ'ai essayé de rester à l'intérieur toute la maison , mais j'ai fini par sortir au jardin.\n",
            "P-7610\t-0.0420 -0.0419 -0.0019 -0.0068 -0.9891 -0.0007 -0.0241 -0.0027 -0.0113 -0.4101 -0.0116 -0.0807 -0.1798 -0.1800 -1.5062 -2.5201 -0.0094 -0.0001\n",
            "S-6513\t<unk> told police that the train was <unk> way over the speed limit when it <unk> going around a <unk>\n",
            "T-6513\tDes <<unk>> ont déclaré à la police que le train roulait bien au-dessus de la limite de vitesse lorsqu'il a <<unk>> dans un <<unk>>\n",
            "H-6513\t-0.7337239980697632\tOn a dit à la police que le train était de la <unk> quand il se passe par la <unk>\n",
            "P-6513\t-1.1468 -1.1832 -0.0052 -0.1554 -0.1037 -0.0063 -0.7600 -0.3462 -0.0000 -0.5380 -2.0542 -0.3994 -1.0192 -1.0681 -0.2810 -0.4739 -1.2200 -2.1453 -0.9839 -0.1694 -1.3490\n",
            "S-10561\tI suppose you want to ask me how I was able to make so much money in so little time.\n",
            "T-10561\tJe suppose que vous voulez me demander comment j'ai été en mesure de me faire autant d'argent en si peu de temps.\n",
            "H-10561\t-0.5618663430213928\tJe suppose que tu veux que tu veuilles me demander en tant que je puisse gagner tant d'argent aussi peu de temps.\n",
            "P-10561\t-0.0004 -0.0016 -0.0013 -0.7139 -0.0039 -1.1541 -0.6853 -0.8353 -0.0175 -0.2829 -1.5293 -0.0269 -0.0805 -0.7608 -1.9327 -1.1334 -0.7559 -0.5250 -0.9175 -0.6519 -0.4835 -0.4288 -0.0009\n",
            "| Translated 14928 sentences (118330 tokens) in 37.5s (397.58 sentences/s, 3151.53 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 42.36, 64.9/47.8/36.5/28.4 (BP=1.000, ratio=1.025, syslen=103402, reflen=100925)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0KEV7dRlFo3",
        "colab_type": "text"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1gkevAlYJa",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeKee0tlFHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed77c4cf-52c9-4408-a98a-59fdd205cd74"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train preprocessed_data/ \\\n",
        "     --clip-norm 0.0 --batch-size 128 \\\n",
        "     --arch transformer --max-epoch 15  \\\n",
        "     --save-dir checkpoints/transformer \\\n",
        "     --optimizer adam  --dropout 0.3 \\\n",
        "     --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
        "     --no-epoch-checkpoints --num-workers 3 \\"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='preprocessed_data/', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=15, max_sentences=128, max_sentences_valid=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.en\n",
            "| loaded 26344 examples from: preprocessed_data/valid.en-fr.fr\n",
            "| preprocessed_data/ valid en-fr 26344 examples\n",
            "TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): Embedding(23320, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): Embedding(25000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "| model transformer, criterion CrossEntropyCriterion\n",
            "| num. model params: 81678336 (num. trained: 81678336)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 128\n",
            "| loaded checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 3 @ 6300 updates)\n",
            "| loading train data for epoch 3\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.en\n",
            "| loaded 134349 examples from: preprocessed_data/train.en-fr.fr\n",
            "| preprocessed_data/ train en-fr 134349 examples\n",
            "| epoch 004 | loss 3.213 | ppl 9.27 | wps 3078 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 7350 | lr 0.000368856 | gnorm 1.473 | clip 0.000 | oom 0.000 | wall 342 | train_wall 1677\n",
            "| epoch 004 | valid on 'valid' subset | loss 3.068 | ppl 8.39 | num_updates 7350 | best_loss 3.06826\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 4 @ 7350 updates) (writing took 20.959300756454468 seconds)\n",
            "| epoch 005 | loss 2.855 | ppl 7.24 | wps 3098 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 8400 | lr 0.000345033 | gnorm 1.527 | clip 0.000 | oom 0.000 | wall 721 | train_wall 2009\n",
            "| epoch 005 | valid on 'valid' subset | loss 2.934 | ppl 7.64 | num_updates 8400 | best_loss 2.93406\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 5 @ 8400 updates) (writing took 17.33105993270874 seconds)\n",
            "| epoch 006 | loss 2.591 | ppl 6.02 | wps 3099 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 9450 | lr 0.0003253 | gnorm 1.565 | clip 0.000 | oom 0.000 | wall 1099 | train_wall 2341\n",
            "| epoch 006 | valid on 'valid' subset | loss 2.797 | ppl 6.95 | num_updates 9450 | best_loss 2.79741\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 6 @ 9450 updates) (writing took 19.892654418945312 seconds)\n",
            "| epoch 007 | loss 2.370 | ppl 5.17 | wps 3098 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 10500 | lr 0.000308607 | gnorm 1.591 | clip 0.000 | oom 0.000 | wall 1477 | train_wall 2673\n",
            "| epoch 007 | valid on 'valid' subset | loss 2.755 | ppl 6.75 | num_updates 10500 | best_loss 2.75499\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 7 @ 10500 updates) (writing took 16.98462677001953 seconds)\n",
            "| epoch 008 | loss 2.190 | ppl 4.56 | wps 3094 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 11550 | lr 0.000294245 | gnorm 1.593 | clip 0.000 | oom 0.000 | wall 1856 | train_wall 3005\n",
            "| epoch 008 | valid on 'valid' subset | loss 2.657 | ppl 6.31 | num_updates 11550 | best_loss 2.65655\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 8 @ 11550 updates) (writing took 17.476290941238403 seconds)\n",
            "| epoch 009 | loss 2.044 | ppl 4.12 | wps 3080 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 12600 | lr 0.000281718 | gnorm 1.604 | clip 0.000 | oom 0.000 | wall 2234 | train_wall 3339\n",
            "| epoch 009 | valid on 'valid' subset | loss 2.616 | ppl 6.13 | num_updates 12600 | best_loss 2.61573\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 9 @ 12600 updates) (writing took 19.50722360610962 seconds)\n",
            "| epoch 010 | loss 1.915 | ppl 3.77 | wps 3093 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 13650 | lr 0.000270666 | gnorm 1.616 | clip 0.000 | oom 0.000 | wall 2612 | train_wall 3671\n",
            "| epoch 010 | valid on 'valid' subset | loss 2.568 | ppl 5.93 | num_updates 13650 | best_loss 2.56763\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 10 @ 13650 updates) (writing took 17.942534923553467 seconds)\n",
            "| epoch 011 | loss 1.806 | ppl 3.5 | wps 3087 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 14700 | lr 0.00026082 | gnorm 1.576 | clip 0.000 | oom 0.000 | wall 2989 | train_wall 4004\n",
            "| epoch 011 | valid on 'valid' subset | loss 2.543 | ppl 5.83 | num_updates 14700 | best_loss 2.54337\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 11 @ 14700 updates) (writing took 17.664472341537476 seconds)\n",
            "| epoch 012 | loss 1.715 | ppl 3.28 | wps 3088 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 15750 | lr 0.000251976 | gnorm 1.593 | clip 0.000 | oom 0.000 | wall 3370 | train_wall 4337\n",
            "| epoch 012 | valid on 'valid' subset | loss 2.524 | ppl 5.75 | num_updates 15750 | best_loss 2.52395\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 12 @ 15750 updates) (writing took 17.218477725982666 seconds)\n",
            "| epoch 013 | loss 1.634 | ppl 3.1 | wps 3090 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 16800 | lr 0.000243975 | gnorm 1.579 | clip 0.000 | oom 0.000 | wall 3750 | train_wall 4670\n",
            "| epoch 013 | valid on 'valid' subset | loss 2.530 | ppl 5.78 | num_updates 16800 | best_loss 2.52395\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 13 @ 16800 updates) (writing took 3.77728009223938 seconds)\n",
            "| epoch 014 | loss 1.559 | ppl 2.95 | wps 3078 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 17850 | lr 0.000236691 | gnorm 1.584 | clip 0.000 | oom 0.000 | wall 4114 | train_wall 5004\n",
            "| epoch 014 | valid on 'valid' subset | loss 2.475 | ppl 5.56 | num_updates 17850 | best_loss 2.47502\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 14 @ 17850 updates) (writing took 18.28604531288147 seconds)\n",
            "| epoch 015 | loss 1.496 | ppl 2.82 | wps 3110 | ups 3 | wpb 996.039 | bsz 127.951 | num_updates 18900 | lr 0.000230022 | gnorm 1.590 | clip 0.000 | oom 0.000 | wall 4489 | train_wall 5334\n",
            "| epoch 015 | valid on 'valid' subset | loss 2.490 | ppl 5.62 | num_updates 18900 | best_loss 2.47502\n",
            "| saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 15 @ 18900 updates) (writing took 3.6316442489624023 seconds)\n",
            "| done training in 4511.4 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaDq40utlqSb",
        "colab_type": "text"
      },
      "source": [
        "#### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8tFMAvvlh-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "6caf352e-0253-4ed7-d720-34d428b14174"
      },
      "source": [
        "!fairseq-generate preprocessed_data \\\n",
        "    --path checkpoints/transformer/checkpoint_best.pt \\\n",
        "    --batch-size 64 --beam 3 > transformer.out"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py\", line 151, in load_checkpoint_to_cpu\n",
            "    from fairseq.fb_pathmgr import fb_pathmgr\n",
            "ModuleNotFoundError: No module named 'fairseq.fb_pathmgr'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-generate\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/generate.py\", line 199, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/generate.py\", line 47, in main\n",
            "    task=task,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py\", line 179, in load_model_ensemble\n",
            "    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py\", line 190, in load_model_ensemble_and_task\n",
            "    state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py\", line 160, in load_checkpoint_to_cpu\n",
            "    path, map_location=lambda s, l: default_restore_location(s, \"cpu\")\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 529, in load\n",
            "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 709, in _legacy_load\n",
            "    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCMPZrcyl4SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5df01345-2693-4a45-bd29-205075bd9a43"
      },
      "source": [
        "!grep ^H transformer.out | cut -f3- > transformer.out.sys\n",
        "!grep ^T transformer.out | cut -f2- > transformer.out.ref\n",
        "!fairseq-score --sys transformer.out.sys --ref transformer.out.ref --ignore-case"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=True, order=4, ref='transformer.out.ref', sacrebleu=False, sentence_bleu=False, sys='transformer.out.sys')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-score\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/score.py\", line 84, in main\n",
            "    score(f)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/score.py\", line 78, in score\n",
            "    print(scorer.result_string(args.order))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/bleu.py\", line 127, in result_string\n",
            "    return fmt.format(order, self.score(order=order), *bleup,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/bleu.py\", line 103, in score\n",
            "    return self.brevity() * math.exp(psum / order) * 100\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/bleu.py\", line 117, in brevity\n",
            "    r = self.stat.reflen / self.stat.predlen\n",
            "ZeroDivisionError: division by zero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svC90M2Jrz6p",
        "colab_type": "text"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqPzUXQGl7vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3ac3e5fe-5df6-4569-bf1a-186fa88bf848"
      },
      "source": [
        "!tail -n 100 transformer.out"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(beam=3, bpe=None, cpu=False, criterion='cross_entropy', data='preprocessed_data', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='checkpoints/transformer/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 23320 types\n",
            "| [fr] dictionary: 25000 types\n",
            "| loaded 14928 examples from: preprocessed_data/test.en-fr.en\n",
            "| loaded 14928 examples from: preprocessed_data/test.en-fr.fr\n",
            "| preprocessed_data test en-fr 14928 examples\n",
            "| loading model(s) from checkpoints/transformer/checkpoint_best.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oMxspblr1LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}