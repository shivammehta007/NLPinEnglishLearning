{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomSeq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfTAAlB7UbOAZZ4wT/UWZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/CustomSeq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bRWsAnPpUSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import urllib.request as req\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import math\n",
        "import torchtext.data as data\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EVY6kH0qHjc",
        "colab_type": "text"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMvkufXeqJGu",
        "colab_type": "text"
      },
      "source": [
        "## data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8_mKsJnqPcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_FOLDER = \"data\"\n",
        "DATA_FOLDER_RAW = \"raw\"\n",
        "DATA_FOLDER_PROCESSED = \"processed\"\n",
        "\n",
        "RAW_FILENAMES = {\n",
        "    \"SQUAD\": {\n",
        "        \"train\": \"squad_train.json\",\n",
        "        \"test\": \"squad_test.json\",\n",
        "        \"valid\": \"squad_valid.json\",\n",
        "    }\n",
        "}\n",
        "\n",
        "SQUAD_NAME = \"SQUAD\"\n",
        "\n",
        "DATASETS = {\n",
        "    \"SQUAD\": {\n",
        "        \"train\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\",\n",
        "        \"test\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\",\n",
        "        \"valid\": None,\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rogW2sHZqQ6F",
        "colab_type": "text"
      },
      "source": [
        "## root.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nob91Dq9piIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "LOGGING_LEVEL = logging.DEBUG\n",
        "LOGGING_FORMAT = (\n",
        "    \"[%(levelname)s | %(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\"\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "def seed_all(seed=1234):\n",
        "    \"\"\"Seed the results for duplication\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_all()\n",
        "models = (1, \"VanillaSeq2Seq\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjXaTsEzqcUo",
        "colab_type": "text"
      },
      "source": [
        "## hyperparameter.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKDx-F98qcce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VANILLA_SEQ2SEQ = {\n",
        "    \"INPUT_DIM\": 45000,\n",
        "    \"OUTPUT_DIM\": 28000,\n",
        "    \"DEC_EMB_DIM\": 300,\n",
        "    \"ENC_EMB_DIM\": 300,\n",
        "    \"HID_DIM\": 600,\n",
        "    \"N_LAYERS\": 2,\n",
        "    \"DROPOUT\": 0.7,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OxlOoWVrAK-",
        "colab_type": "text"
      },
      "source": [
        "## Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zRrCz0yrB6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "nlp_word = English()\n",
        "nlp_sentence = English()\n",
        "nlp_sentence.add_pipe(nlp_sentence.create_pipe(\"sentencizer\"))\n",
        "\n",
        "\n",
        "def word_tokenizer(sentence):\n",
        "    return [word.text for word in nlp_word(sentence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFdUykYcp6E4",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic8uu_-fpvny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(dataset_name):\n",
        "    \"\"\"\n",
        "    Downloads the dataset which is passed as parameter\n",
        "    Input:\n",
        "    dataset_name: string\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "\n",
        "    logger.info(\"Downloading {}\".format(dataset_name))\n",
        "    output_path = os.path.join(DATA_FOLDER, DATA_FOLDER_RAW)\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        logger.debug(\"Folders doesn't exists creating it\")\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    dataset_name = dataset_name.upper()\n",
        "    if DATASETS[dataset_name][\"train\"]:\n",
        "        train_filename, _ = req.urlretrieve(\n",
        "            url=DATASETS[dataset_name][\"train\"],\n",
        "            filename=os.path.join(output_path, RAW_FILENAMES[dataset_name][\"train\"]),\n",
        "        )\n",
        "        logger.debug(\"Downloaded Train set -> {}\".format(train_filename))\n",
        "\n",
        "    if DATASETS[dataset_name][\"test\"]:\n",
        "        test_filename, _ = req.urlretrieve(\n",
        "            url=DATASETS[dataset_name][\"test\"],\n",
        "            filename=os.path.join(output_path, RAW_FILENAMES[dataset_name][\"test\"]),\n",
        "        )\n",
        "        logger.debug(\"Downloaded Test set -> {}\".format(test_filename))\n",
        "\n",
        "    if DATASETS[dataset_name][\"valid\"]:\n",
        "        valid_filename, _ = req.urlretrieve(\n",
        "            url=DATASETS[dataset_name][\"valid\"],\n",
        "            filename=os.path.join(output_path, RAW_FILENAMES[dataset_name][\"valid\"]),\n",
        "        )\n",
        "        logger.debug(\"Downloaded Valid Set -> {}\".format(valid_filename))\n",
        "\n",
        "    logger.info(\"Files Downloaded Successfully!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0MwjXaqqimE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bdffb5fa-179b-4966-87c8-77f0ec429d64"
      },
      "source": [
        "download_dataset(\"SQUAD\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO | <ipython-input-13-37e6b7cd460b>:9 -     download_dataset() ] Downloading SQUAD\n",
            "[DEBUG | <ipython-input-13-37e6b7cd460b>:22 -     download_dataset() ] Downloaded Train set -> data/raw/squad_train.json\n",
            "[DEBUG | <ipython-input-13-37e6b7cd460b>:29 -     download_dataset() ] Downloaded Test set -> data/raw/squad_test.json\n",
            "[INFO | <ipython-input-13-37e6b7cd460b>:38 -     download_dataset() ] Files Downloaded Successfully!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpXE-OVMrNHx",
        "colab_type": "text"
      },
      "source": [
        "# PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1qjg42mqq4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "INPUT_PATH = os.path.join(DATA_FOLDER, DATA_FOLDER_RAW)\n",
        "OUTPUT_PATH = os.path.join(DATA_FOLDER, DATA_FOLDER_PROCESSED)\n",
        "\n",
        "\n",
        "def convert_to_file_without_answers(\n",
        "    dataset, dataset_type=\"train\", get_impossible=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes an input json and generates dataset_type.paragraphs and dataset_type.questions\n",
        "    Input:\n",
        "    dataset : string -> Name of json input\n",
        "    dataset_type: string -> Type of dataset like (Train, test, valid)\n",
        "    get_impossible: boolean -> Flag to get unanswerable questions\n",
        "    \"\"\"\n",
        "    if not os.path.exists(os.path.join(OUTPUT_PATH, SQUAD_NAME)):\n",
        "        os.makedirs(os.path.join(OUTPUT_PATH, SQUAD_NAME))\n",
        "\n",
        "    para_output = open(\n",
        "        os.path.join(OUTPUT_PATH, SQUAD_NAME, dataset_type + \".paragraphs\"), \"w\"\n",
        "    )\n",
        "    question_output = open(\n",
        "        os.path.join(OUTPUT_PATH, SQUAD_NAME, dataset_type + \".questions\"), \"w\"\n",
        "    )\n",
        "    dataset = dataset[\"data\"]\n",
        "    dataset_size = []\n",
        "    for paragraphs in tqdm(dataset):\n",
        "        paragraphs = paragraphs[\"paragraphs\"]\n",
        "        for i, paragraph in enumerate(paragraphs):\n",
        "            para = paragraph[\"context\"]\n",
        "            for questionanswers in paragraph[\"qas\"]:\n",
        "                if questionanswers[\"is_impossible\"]:\n",
        "                    continue\n",
        "                question = questionanswers[\"question\"]\n",
        "                para = para.replace(\"\\n\", \" \")\n",
        "                para_output.write(para.strip().lower() + \"\\n\")\n",
        "                question_output.write(question.strip().lower() + \"\\n\")\n",
        "                dataset_size.append(i)\n",
        "    logger.info(\"Size of the {} dataset: {}\".format(dataset_type, len(dataset_size)))\n",
        "    para_output.close()\n",
        "    question_output.close()\n",
        "\n",
        "\n",
        "def split_train_valid(dataset_name, split_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Splits the train set to a validation set\n",
        "    creates files in the processed folder with \n",
        "    \"\"\"\n",
        "    logger.debug(\n",
        "        \"Splitting the {}'s train set into train and valid\".format(dataset_name)\n",
        "    )\n",
        "    if not os.path.exists(os.path.join(OUTPUT_PATH, dataset_name)):\n",
        "        raise NotImplementedError(\n",
        "            \"The Dataset has not been preprocessed yet please call the \\\n",
        "                 processing method before spliting the trainset\"\n",
        "        )\n",
        "\n",
        "    filename_paragraph = os.path.join(OUTPUT_PATH, dataset_name, \"train.paragraphs\")\n",
        "    filename_questions = os.path.join(OUTPUT_PATH, dataset_name, \"train.questions\")\n",
        "\n",
        "    with open(filename_paragraph) as paragraphs_file, open(\n",
        "        filename_questions\n",
        "    ) as questions_file:\n",
        "        data_paragraphs = paragraphs_file.readlines()\n",
        "        data_questions = questions_file.readlines()\n",
        "\n",
        "    logger.debug(\n",
        "        \"# of Paragraphs: {} # of Questions: {} \".format(\n",
        "            len(data_paragraphs), len(data_questions)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    assert len(data_paragraphs) == len(\n",
        "        data_questions\n",
        "    ), \"Number of Paragraphs and Questions mismatch\"\n",
        "\n",
        "    # Output files\n",
        "    train_paragraphs_file = open(\n",
        "        os.path.join(OUTPUT_PATH, dataset_name, \"train.paragraphs\"), \"w\"\n",
        "    )\n",
        "    valid_paragraphs_file = open(\n",
        "        os.path.join(OUTPUT_PATH, dataset_name, \"valid.paragraphs\"), \"w\"\n",
        "    )\n",
        "    train_questions_file = open(\n",
        "        os.path.join(OUTPUT_PATH, dataset_name, \"train.questions\"), \"w\"\n",
        "    )\n",
        "    valid_questions_file = open(\n",
        "        os.path.join(OUTPUT_PATH, dataset_name, \"valid.questions\"), \"w\"\n",
        "    )\n",
        "\n",
        "    train_count, valid_count = 0, 0\n",
        "\n",
        "    for i in tqdm(range(len(data_paragraphs))):\n",
        "        if random.random() < split_ratio:\n",
        "            train_paragraphs_file.write(data_paragraphs[i].strip() + \"\\n\")\n",
        "            train_questions_file.write(data_questions[i].strip() + \"\\n\")\n",
        "            train_count += 1\n",
        "        else:\n",
        "            valid_paragraphs_file.write(data_paragraphs[i].strip() + \"\\n\")\n",
        "            valid_questions_file.write(data_questions[i].strip() + \"\\n\")\n",
        "            valid_count += 1\n",
        "\n",
        "    logger.info(\n",
        "        \"Total Trainset: {} | Total ValidSet: {}\".format(train_count, valid_count)\n",
        "    )\n",
        "\n",
        "\n",
        "def load_json(filelocation):\n",
        "    \"\"\"\n",
        "    Takes Filename as input and returns a Json object\n",
        "    Input:\n",
        "    filelocation: string\n",
        "    Returns:\n",
        "    json_data: json object\n",
        "    \"\"\"\n",
        "    with open(filelocation) as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def preprocess_squad(name, mode, filter):\n",
        "    \"\"\"\n",
        "    PreProcesses Squad\n",
        "    Input:\n",
        "    name: string -> Name of the dataset\n",
        "    mode: string -> To replicate sentences based on number of answers or just questions\n",
        "    \"\"\"\n",
        "    logger.debug(\"PreProcessing SQUAD\")\n",
        "    logger.debug(\"Loading JSON\")\n",
        "    train_file = load_json(os.path.join(INPUT_PATH, RAW_FILENAMES[name][\"train\"]))\n",
        "    test_file = load_json(os.path.join(INPUT_PATH, RAW_FILENAMES[name][\"test\"]))\n",
        "\n",
        "    if mode.upper() == \"QUESTION\" and not filter:\n",
        "        convert_to_file_without_answers(train_file, \"train\")\n",
        "        convert_to_file_without_answers(test_file, \"test\")\n",
        "    else:\n",
        "        filter_sentences_on_answer(train_file, \"train\")\n",
        "        filter_sentences_on_answer(test_file, \"test\")\n",
        "\n",
        "    logger.debug(\"Now we will split train set to train and valid set\")\n",
        "    split_train_valid(name)\n",
        "\n",
        "    logger.info(\"{} Preprocessed\".format(name))\n",
        "\n",
        "\n",
        "def extract_filtered_sentences(questionanswers, para):\n",
        "    \"\"\"\n",
        "    Method returns filtered sentences from the answers and para for SQUAD\n",
        "    \"\"\"\n",
        "    tokenized_paragraph = nlp_sentence(para)\n",
        "    sentences = [sent.string for sent in tokenized_paragraph.sents]\n",
        "\n",
        "    filtered_sentences = set()\n",
        "\n",
        "    # This iterates over every answer in question\n",
        "    for answer in questionanswers[\"answers\"]:\n",
        "        answer_index = answer[\"answer_start\"]\n",
        "        length = 0\n",
        "\n",
        "        # find sentence that has answer and filter them\n",
        "        for sentence in sentences:\n",
        "            if answer_index <= length + len(sentence):\n",
        "                filtered_sentences.add(sentence.replace(\"\\n\", \" \").strip())\n",
        "                break\n",
        "            length += len(sentence)\n",
        "\n",
        "        if not filtered_sentences:\n",
        "            print(\"Length : {}\".format(length))\n",
        "            raise Exception(\"One of the Answers had no sentence please check the data\")\n",
        "\n",
        "    return \" \".join(filtered_sentences)\n",
        "\n",
        "\n",
        "def filter_sentences_on_answer(dataset, dataset_type=\"train\", get_impossible=False):\n",
        "    \"\"\"\n",
        "    Filter the paragraph with only sentences relevant to answer and generates files\n",
        "    with sentences and questions instead of paragraphs and questions\n",
        "    Input:\n",
        "    dataset: string\n",
        "    dataset_type: string\n",
        "    get_impossible: boolean\n",
        "    \"\"\"\n",
        "    if not os.path.exists(os.path.join(OUTPUT_PATH, SQUAD_NAME)):\n",
        "        os.makedirs(os.path.join(OUTPUT_PATH, SQUAD_NAME))\n",
        "\n",
        "    para_output = open(\n",
        "        os.path.join(OUTPUT_PATH, SQUAD_NAME, dataset_type + \".paragraphs\"), \"w\"\n",
        "    )\n",
        "    question_output = open(\n",
        "        os.path.join(OUTPUT_PATH, SQUAD_NAME, dataset_type + \".questions\"), \"w\"\n",
        "    )\n",
        "    dataset = dataset[\"data\"]\n",
        "    dataset_size = []\n",
        "\n",
        "    logger.debug(\"Starting to filter sentences on answer\")\n",
        "\n",
        "    # This loops iterates over every paragraph\n",
        "    for paragraphs in tqdm(dataset):\n",
        "        paragraphs = paragraphs[\"paragraphs\"]\n",
        "        for i, paragraph in enumerate(paragraphs):\n",
        "            para = paragraph[\"context\"]\n",
        "            # This loop iterates over every question in para\n",
        "            for questionanswers in paragraph[\"qas\"]:\n",
        "                if questionanswers[\"is_impossible\"]:\n",
        "                    continue\n",
        "                question = questionanswers[\"question\"]\n",
        "\n",
        "                filtered_sentences = extract_filtered_sentences(questionanswers, para)\n",
        "\n",
        "                para_output.write(filtered_sentences.strip().lower() + \"\\n\")\n",
        "                question_output.write(question.strip().lower() + \"\\n\")\n",
        "\n",
        "                dataset_size.append(i)\n",
        "\n",
        "    logger.info(\"Size of the {} dataset: {}\".format(dataset_type, len(dataset_size)))\n",
        "    para_output.close()\n",
        "    question_output.close()\n",
        "\n",
        "    logger.debug(\"Sentences Filtered on Answers\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be95tzV3rU3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c2e4de3a-a8b7-4048-ca2d-46a52f612eff"
      },
      "source": [
        "preprocess_squad(\"SQUAD\", \"QUESTION\", True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:130 -     preprocess_squad() ] PreProcessing SQUAD\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:131 -     preprocess_squad() ] Loading JSON\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:197 - filter_sentences_on_answer() ] Starting to filter sentences on answer\n",
            "100%|██████████| 442/442 [02:37<00:00,  2.81it/s]\n",
            "[INFO | <ipython-input-18-7fa915c1dd65>:217 - filter_sentences_on_answer() ] Size of the train dataset: 86821\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:221 - filter_sentences_on_answer() ] Sentences Filtered on Answers\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:197 - filter_sentences_on_answer() ] Starting to filter sentences on answer\n",
            "100%|██████████| 35/35 [00:11<00:00,  2.65it/s]\n",
            "[INFO | <ipython-input-18-7fa915c1dd65>:217 - filter_sentences_on_answer() ] Size of the test dataset: 5928\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:221 - filter_sentences_on_answer() ] Sentences Filtered on Answers\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:142 -     preprocess_squad() ] Now we will split train set to train and valid set\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:51 -    split_train_valid() ] Splitting the SQUAD's train set into train and valid\n",
            "[DEBUG | <ipython-input-18-7fa915c1dd65>:70 -    split_train_valid() ] # of Paragraphs: 86821 # of Questions: 86821 \n",
            "100%|██████████| 86821/86821 [00:00<00:00, 473613.27it/s]\n",
            "[INFO | <ipython-input-18-7fa915c1dd65>:105 -    split_train_valid() ] Total Trainset: 78120 | Total ValidSet: 8701\n",
            "[INFO | <ipython-input-18-7fa915c1dd65>:145 -     preprocess_squad() ] SQUAD Preprocessed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLBZCWuMswAX",
        "colab_type": "text"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MZt97zXrjH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = os.path.join(DATA_FOLDER, DATA_FOLDER_PROCESSED)\n",
        "\n",
        "\n",
        "def load_dataset(\n",
        "    dataset_name=\"SQUAD\",\n",
        "    tokenizer=word_tokenizer,\n",
        "    init_token=\"<sos>\",\n",
        "    eos_token=\"<eos>\",\n",
        "    lower=True,\n",
        "    use_glove=True,\n",
        "    source_vocab=45000,\n",
        "    target_vocab=28000,\n",
        "    batch_size=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Method Loads the dataset from location and returns three iterators and SRC and TRG fields\n",
        "    \"\"\"\n",
        "    logger.debug(\"Loading {} dataset\".format(dataset_name))\n",
        "    SRC = data.Field(\n",
        "        tokenize=tokenizer,\n",
        "        init_token=init_token,\n",
        "        eos_token=eos_token,\n",
        "        lower=True,\n",
        "        include_lengths=True,\n",
        "    )\n",
        "    TRG = data.Field(\n",
        "        tokenize=tokenizer, init_token=init_token, eos_token=eos_token, lower=True\n",
        "    )\n",
        "\n",
        "    location = os.path.join(FILE_PATH, dataset_name)\n",
        "\n",
        "    logger.debug(\"Loading from location: {}\".format(location))\n",
        "    start_time = time.time()\n",
        "    train_dataset, valid_dataset, test_dataset = TranslationDataset.splits(\n",
        "        exts=(\".paragraphs\", \".questions\"),\n",
        "        fields=(SRC, TRG),\n",
        "        path=location,\n",
        "        train=\"train\",\n",
        "        validation=\"valid\",\n",
        "        test=\"test\",\n",
        "    )\n",
        "\n",
        "    logger.debug(\n",
        "        \"Number of Samples: Training = {} | Validation = {} | Testing = {}\".format(\n",
        "            len(train_dataset.examples),\n",
        "            len(valid_dataset.examples),\n",
        "            len(test_dataset.examples),\n",
        "        )\n",
        "    )\n",
        "    logger.debug(\"Time Taken: {:.6f}s\".format(time.time() - start_time))\n",
        "    logger.debug(\"Building Vocab\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    if use_glove:\n",
        "        logger.debug(\"Using Glove vectors\")\n",
        "        SRC.build_vocab(train_dataset, max_size=source_vocab, vectors=\"glove.6B.300d\")\n",
        "        TRG.build_vocab(train_dataset, max_size=target_vocab, vectors=\"glove.6B.300d\")\n",
        "    else:\n",
        "        SRC.build_vocab(train_dataset, max_size=source_vocab)\n",
        "        TRG.build_vocab(train_dataset, max_size=target_vocab)\n",
        "\n",
        "    logger.info(\n",
        "        \"Vocabulary Built! Source Tokens = {} | Target Tokens = {}  \\nCreating Iterators\".format(\n",
        "            len(SRC.vocab), len(TRG.vocab)\n",
        "        )\n",
        "    )\n",
        "    logger.debug(\"Time Taken: {:.6f}s\".format(time.time() - start_time))\n",
        "\n",
        "    return (\n",
        "        BucketIterator.splits(\n",
        "            (train_dataset, valid_dataset, test_dataset),\n",
        "            batch_size=batch_size,\n",
        "            sort_within_batch=True,\n",
        "            sort_key=lambda x: len(x.src),\n",
        "            device=device,\n",
        "        ),\n",
        "        SRC,\n",
        "        TRG,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EV74o1Ts2Cn",
        "colab_type": "text"
      },
      "source": [
        "## Defining Vanilla Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipjq0L7zsx_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A bidirectional GRU Encoder\n",
        "    Input:\n",
        "        input_dim: Vocab length of input\n",
        "        embedding_dim: Dimension of Embeddings\n",
        "        hidden_dim: Dimension of Hidden vectors of LSTM\n",
        "        n_layers: Layers of LSTM\n",
        "        dropout: Dropout applied\n",
        "    Returns:\n",
        "        hidden: hidden layers of LSTM\n",
        "        cell: cell state of LSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        # src = [src_len, batch_size]\n",
        "        # src_len = [batch_size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "\n",
        "        packed_output, hidden = self.gru(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A Decoder GRU Decoder\n",
        "    Input:\n",
        "        output_dim: Vocab length of the output\n",
        "        embedding_dim: Decoder Embedding Dimension\n",
        "        hidden_dim: Hidden Dimensions of the GRU Layer\n",
        "        n_layer: Number of layer for GRU\n",
        "        dropout: Dropout Applied\n",
        "    Output:\n",
        "        prediction: Output of the Fully connected layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            embedding_dim, 2 * hidden_dim, num_layers=n_layers, dropout=dropout\n",
        "        )\n",
        "        self.fc_out = nn.Linear(2 * hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, (hidden))\n",
        "        output = self.dropout(self.fc_out(output))\n",
        "        prediction = output.squeeze(0)\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "class VanillaSeq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Final EncoderDecoderModel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(VanillaSeq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing=0.5):\n",
        "        encoder_hidden = self.encoder(src, src_len)\n",
        "\n",
        "        encoder_hidden = encoder_hidden.view(encoder_hidden.shape[0]//2, encoder_hidden.shape[1], encoder_hidden.shape[2] * 2)\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Take first letter of the input\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, encoder_hidden)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            teacher_forcing = random.random() < teacher_forcing\n",
        "\n",
        "            if teacher_forcing:\n",
        "                input = trg[t]\n",
        "            else:\n",
        "                input = torch.argmax(output, dim=1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAzggFOrs_M2",
        "colab_type": "text"
      },
      "source": [
        "## Training Vanilla Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_qSe6Eis4Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \"\"\"\n",
        "    Generic Training Method\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
        "\n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_len, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.detach().item()\n",
        "\n",
        "        del output\n",
        "        del loss\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Generic Evaluation Method\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.detach().item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train_vanilla_seq2seq(\n",
        "    dataset_name=\"SQUAD\",\n",
        "    clip=1,\n",
        "    lr=0.001,\n",
        "    validation=True,\n",
        "    epochs=5,\n",
        "    teacher_forcing=0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Method to train the Vanilla Seq2Seq\n",
        "    \"\"\"\n",
        "\n",
        "    logger.debug(\"Data Loading\")\n",
        "\n",
        "    (train_iterator, valid_iterator, test_iterator), SRC, TRG = load_dataset(\n",
        "        dataset_name,\n",
        "        source_vocab=VANILLA_SEQ2SEQ[\"INPUT_DIM\"],\n",
        "        target_vocab=VANILLA_SEQ2SEQ[\"OUTPUT_DIM\"],\n",
        "    )\n",
        "\n",
        "    INPUT_DIM = len(SRC.vocab)\n",
        "    OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "    logger.debug(\"Initializing Models on {}\".format(device))\n",
        "    enc = Encoder(\n",
        "        INPUT_DIM,\n",
        "        VANILLA_SEQ2SEQ[\"ENC_EMB_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"HID_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"N_LAYERS\"],\n",
        "        VANILLA_SEQ2SEQ[\"DROPOUT\"],\n",
        "    )\n",
        "    dec = Decoder(\n",
        "        OUTPUT_DIM,\n",
        "        VANILLA_SEQ2SEQ[\"DEC_EMB_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"HID_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"N_LAYERS\"],\n",
        "        VANILLA_SEQ2SEQ[\"DROPOUT\"],\n",
        "    )\n",
        "\n",
        "    model = VanillaSeq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "    logger.info(\n",
        "        \"The model has {:,} trainable parameters\".format(count_parameters(model))\n",
        "    )\n",
        "\n",
        "    logger.debug(model)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    TRG_PADDING = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PADDING)\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
        "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), \"trained_model.pt\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        logger.info(\n",
        "            \"Epoch: {:02} | Time: {}m {}s\".format(epoch + 1, epoch_mins, epoch_secs)\n",
        "        )\n",
        "        logger.info(\n",
        "            \"\\tTrain Loss: {:.3f} | Train PPL: {:7.3f}\".format(\n",
        "                train_loss, math.exp(train_loss)\n",
        "            )\n",
        "        )\n",
        "        logger.info(\n",
        "            \"\\t Val. Loss: {:.3f} |  Val. PPL: {:7.3f}\".format(\n",
        "                valid_loss, math.exp(valid_loss)\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ4tk_ZctTvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "cc5cb7ce-b35d-4432-b6f3-26f1100fc036"
      },
      "source": [
        "train_vanilla_seq2seq()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | <ipython-input-8-d1ec07b35905>:91 - train_vanilla_seq2seq() ] Data Loading\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:18 -         load_dataset() ] Loading SQUAD dataset\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:32 -         load_dataset() ] Loading from location: data/processed/SQUAD\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:47 -         load_dataset() ] Number of Samples: Training = 78120 | Validation = 8701 | Testing = 5928\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:50 -         load_dataset() ] Time Taken: 102.617422s\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:51 -         load_dataset() ] Building Vocab\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:55 -         load_dataset() ] Using Glove vectors\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[INFO | <ipython-input-6-796282a7e7a5>:64 -         load_dataset() ] Vocabulary Built! Source Tokens = 45004 | Target Tokens = 28004  \n",
            "Creating Iterators\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:67 -         load_dataset() ] Time Taken: 3.276188s\n",
            "[DEBUG | <ipython-input-8-d1ec07b35905>:102 - train_vanilla_seq2seq() ] Initializing Models on cuda\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:121 - train_vanilla_seq2seq() ] The model has 79,324,004 trainable parameters\n",
            "[DEBUG | <ipython-input-8-d1ec07b35905>:124 - train_vanilla_seq2seq() ] VanillaSeq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(45004, 300)\n",
            "    (gru): GRU(300, 600, num_layers=2, dropout=0.7, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.7, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(28004, 300)\n",
            "    (gru): GRU(300, 1200, num_layers=2, dropout=0.7)\n",
            "    (fc_out): Linear(in_features=1200, out_features=28004, bias=True)\n",
            "    (dropout): Dropout(p=0.7, inplace=False)\n",
            "  )\n",
            ")\n",
            "100%|██████████| 306/306 [11:20<00:00,  2.00s/it]\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:151 - train_vanilla_seq2seq() ] Epoch: 01 | Time: 11m 46s\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:155 - train_vanilla_seq2seq() ] \tTrain Loss: 9.092 | Train PPL: 8880.868\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:160 - train_vanilla_seq2seq() ] \t Val. Loss: 8.255 |  Val. PPL: 3846.103\n",
            "100%|██████████| 306/306 [11:12<00:00,  2.09s/it]\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:151 - train_vanilla_seq2seq() ] Epoch: 02 | Time: 11m 38s\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:155 - train_vanilla_seq2seq() ] \tTrain Loss: 8.998 | Train PPL: 8088.189\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:160 - train_vanilla_seq2seq() ] \t Val. Loss: 8.299 |  Val. PPL: 4021.004\n",
            "100%|██████████| 306/306 [11:20<00:00,  2.22s/it]\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:151 - train_vanilla_seq2seq() ] Epoch: 03 | Time: 11m 47s\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:155 - train_vanilla_seq2seq() ] \tTrain Loss: 8.961 | Train PPL: 7796.195\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:160 - train_vanilla_seq2seq() ] \t Val. Loss: 8.308 |  Val. PPL: 4057.158\n",
            "100%|██████████| 306/306 [11:22<00:00,  2.26s/it]\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:151 - train_vanilla_seq2seq() ] Epoch: 04 | Time: 11m 48s\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:155 - train_vanilla_seq2seq() ] \tTrain Loss: 8.940 | Train PPL: 7633.252\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:160 - train_vanilla_seq2seq() ] \t Val. Loss: 8.297 |  Val. PPL: 4011.446\n",
            "100%|██████████| 306/306 [11:14<00:00,  2.05s/it]\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:151 - train_vanilla_seq2seq() ] Epoch: 05 | Time: 11m 40s\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:155 - train_vanilla_seq2seq() ] \tTrain Loss: 8.928 | Train PPL: 7540.659\n",
            "[INFO | <ipython-input-8-d1ec07b35905>:160 - train_vanilla_seq2seq() ] \t Val. Loss: 8.300 |  Val. PPL: 4023.230\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It286MLTtXMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "49baa52c-b88d-46b5-9839-57d4e555062d"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.8 GB  | Proc size: 4.4 GB\n",
            "GPU RAM Free: 9198MB | Used: 2243MB | Util  20% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV_q4GVSI1OE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_test_set(dataset_name=\"SQUAD\"):\n",
        "    (train_iterator, valid_iterator, test_iterator), SRC, TRG = load_dataset(\n",
        "        dataset_name,\n",
        "        source_vocab=VANILLA_SEQ2SEQ[\"INPUT_DIM\"],\n",
        "        target_vocab=VANILLA_SEQ2SEQ[\"OUTPUT_DIM\"],\n",
        "    )\n",
        "\n",
        "    INPUT_DIM = len(SRC.vocab)\n",
        "    OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "    logger.debug(\"Initializing Models on {}\".format(device))\n",
        "    enc = Encoder(\n",
        "        INPUT_DIM,\n",
        "        VANILLA_SEQ2SEQ[\"ENC_EMB_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"HID_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"N_LAYERS\"],\n",
        "        VANILLA_SEQ2SEQ[\"DROPOUT\"],\n",
        "    )\n",
        "    dec = Decoder(\n",
        "        OUTPUT_DIM,\n",
        "        VANILLA_SEQ2SEQ[\"DEC_EMB_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"HID_DIM\"],\n",
        "        VANILLA_SEQ2SEQ[\"N_LAYERS\"],\n",
        "        VANILLA_SEQ2SEQ[\"DROPOUT\"],\n",
        "    )\n",
        "    model = VanillaSeq2Seq(enc, dec, device).to(device)\n",
        "    model.load_state_dict(torch.load('trained_model.pt'))\n",
        "\n",
        "    TRG_PADDING = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PADDING)\n",
        "\n",
        "    test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "    logger.info(\n",
        "            \"Test Loss: {:.3f} | Train PPL: {:7.3f}\".format(\n",
        "                test_loss, math.exp(test_loss)\n",
        "            )\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WclYNieXtF3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b4d4f609-3276-431b-be11-e8147807bc0a"
      },
      "source": [
        "evaluate_test_set()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | <ipython-input-6-796282a7e7a5>:18 -         load_dataset() ] Loading SQUAD dataset\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:32 -         load_dataset() ] Loading from location: data/processed/SQUAD\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:47 -         load_dataset() ] Number of Samples: Training = 78120 | Validation = 8701 | Testing = 5928\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:50 -         load_dataset() ] Time Taken: 100.966801s\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:51 -         load_dataset() ] Building Vocab\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:55 -         load_dataset() ] Using Glove vectors\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[INFO | <ipython-input-6-796282a7e7a5>:64 -         load_dataset() ] Vocabulary Built! Source Tokens = 45004 | Target Tokens = 28004  \n",
            "Creating Iterators\n",
            "[DEBUG | <ipython-input-6-796282a7e7a5>:67 -         load_dataset() ] Time Taken: 3.252636s\n",
            "[DEBUG | <ipython-input-21-eb03bdbed8ce>:11 -    evaluate_test_set() ] Initializing Models on cuda\n",
            "[INFO | <ipython-input-21-eb03bdbed8ce>:37 -    evaluate_test_set() ] \\Test Loss: 8.243 | Train PPL: 3802.540\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yGSMPnFtoUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}