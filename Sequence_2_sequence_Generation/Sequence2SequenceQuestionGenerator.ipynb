{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/NLPinEnglishLearning/blob/master/Sequence_2_sequence_Generation/Sequence2SequenceQuestionGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLtWBqUscOtx",
        "colab_type": "text"
      },
      "source": [
        "# Question Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T808ilq_g2bm",
        "colab_type": "text"
      },
      "source": [
        "Additional Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LU58nY0cTuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSk9ez44VPd2",
        "colab_type": "code",
        "outputId": "efeee744-2b46-4175-f6b1-79fd2a1ef7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!rm -rf NLPinEnglishLearning\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import subprocess\n",
        "\n",
        "def run_command(command):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        shell=True,\n",
        "        encoding=\"utf-8\",\n",
        "        errors=\"replace\",\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        realtime_output = process.stdout.readline()\n",
        "\n",
        "        if realtime_output == \"\" and process.poll() is not None:\n",
        "            break\n",
        "\n",
        "        if realtime_output:\n",
        "            print(realtime_output.strip(), flush=True)\n",
        "\n",
        "\n",
        "cmd_string = 'git clone https://github.com/shivammehta007/NLPinEnglishLearning.git'\n",
        "run_command(cmd_string)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLPinEnglishLearning'...\n",
            "Checking out files:  94% (127/134)\n",
            "Checking out files:  95% (128/134)\n",
            "Checking out files:  96% (129/134)\n",
            "Checking out files:  97% (130/134)\n",
            "Checking out files:  98% (132/134)\n",
            "Checking out files:  99% (133/134)\n",
            "Checking out files: 100% (134/134)\n",
            "Checking out files: 100% (134/134), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YST7xIhop87U",
        "colab_type": "code",
        "outputId": "cd62341a-7435-4701-d50e-5b5abae9eb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "ae09a37a-3205-46af-ea92-4de5f5e8a325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2926  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "557e35ef-50c4-4f7f-ee26-4c311af8811e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/NLPinEnglishLearning/Sequence_2_sequence_Generation/FairSeq_models/data/raw\n",
            " 97% 376M/386M [00:05<00:00, 80.7MB/s]\n",
            "100% 386M/386M [00:06<00:00, 66.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "a82f19da-6e71-4c7a-c99c-ae3b5fde6204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8P_KmkLuS_K",
        "colab_type": "text"
      },
      "source": [
        "## Training Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vp6priLuScN",
        "colab_type": "code",
        "outputId": "13e81d25-8633-4db3-d8cb-41e67eb80795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd QuestionGenerator/Sequence_2_sequence_Generation/Baseline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/Sequence_2_sequence_Generation/Baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8InSCqJsz2t2",
        "colab_type": "text"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-o86yRzqGs",
        "colab_type": "code",
        "outputId": "ec68274c-5ea4-47b7-affc-5587049e2657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!python datadownloader.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: datadownloader.py [-h] [-d DATASET] [-f]\n",
            "\n",
            "Utility to download datasets currently available datasets: SQUAD\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -d DATASET, --dataset DATASET\n",
            "                        Name of Dataset\n",
            "  -f, --force           Skip Directory check and force override download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFe8BPEHz5Oq",
        "colab_type": "text"
      },
      "source": [
        "### PreProcess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_7dVFhFuRfr",
        "colab_type": "code",
        "outputId": "c39b36ca-89a5-4046-ac54-6c6e5ac93032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!python preprocessing.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: preprocessing.py [-h] [-d DATASET] [-m MODE] [-f]\n",
            "\n",
            "Utility to Preprocess datasets currently available datasets: SQUAD\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -d DATASET, --dataset DATASET\n",
            "                        Name of Dataset\n",
            "  -m MODE, --mode MODE  Split on ANSWER or QUESTION\n",
            "  -f, --filter          filter the sentences on answers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC3E8GHCz_gT",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZORRhCvzJE-",
        "colab_type": "code",
        "outputId": "fcafe119-b9c4-4cbd-935d-9f6a820f75ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!python train.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [-d DATASET] [-m MODEL] [-c CLIPNORM] [-l LEARNINGRATE]\n",
            "                [-v] [-e EPOCHS] [-t TEACHERFORCING] [-tmp TRAINED_MODEL_PATH]\n",
            "\n",
            "Utility to Train datasets {1: 'VanillaSeq2Seq'}\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -d DATASET, --dataset DATASET\n",
            "                        which dataset to train on\n",
            "  -m MODEL, --model MODEL\n",
            "                        Which Model to Train\n",
            "  -c CLIPNORM, --clipnorm CLIPNORM\n",
            "                        Value to clip gradients\n",
            "  -l LEARNINGRATE, --learningrate LEARNINGRATE\n",
            "                        Learning rate of Adam Optmizer\n",
            "  -v, --validation      Flag to turn validation on and off\n",
            "  -e EPOCHS, --epochs EPOCHS\n",
            "                        Number of Epochs to train\n",
            "  -t TEACHERFORCING, --teacherforcing TEACHERFORCING\n",
            "                        Teacher Forcing\n",
            "  -tmp TRAINED_MODEL_PATH, --trained-model-path TRAINED_MODEL_PATH\n",
            "                        Load the model from the directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx2xIwdY0A97",
        "colab_type": "text"
      },
      "source": [
        "### Inference Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHjlEcrnzmoe",
        "colab_type": "code",
        "outputId": "0925d32e-fb39-42ba-df4b-7845dc6df6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!python inference.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: inference.py [-h] [-d DATASET] [-m MODEL] [-ml MAX_LEN]\n",
            "                    [-l MODEL_LOCATION]\n",
            "\n",
            "Utility to generate Inference\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -d DATASET, --dataset DATASET\n",
            "                        which dataset to train on\n",
            "  -m MODEL, --model MODEL\n",
            "                        Which Model to Train\n",
            "  -ml MAX_LEN, --max-len MAX_LEN\n",
            "                        Max length of the question to be generated\n",
            "  -l MODEL_LOCATION, --model-location MODEL_LOCATION\n",
            "                        Location of Model File\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvWOHszAk_Xf",
        "colab_type": "text"
      },
      "source": [
        "# Other Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfnhruSsTpuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc8103ad-7a64-415b-a078-42657c9f2765"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLPinEnglishLearning/Sequence_2_sequence_Generation/FairSeq_models/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCsoDiJ4k9I8",
        "colab_type": "code",
        "outputId": "77e33c21-31fa-4b6e-9de2-7e5137a4e4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd ../FairSeq_models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/Sequence_2_sequence_Generation/FairSeq_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IaIDte6mzQK",
        "colab_type": "code",
        "outputId": "fc86fb0d-5e5c-4114-fbe6-8ee5aa0920f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!python preprocess.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO | preprocess.py:92 -           preprocess() ] Running FairSeq Preprocessing to convert files into fairseq binaries\n",
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/fairseq_binaries', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=45000, nwordstgt=28000, only_source=False, optimizer='nag', padding_factor=8, seed=1234, source_lang='sentence', srcdict=None, target_lang='question', task='translation', tensorboard_logdir='', testpref='data/processed/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='data/processed/train', user_dir=None, validpref='data/processed/valid', workers=1)\n",
            "| [sentence] Dictionary: 44999 types\n",
            "| [sentence] data/processed/train.sentence: 70484 sents, 2386532 tokens, 1.32% replaced by <unk>\n",
            "| [sentence] Dictionary: 44999 types\n",
            "| [sentence] data/processed/valid.sentence: 10570 sents, 368586 tokens, 5.17% replaced by <unk>\n",
            "| [sentence] Dictionary: 44999 types\n",
            "| [sentence] data/processed/test.sentence: 11877 sents, 397472 tokens, 5.35% replaced by <unk>\n",
            "| [question] Dictionary: 27999 types\n",
            "| [question] data/processed/train.question: 70484 sents, 866317 tokens, 0.85% replaced by <unk>\n",
            "| [question] Dictionary: 27999 types\n",
            "| [question] data/processed/valid.question: 10570 sents, 131365 tokens, 4.72% replaced by <unk>\n",
            "| [question] Dictionary: 27999 types\n",
            "| [question] data/processed/test.question: 11877 sents, 149804 tokens, 5.33% replaced by <unk>\n",
            "| Wrote preprocessed data to data/fairseq_binaries\n",
            "[DEBUG | preprocess.py:136 -             <module>() ] Utility Finished Execution in: 35.7204ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rVCq14cnS6Q",
        "colab_type": "code",
        "outputId": "5b0b29af-310f-4e3a-e85b-35a1a7755e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!python train.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [-m {LSTM,CNN,Transformer}] [-n NUM_EPOCHS]\n",
            "                [-b BATCH_SIZE]\n",
            "\n",
            "Utility to train the models\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -m {LSTM,CNN,Transformer}, --model {LSTM,CNN,Transformer}\n",
            "                        Select the Seq2Seq Model to train\n",
            "  -n NUM_EPOCHS, --num-epochs NUM_EPOCHS\n",
            "                        Number of epochs to train\n",
            "  -b BATCH_SIZE, --batch-size BATCH_SIZE\n",
            "                        Training Batch Size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duH2P4mKq-nT",
        "colab_type": "code",
        "outputId": "037ab390-f849-4432-b898-543b9a92b3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: generate.py [-h] [-m {LSTM,CNN}] [-sm {best,last}] [-b BATCH_SIZE]\n",
            "\n",
            "Utility to Generate Sentences from Test Set\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -m {LSTM,CNN}, --model {LSTM,CNN}\n",
            "                        Select the Seq2Seq Model to train\n",
            "  -sm {best,last}, --sub-model {best,last}\n",
            "                        Select which model to generate with the one with best\n",
            "                        valid loss or the last epoch trained model\n",
            "  -b BATCH_SIZE, --batch-size BATCH_SIZE\n",
            "                        Training Batch Size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYmHqqJAPUYQ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KifZn9hLPTBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbCvK5CVtQ42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "a3cc0c26-e266-4b80-86fb-7ad7f15135f8"
      },
      "source": [
        "!nlg-eval --setup"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "\u001b[31mInstalling to /root/.cache/nlgeval\u001b[0m\n",
            "\u001b[31mIn case of incomplete downloads, delete the directory and run `nlg-eval --setup /root/.cache/nlgeval' again.\u001b[0m\n",
            "Downloading https://raw.githubusercontent.com/robmsmt/glove-gensim/4c2224bccd61627b76c50a5e1d6afd1c82699d22/glove2word2vec.py to /usr/local/lib/python3.6/dist-packages/nlgeval/word2vec.\n",
            "Downloading http://nlp.stanford.edu/data/glove.6B.zip to /root/.cache/nlgeval.\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/utable.npy to /root/.cache/nlgeval.\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/dictionary.txt to /root/.cache/nlgeval.\n",
            "glove2word2vec.py: 100% 1.00/1.00 [00:00<00:00, 467 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/btable.npy to /root/.cache/nlgeval.\n",
            "dictionary.txt: 550 chunks [00:01, 377 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz to /root/.cache/nlgeval.\n",
            "uni_skip.npz: 100% 634/634 [00:40<00:00, 15.7 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl to /root/.cache/nlgeval.\n",
            "uni_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.37k chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz to /root/.cache/nlgeval.\n",
            "bi_skip.npz: 100% 276/276 [00:18<00:00, 14.7 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl to /root/.cache/nlgeval.\n",
            "bi_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.74k chunks/s]\n",
            "Downloading https://raw.githubusercontent.com/moses-smt/mosesdecoder/b199e654df2a26ea58f234cbb642e89d9c1f269d/scripts/generic/multi-bleu.perl to /usr/local/lib/python3.6/dist-packages/nlgeval/multibleu.\n",
            "multi-bleu.perl: 100% 1.00/1.00 [00:00<00:00, 370 chunks/s]\n",
            "utable.npy: 100% 2.23k/2.23k [02:20<00:00, 15.9 chunks/s]\n",
            "btable.npy: 100% 2.23k/2.23k [02:24<00:00, 15.5 chunks/s]\n",
            "glove.6B.zip: 100% 823/823 [06:29<00:00, 2.11 chunks/s]\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-06-03 10:54:52,223 : MainThread : INFO : 400000 lines with 300 dimensions\n",
            "2020-06-03 10:55:02,104 : MainThread : INFO : Model /root/.cache/nlgeval/glove.6B.300d.model.txt successfully created !!\n",
            "2020-06-03 10:55:02,104 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-06-03 10:56:59,747 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-06-03 10:56:59,747 : MainThread : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "2020-06-03 10:57:00,658 : MainThread : INFO : Most similar to king are: [('queen', 0.6336469054222107), ('prince', 0.619662344455719), ('monarch', 0.5899620652198792), ('kingdom', 0.5791267156600952), ('throne', 0.5606487989425659), ('ii', 0.5562329888343811), ('iii', 0.5503199100494385), ('crown', 0.5224862694740295), ('reign', 0.521735429763794), ('kings', 0.5066401362419128)]\n",
            "2020-06-03 10:57:00,658 : MainThread : INFO : Similarity score between woman and man is 0.6998663 \n",
            "2020-06-03 10:57:00,658 : MainThread : INFO : Finished running --setup\n",
            "2020-06-03 10:57:00,773 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-06-03 10:58:57,537 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-06-03 10:58:57,538 : MainThread : INFO : saving Word2VecKeyedVectors object under /root/.cache/nlgeval/glove.6B.300d.model.bin, separately None\n",
            "2020-06-03 10:58:57,538 : MainThread : INFO : storing np array 'vectors' to /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy\n",
            "2020-06-03 10:58:58,584 : MainThread : INFO : not storing attribute vectors_norm\n",
            "2020-06-03 10:58:59,440 : MainThread : INFO : saved /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "2020-06-03 10:58:59,440 : MainThread : INFO : loading Word2VecKeyedVectors object from /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "2020-06-03 10:59:00,449 : MainThread : INFO : loading vectors from /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy with mmap=r\n",
            "2020-06-03 10:59:00,451 : MainThread : INFO : setting ignored attribute vectors_norm to None\n",
            "2020-06-03 10:59:00,451 : MainThread : INFO : loaded /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "WARNING: could not read rc.json in /root/.config/nlgeval, overwriting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TyGJekmQEHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee4923c3-45d7-47c4-d1ed-5b5fa6ff76cb"
      },
      "source": [
        "%cd NLPinEnglishLearning/Sequence_2_sequence_Generation/FairSeq_models/data/raw/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLPinEnglishLearning/Sequence_2_sequence_Generation/FairSeq_models/data/raw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJt-pMTG-ZM3",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IvBtnZ2TYWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!grep ^H lstm.out | cut -f3- > lstm.out.sys\n",
        "!grep ^T lstm.out | cut -f2- > lstm.out.ref"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09TKqbaX90fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f40c964b-7433-4b49-c1b5-b1409f5be2ad"
      },
      "source": [
        "!nlg-eval --hypothesis=lstm.out.sys --references=lstm.out.ref"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
            "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
            "Bleu_1: 0.341780\n",
            "Bleu_2: 0.185160\n",
            "Bleu_3: 0.117828\n",
            "Bleu_4: 0.079173\n",
            "METEOR: 0.129704\n",
            "ROUGE_L: 0.337291\n",
            "CIDEr: 0.711782\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "\n",
            "Aborted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMXAk6b-eTK",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5pJNSoe-ij7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!grep ^H cnn.out | cut -f3- > cnn.out.sys\n",
        "!grep ^T cnn.out | cut -f2- > cnn.out.ref"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noDkGTQX-msb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b3a64e6c-ed55-4282-86d4-8633199e402c"
      },
      "source": [
        "!nlg-eval --hypothesis=cnn.out.sys --references=cnn.out.ref"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
            "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
            "Bleu_1: 0.322365\n",
            "Bleu_2: 0.166536\n",
            "Bleu_3: 0.103490\n",
            "Bleu_4: 0.067834\n",
            "METEOR: 0.125735\n",
            "ROUGE_L: 0.323530\n",
            "CIDEr: 0.578102\n",
            "\n",
            "Aborted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfCppPcw_6rj",
        "colab_type": "text"
      },
      "source": [
        "## Tranformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJE3zQaVAA2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!grep ^H transformer.out | cut -f3- > transformer.out.sys\n",
        "!grep ^T transformer.out | cut -f2- > transformer.out.ref"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqI37Na6ADsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "35869a13-64fb-4faa-961c-039cd67c6ee7"
      },
      "source": [
        "!nlg-eval --hypothesis=transformer.out.sys --references=transformer.out.ref"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
            "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
            "Bleu_1: 0.277407\n",
            "Bleu_2: 0.113273\n",
            "Bleu_3: 0.059634\n",
            "Bleu_4: 0.034863\n",
            "METEOR: 0.072429\n",
            "ROUGE_L: 0.272074\n",
            "CIDEr: 0.172780\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5g80hPJAGeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}