{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomClassifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1PQCpSDGct4BTv3x9oH8GD1HIdNihgWj0",
      "authorship_tag": "ABX9TyPwn/x9KN7s86CHjmxg2Nmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/field_classifier/CustomClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCToCs_b4d1",
        "colab_type": "text"
      },
      "source": [
        "# Testing Classifier Deep Learning Architecture Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9CNpxNblqe",
        "colab_type": "code",
        "outputId": "60af039f-bc4d-4e8d-d9ce-e57fb990f330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!rm -rf QuestionGenerator\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone --single-branch --branch field_classifier  https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, 'QuestionGenerator')\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: shivammehta007\n",
            "Password: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW_iNs97b9e9",
        "colab_type": "code",
        "outputId": "ef3c6aad-ef6e-442d-f67e-23fde52a6737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -rf QuestionGenerator/FromScratch\n",
        "%cd QuestionGenerator/classifier/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxftkthaPIGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "42bcec18-658c-4b07-dd19-f913d2078877"
      },
      "source": [
        "# !rm -rf data/processed/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d480480aad0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf data/processed/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAatXS64cdOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python preprocessdata.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0OzHqrpJEx",
        "colab_type": "code",
        "outputId": "a40377c2-200b-4a87-e3aa-da39a7450d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%ls -a"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34mconfig\u001b[0m/  datasetloader.py    \u001b[01;34mmodel\u001b[0m/             \u001b[01;34mtrained\u001b[0m/  utility.py\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mdata\u001b[0m/    helperfunctions.py  preprocessdata.py  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "2fbdc5fc-88c5-4980-c187-8546ac08d47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2626  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "6ce35cf0-c4fe-41fd-9cfb-2fef4cce0c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/QuestionGenerator/classifier\n",
            " 96% 370M/386M [00:05<00:00, 56.1MB/s]\n",
            "100% 386M/386M [00:05<00:00, 70.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "1571c5b2-94d8-4978-c86e-164d1a0d1a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "a24c4274-d87d-41ca-d0f7-3844e086f11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2621  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "6d342ca5-01a7-465d-bc3f-2c9e01ec9e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/QuestionGenerator/classifier\n",
            " 96% 369M/386M [00:10<00:00, 38.3MB/s]\n",
            "100% 386M/386M [00:10<00:00, 36.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "60fca98b-3c48-4ee0-8b05-e8351f5398d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nhgkJx_ne2G",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIWmWDYBxNI7",
        "colab_type": "code",
        "outputId": "98f69728-e208-4d3f-894d-6f486ad3aef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!python train.py -h"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [-s SEED] [-loc MODEL_LOCATION] [-b BIDIRECTIONAL]\n",
            "                [-d DROPOUT] [-e EMBEDDING_DIM] [-hd HIDDEN_DIM] [-l N_LAYERS]\n",
            "                [-lr LEARNING_RATE] [-n EPOCHS] [-batch BATCH_SIZE]\n",
            "                [-f FREEZE_EMBEDDINGS] [-t {multi,single}]\n",
            "                [-l2 L2_REGULARIZATION]\n",
            "                [-m {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer}]\n",
            "\n",
            "Utility to train the Model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -s SEED, --seed SEED  Set custom seed for reproducibility\n",
            "  -loc MODEL_LOCATION, --model-location MODEL_LOCATION\n",
            "                        Give an already trained model location to use and\n",
            "                        train more epochs on it\n",
            "  -b BIDIRECTIONAL, --bidirectional BIDIRECTIONAL\n",
            "                        Makes the model Bidirectional\n",
            "  -d DROPOUT, --dropout DROPOUT\n",
            "                        Dropout count for the model\n",
            "  -e EMBEDDING_DIM, --embedding-dim EMBEDDING_DIM\n",
            "                        Embedding Dimensions\n",
            "  -hd HIDDEN_DIM, --hidden-dim HIDDEN_DIM\n",
            "                        Hidden dimensions of the RNN\n",
            "  -l N_LAYERS, --n-layers N_LAYERS\n",
            "                        Number of layers in RNN\n",
            "  -lr LEARNING_RATE, --learning-rate LEARNING_RATE\n",
            "                        Learning rate of Adam Optimizer\n",
            "  -n EPOCHS, --epochs EPOCHS\n",
            "                        Number of Epochs to train model\n",
            "  -batch BATCH_SIZE, --batch_size BATCH_SIZE\n",
            "                        Number of Epochs to train model\n",
            "  -f FREEZE_EMBEDDINGS, --freeze-embeddings FREEZE_EMBEDDINGS\n",
            "                        Freeze Embeddings of Model\n",
            "  -t {multi,single}, --tag {multi,single}\n",
            "                        Use two different dataset type, multi type and single\n",
            "                        type where all are merged into same key\n",
            "  -l2 L2_REGULARIZATION, --l2-regularization L2_REGULARIZATION\n",
            "                        Value of alpha in l2 regularization 0 means no\n",
            "                        regularization\n",
            "  -m {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer}, --model {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer}\n",
            "                        select the classifier to train on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd6-Cp6UiuT8",
        "colab_type": "text"
      },
      "source": [
        "## RNNClassifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adzPuBh7NbLC",
        "colab_type": "text"
      },
      "source": [
        "### RNNHiddenClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHpPoyGcz9H",
        "colab_type": "code",
        "outputId": "87e8efad-5206-45e7-d4d6-81d9e511660c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:284 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:285 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:287 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:294 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:70 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:148 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:154 - initialize_new_model() ] Model Initialized with 443,147 trainiable parameters\n",
            "[DEBUG | train.py:166 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:319 -             <module>() ] RNNHiddenClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 102.48it/s]\n",
            "100% 2/2 [00:00<00:00, 281.08it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.386 | Train Acc: 12.22%\n",
            "\t Val. Loss: 2.246 |  Val. Acc: 24.09%\n",
            "100% 9/9 [00:00<00:00, 129.86it/s]\n",
            "100% 2/2 [00:00<00:00, 255.12it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.272 | Train Acc: 16.73%\n",
            "\t Val. Loss: 2.199 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 117.76it/s]\n",
            "100% 2/2 [00:00<00:00, 248.30it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.222 | Train Acc: 18.64%\n",
            "\t Val. Loss: 2.191 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 128.22it/s]\n",
            "100% 2/2 [00:00<00:00, 248.32it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.179 | Train Acc: 20.31%\n",
            "\t Val. Loss: 2.149 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 126.04it/s]\n",
            "100% 2/2 [00:00<00:00, 185.49it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.134 | Train Acc: 23.41%\n",
            "\t Val. Loss: 2.113 |  Val. Acc: 16.20%\n",
            "100% 9/9 [00:00<00:00, 131.62it/s]\n",
            "100% 2/2 [00:00<00:00, 276.14it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.096 | Train Acc: 27.54%\n",
            "\t Val. Loss: 2.040 |  Val. Acc: 26.08%\n",
            "100% 9/9 [00:00<00:00, 155.15it/s]\n",
            "100% 2/2 [00:00<00:00, 284.62it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.002 | Train Acc: 29.77%\n",
            "\t Val. Loss: 1.925 |  Val. Acc: 30.34%\n",
            "100% 9/9 [00:00<00:00, 153.51it/s]\n",
            "100% 2/2 [00:00<00:00, 294.28it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.886 | Train Acc: 38.67%\n",
            "\t Val. Loss: 1.713 |  Val. Acc: 47.19%\n",
            "100% 9/9 [00:00<00:00, 156.81it/s]\n",
            "100% 2/2 [00:00<00:00, 289.71it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.754 | Train Acc: 40.08%\n",
            "\t Val. Loss: 1.600 |  Val. Acc: 46.41%\n",
            "100% 9/9 [00:00<00:00, 154.71it/s]\n",
            "100% 2/2 [00:00<00:00, 283.28it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.606 | Train Acc: 45.46%\n",
            "\t Val. Loss: 1.556 |  Val. Acc: 44.49%\n",
            "100% 9/9 [00:00<00:00, 159.52it/s]\n",
            "100% 2/2 [00:00<00:00, 293.25it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.493 | Train Acc: 47.85%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 51.25%\n",
            "100% 9/9 [00:00<00:00, 158.65it/s]\n",
            "100% 2/2 [00:00<00:00, 295.55it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.371 | Train Acc: 55.74%\n",
            "\t Val. Loss: 1.322 |  Val. Acc: 53.80%\n",
            "100% 9/9 [00:00<00:00, 157.16it/s]\n",
            "100% 2/2 [00:00<00:00, 288.95it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.288 | Train Acc: 58.76%\n",
            "\t Val. Loss: 1.217 |  Val. Acc: 60.98%\n",
            "100% 9/9 [00:00<00:00, 137.71it/s]\n",
            "100% 2/2 [00:00<00:00, 224.81it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.145 | Train Acc: 66.87%\n",
            "\t Val. Loss: 1.209 |  Val. Acc: 59.63%\n",
            "100% 9/9 [00:00<00:00, 109.77it/s]\n",
            "100% 2/2 [00:00<00:00, 256.45it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.001 | Train Acc: 68.63%\n",
            "\t Val. Loss: 1.034 |  Val. Acc: 71.85%\n",
            "100% 9/9 [00:00<00:00, 132.94it/s]\n",
            "100% 2/2 [00:00<00:00, 247.13it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.930 | Train Acc: 70.54%\n",
            "\t Val. Loss: 0.952 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 130.04it/s]\n",
            "100% 2/2 [00:00<00:00, 184.30it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.870 | Train Acc: 73.64%\n",
            "\t Val. Loss: 1.012 |  Val. Acc: 62.12%\n",
            "100% 9/9 [00:00<00:00, 113.03it/s]\n",
            "100% 2/2 [00:00<00:00, 217.52it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.806 | Train Acc: 76.77%\n",
            "\t Val. Loss: 0.897 |  Val. Acc: 73.78%\n",
            "100% 9/9 [00:00<00:00, 114.86it/s]\n",
            "100% 2/2 [00:00<00:00, 213.07it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.737 | Train Acc: 79.50%\n",
            "\t Val. Loss: 0.923 |  Val. Acc: 70.29%\n",
            "100% 9/9 [00:00<00:00, 128.09it/s]\n",
            "100% 2/2 [00:00<00:00, 286.25it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.666 | Train Acc: 79.43%\n",
            "\t Val. Loss: 0.815 |  Val. Acc: 73.99%\n",
            "100% 9/9 [00:00<00:00, 142.95it/s]\n",
            "100% 2/2 [00:00<00:00, 188.13it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.651 | Train Acc: 81.02%\n",
            "\t Val. Loss: 0.808 |  Val. Acc: 71.85%\n",
            "100% 9/9 [00:00<00:00, 126.76it/s]\n",
            "100% 2/2 [00:00<00:00, 268.07it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.595 | Train Acc: 81.21%\n",
            "\t Val. Loss: 0.902 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 158.43it/s]\n",
            "100% 2/2 [00:00<00:00, 300.80it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.578 | Train Acc: 80.54%\n",
            "\t Val. Loss: 0.808 |  Val. Acc: 68.94%\n",
            "100% 9/9 [00:00<00:00, 160.97it/s]\n",
            "100% 2/2 [00:00<00:00, 292.24it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.539 | Train Acc: 83.28%\n",
            "\t Val. Loss: 0.815 |  Val. Acc: 68.52%\n",
            "100% 9/9 [00:00<00:00, 157.83it/s]\n",
            "100% 2/2 [00:00<00:00, 300.05it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.540 | Train Acc: 84.23%\n",
            "\t Val. Loss: 0.819 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 153.73it/s]\n",
            "100% 2/2 [00:00<00:00, 282.69it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.547 | Train Acc: 82.62%\n",
            "\t Val. Loss: 0.750 |  Val. Acc: 73.99%\n",
            "100% 9/9 [00:00<00:00, 136.88it/s]\n",
            "100% 2/2 [00:00<00:00, 199.75it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.480 | Train Acc: 84.92%\n",
            "\t Val. Loss: 0.830 |  Val. Acc: 69.30%\n",
            "100% 9/9 [00:00<00:00, 142.45it/s]\n",
            "100% 2/2 [00:00<00:00, 271.26it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.478 | Train Acc: 85.36%\n",
            "\t Val. Loss: 0.738 |  Val. Acc: 73.78%\n",
            "100% 9/9 [00:00<00:00, 151.75it/s]\n",
            "100% 2/2 [00:00<00:00, 292.52it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.448 | Train Acc: 86.64%\n",
            "\t Val. Loss: 0.814 |  Val. Acc: 72.99%\n",
            "100% 9/9 [00:00<00:00, 158.84it/s]\n",
            "100% 2/2 [00:00<00:00, 295.76it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.409 | Train Acc: 87.51%\n",
            "\t Val. Loss: 0.776 |  Val. Acc: 75.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hQXMDrqis75",
        "colab_type": "text"
      },
      "source": [
        "### RNNMaxpoolClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7QpzdKfir48",
        "colab_type": "code",
        "outputId": "1ea7eefd-4d17-4f45-9e92-a63810cd2780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m RNNMaxpoolClassifier  --freeze-embeddings 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNMaxpoolClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 633,739 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] RNNMaxpoolClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 45.96it/s]\n",
            "100% 2/2 [00:00<00:00, 128.16it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.359 | Train Acc: 14.19%\n",
            "\t Val. Loss: 2.261 |  Val. Acc: 21.75%\n",
            "100% 9/9 [00:00<00:00, 59.39it/s]\n",
            "100% 2/2 [00:00<00:00, 128.92it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.261 | Train Acc: 19.05%\n",
            "\t Val. Loss: 2.185 |  Val. Acc: 26.44%\n",
            "100% 9/9 [00:00<00:00, 61.31it/s]\n",
            "100% 2/2 [00:00<00:00, 111.88it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.183 | Train Acc: 22.81%\n",
            "\t Val. Loss: 2.153 |  Val. Acc: 18.12%\n",
            "100% 9/9 [00:00<00:00, 60.23it/s]\n",
            "100% 2/2 [00:00<00:00, 132.06it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.138 | Train Acc: 24.63%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 24.51%\n",
            "100% 9/9 [00:00<00:00, 59.33it/s]\n",
            "100% 2/2 [00:00<00:00, 129.69it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.071 | Train Acc: 36.28%\n",
            "\t Val. Loss: 2.018 |  Val. Acc: 33.04%\n",
            "100% 9/9 [00:00<00:00, 56.95it/s]\n",
            "100% 2/2 [00:00<00:00, 130.43it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.978 | Train Acc: 38.02%\n",
            "\t Val. Loss: 1.898 |  Val. Acc: 33.61%\n",
            "100% 9/9 [00:00<00:00, 60.96it/s]\n",
            "100% 2/2 [00:00<00:00, 133.89it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.852 | Train Acc: 42.64%\n",
            "\t Val. Loss: 1.771 |  Val. Acc: 39.23%\n",
            "100% 9/9 [00:00<00:00, 62.05it/s]\n",
            "100% 2/2 [00:00<00:00, 137.62it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.704 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.613 |  Val. Acc: 44.64%\n",
            "100% 9/9 [00:00<00:00, 61.60it/s]\n",
            "100% 2/2 [00:00<00:00, 135.85it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.522 | Train Acc: 49.88%\n",
            "\t Val. Loss: 1.482 |  Val. Acc: 50.89%\n",
            "100% 9/9 [00:00<00:00, 60.74it/s]\n",
            "100% 2/2 [00:00<00:00, 136.99it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.362 | Train Acc: 56.00%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 55.93%\n",
            "100% 9/9 [00:00<00:00, 58.66it/s]\n",
            "100% 2/2 [00:00<00:00, 131.28it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.194 | Train Acc: 62.49%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 58.07%\n",
            "100% 9/9 [00:00<00:00, 60.97it/s]\n",
            "100% 2/2 [00:00<00:00, 131.78it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.994 | Train Acc: 69.76%\n",
            "\t Val. Loss: 1.124 |  Val. Acc: 56.93%\n",
            "100% 9/9 [00:00<00:00, 61.20it/s]\n",
            "100% 2/2 [00:00<00:00, 136.04it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.835 | Train Acc: 76.70%\n",
            "\t Val. Loss: 1.074 |  Val. Acc: 63.11%\n",
            "100% 9/9 [00:00<00:00, 61.56it/s]\n",
            "100% 2/2 [00:00<00:00, 130.80it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.698 | Train Acc: 81.06%\n",
            "\t Val. Loss: 0.943 |  Val. Acc: 68.16%\n",
            "100% 9/9 [00:00<00:00, 60.74it/s]\n",
            "100% 2/2 [00:00<00:00, 128.97it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.580 | Train Acc: 83.58%\n",
            "\t Val. Loss: 0.970 |  Val. Acc: 63.68%\n",
            "100% 9/9 [00:00<00:00, 62.34it/s]\n",
            "100% 2/2 [00:00<00:00, 135.35it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.547 | Train Acc: 84.62%\n",
            "\t Val. Loss: 1.049 |  Val. Acc: 65.24%\n",
            "100% 9/9 [00:00<00:00, 62.47it/s]\n",
            "100% 2/2 [00:00<00:00, 137.99it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.550 | Train Acc: 83.67%\n",
            "\t Val. Loss: 0.862 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 56.97it/s]\n",
            "100% 2/2 [00:00<00:00, 138.13it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.446 | Train Acc: 88.15%\n",
            "\t Val. Loss: 0.884 |  Val. Acc: 67.95%\n",
            "100% 9/9 [00:00<00:00, 61.13it/s]\n",
            "100% 2/2 [00:00<00:00, 127.91it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.432 | Train Acc: 86.64%\n",
            "\t Val. Loss: 0.787 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 66.66it/s]\n",
            "100% 2/2 [00:00<00:00, 141.37it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train Acc: 88.96%\n",
            "\t Val. Loss: 0.798 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 65.58it/s]\n",
            "100% 2/2 [00:00<00:00, 136.74it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.347 | Train Acc: 91.45%\n",
            "\t Val. Loss: 0.813 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 66.60it/s]\n",
            "100% 2/2 [00:00<00:00, 147.67it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.284 | Train Acc: 95.25%\n",
            "\t Val. Loss: 0.791 |  Val. Acc: 75.13%\n",
            "100% 9/9 [00:00<00:00, 65.68it/s]\n",
            "100% 2/2 [00:00<00:00, 150.11it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train Acc: 92.60%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 74.56%\n",
            "100% 9/9 [00:00<00:00, 62.09it/s]\n",
            "100% 2/2 [00:00<00:00, 144.34it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.291 | Train Acc: 92.21%\n",
            "\t Val. Loss: 0.823 |  Val. Acc: 72.78%\n",
            "100% 9/9 [00:00<00:00, 67.12it/s]\n",
            "100% 2/2 [00:00<00:00, 147.93it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train Acc: 94.71%\n",
            "\t Val. Loss: 0.803 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 65.78it/s]\n",
            "100% 2/2 [00:00<00:00, 151.76it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train Acc: 93.80%\n",
            "\t Val. Loss: 0.767 |  Val. Acc: 73.78%\n",
            "100% 9/9 [00:00<00:00, 65.63it/s]\n",
            "100% 2/2 [00:00<00:00, 148.93it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train Acc: 94.82%\n",
            "\t Val. Loss: 0.855 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 68.89it/s]\n",
            "100% 2/2 [00:00<00:00, 145.39it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train Acc: 94.79%\n",
            "\t Val. Loss: 0.769 |  Val. Acc: 74.13%\n",
            "100% 9/9 [00:00<00:00, 67.76it/s]\n",
            "100% 2/2 [00:00<00:00, 146.70it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train Acc: 95.42%\n",
            "\t Val. Loss: 0.818 |  Val. Acc: 75.13%\n",
            "100% 9/9 [00:00<00:00, 67.80it/s]\n",
            "100% 2/2 [00:00<00:00, 147.92it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train Acc: 96.14%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 77.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPuuJNZ3YefO",
        "colab_type": "text"
      },
      "source": [
        "### RNNFieldClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWCPNwHPYhN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1251fc2-c1a8-4f20-fdcb-36b55cdf52ad"
      },
      "source": [
        "!python train.py -n 50 -m RNNFieldClassifer"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:284 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNFieldClassifer', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:285 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:287 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:294 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:70 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:148 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:154 - initialize_new_model() ] Model Initialized with 494,347 trainiable parameters\n",
            "[DEBUG | train.py:166 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:319 -             <module>() ] RNNFieldClassifer(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (tag_embedding): Embedding(5, 50, padding_idx=1)\n",
            "  (rnn): LSTM(350, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 93.75it/s]\n",
            "100% 2/2 [00:00<00:00, 293.72it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.356 | Train Acc: 13.32%\n",
            "\t Val. Loss: 2.215 |  Val. Acc: 17.63%\n",
            "100% 9/9 [00:00<00:00, 131.24it/s]\n",
            "100% 2/2 [00:00<00:00, 293.23it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.259 | Train Acc: 16.88%\n",
            "\t Val. Loss: 2.237 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 125.18it/s]\n",
            "100% 2/2 [00:00<00:00, 305.21it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.235 | Train Acc: 17.45%\n",
            "\t Val. Loss: 2.271 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 132.98it/s]\n",
            "100% 2/2 [00:00<00:00, 290.38it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.245 | Train Acc: 17.40%\n",
            "\t Val. Loss: 2.184 |  Val. Acc: 15.56%\n",
            "100% 9/9 [00:00<00:00, 137.43it/s]\n",
            "100% 2/2 [00:00<00:00, 328.99it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.199 | Train Acc: 19.96%\n",
            "\t Val. Loss: 2.150 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 150.12it/s]\n",
            "100% 2/2 [00:00<00:00, 336.14it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.162 | Train Acc: 19.94%\n",
            "\t Val. Loss: 2.104 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 131.55it/s]\n",
            "100% 2/2 [00:00<00:00, 250.30it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.146 | Train Acc: 22.33%\n",
            "\t Val. Loss: 2.036 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 137.14it/s]\n",
            "100% 2/2 [00:00<00:00, 275.16it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.064 | Train Acc: 21.81%\n",
            "\t Val. Loss: 1.884 |  Val. Acc: 33.97%\n",
            "100% 9/9 [00:00<00:00, 131.82it/s]\n",
            "100% 2/2 [00:00<00:00, 310.51it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.993 | Train Acc: 29.12%\n",
            "\t Val. Loss: 1.801 |  Val. Acc: 33.76%\n",
            "100% 9/9 [00:00<00:00, 137.74it/s]\n",
            "100% 2/2 [00:00<00:00, 308.51it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.911 | Train Acc: 29.16%\n",
            "\t Val. Loss: 1.794 |  Val. Acc: 33.97%\n",
            "100% 9/9 [00:00<00:00, 137.59it/s]\n",
            "100% 2/2 [00:00<00:00, 306.75it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.857 | Train Acc: 29.70%\n",
            "\t Val. Loss: 1.726 |  Val. Acc: 35.33%\n",
            "100% 9/9 [00:00<00:00, 137.85it/s]\n",
            "100% 2/2 [00:00<00:00, 309.08it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.816 | Train Acc: 31.46%\n",
            "\t Val. Loss: 1.686 |  Val. Acc: 36.89%\n",
            "100% 9/9 [00:00<00:00, 147.27it/s]\n",
            "100% 2/2 [00:00<00:00, 338.50it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.787 | Train Acc: 33.65%\n",
            "\t Val. Loss: 1.599 |  Val. Acc: 48.69%\n",
            "100% 9/9 [00:00<00:00, 153.57it/s]\n",
            "100% 2/2 [00:00<00:00, 343.96it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.656 | Train Acc: 41.92%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 44.85%\n",
            "100% 9/9 [00:00<00:00, 153.11it/s]\n",
            "100% 2/2 [00:00<00:00, 336.03it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.619 | Train Acc: 44.85%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 47.34%\n",
            "100% 9/9 [00:00<00:00, 155.10it/s]\n",
            "100% 2/2 [00:00<00:00, 248.85it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.542 | Train Acc: 45.89%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 45.42%\n",
            "100% 9/9 [00:00<00:00, 111.25it/s]\n",
            "100% 2/2 [00:00<00:00, 248.19it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.474 | Train Acc: 46.93%\n",
            "\t Val. Loss: 1.426 |  Val. Acc: 49.11%\n",
            "100% 9/9 [00:00<00:00, 143.20it/s]\n",
            "100% 2/2 [00:00<00:00, 293.93it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.378 | Train Acc: 52.66%\n",
            "\t Val. Loss: 1.315 |  Val. Acc: 53.17%\n",
            "100% 9/9 [00:00<00:00, 150.94it/s]\n",
            "100% 2/2 [00:00<00:00, 288.13it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.349 | Train Acc: 50.93%\n",
            "\t Val. Loss: 1.287 |  Val. Acc: 56.65%\n",
            "100% 9/9 [00:00<00:00, 133.77it/s]\n",
            "100% 2/2 [00:00<00:00, 312.04it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.328 | Train Acc: 53.42%\n",
            "\t Val. Loss: 1.120 |  Val. Acc: 60.56%\n",
            "100% 9/9 [00:00<00:00, 132.84it/s]\n",
            "100% 2/2 [00:00<00:00, 306.74it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.230 | Train Acc: 55.83%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 52.03%\n",
            "100% 9/9 [00:00<00:00, 128.56it/s]\n",
            "100% 2/2 [00:00<00:00, 306.46it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.162 | Train Acc: 59.32%\n",
            "\t Val. Loss: 1.132 |  Val. Acc: 56.29%\n",
            "100% 9/9 [00:00<00:00, 137.81it/s]\n",
            "100% 2/2 [00:00<00:00, 311.98it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.141 | Train Acc: 59.26%\n",
            "\t Val. Loss: 1.003 |  Val. Acc: 61.70%\n",
            "100% 9/9 [00:00<00:00, 127.66it/s]\n",
            "100% 2/2 [00:00<00:00, 307.97it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.047 | Train Acc: 62.70%\n",
            "\t Val. Loss: 1.087 |  Val. Acc: 59.99%\n",
            "100% 9/9 [00:00<00:00, 138.71it/s]\n",
            "100% 2/2 [00:00<00:00, 312.86it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.067 | Train Acc: 61.40%\n",
            "\t Val. Loss: 1.195 |  Val. Acc: 59.78%\n",
            "100% 9/9 [00:00<00:00, 141.29it/s]\n",
            "100% 2/2 [00:00<00:00, 310.93it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.993 | Train Acc: 65.79%\n",
            "\t Val. Loss: 1.034 |  Val. Acc: 59.78%\n",
            "100% 9/9 [00:00<00:00, 140.23it/s]\n",
            "100% 2/2 [00:00<00:00, 309.10it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.926 | Train Acc: 67.74%\n",
            "\t Val. Loss: 1.038 |  Val. Acc: 62.33%\n",
            "100% 9/9 [00:00<00:00, 135.47it/s]\n",
            "100% 2/2 [00:00<00:00, 310.77it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.917 | Train Acc: 68.56%\n",
            "\t Val. Loss: 0.996 |  Val. Acc: 64.46%\n",
            "100% 9/9 [00:00<00:00, 143.76it/s]\n",
            "100% 2/2 [00:00<00:00, 316.34it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.849 | Train Acc: 70.34%\n",
            "\t Val. Loss: 1.043 |  Val. Acc: 63.47%\n",
            "100% 9/9 [00:00<00:00, 121.73it/s]\n",
            "100% 2/2 [00:00<00:00, 303.22it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.751 | Train Acc: 74.77%\n",
            "\t Val. Loss: 0.953 |  Val. Acc: 69.30%\n",
            "100% 9/9 [00:00<00:00, 136.84it/s]\n",
            "100% 2/2 [00:00<00:00, 306.46it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.701 | Train Acc: 76.59%\n",
            "\t Val. Loss: 0.760 |  Val. Acc: 67.95%\n",
            "100% 9/9 [00:00<00:00, 137.36it/s]\n",
            "100% 2/2 [00:00<00:00, 311.81it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.697 | Train Acc: 74.77%\n",
            "\t Val. Loss: 0.791 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 125.51it/s]\n",
            "100% 2/2 [00:00<00:00, 311.60it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.679 | Train Acc: 77.16%\n",
            "\t Val. Loss: 0.739 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 138.39it/s]\n",
            "100% 2/2 [00:00<00:00, 312.09it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.615 | Train Acc: 77.91%\n",
            "\t Val. Loss: 0.849 |  Val. Acc: 70.29%\n",
            "100% 9/9 [00:00<00:00, 140.81it/s]\n",
            "100% 2/2 [00:00<00:00, 280.93it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.578 | Train Acc: 79.15%\n",
            "\t Val. Loss: 0.739 |  Val. Acc: 73.21%\n",
            "100% 9/9 [00:00<00:00, 139.49it/s]\n",
            "100% 2/2 [00:00<00:00, 311.27it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.510 | Train Acc: 82.97%\n",
            "\t Val. Loss: 0.793 |  Val. Acc: 76.12%\n",
            "100% 9/9 [00:00<00:00, 133.85it/s]\n",
            "100% 2/2 [00:00<00:00, 301.39it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.497 | Train Acc: 83.38%\n",
            "\t Val. Loss: 0.604 |  Val. Acc: 77.68%\n",
            "100% 9/9 [00:00<00:00, 137.07it/s]\n",
            "100% 2/2 [00:00<00:00, 311.47it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.484 | Train Acc: 81.21%\n",
            "\t Val. Loss: 0.728 |  Val. Acc: 76.33%\n",
            "100% 9/9 [00:00<00:00, 134.31it/s]\n",
            "100% 2/2 [00:00<00:00, 312.97it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.480 | Train Acc: 84.51%\n",
            "\t Val. Loss: 0.709 |  Val. Acc: 75.34%\n",
            "100% 9/9 [00:00<00:00, 149.98it/s]\n",
            "100% 2/2 [00:00<00:00, 336.27it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.436 | Train Acc: 83.88%\n",
            "\t Val. Loss: 0.765 |  Val. Acc: 73.21%\n",
            "100% 9/9 [00:00<00:00, 157.04it/s]\n",
            "100% 2/2 [00:00<00:00, 344.94it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.454 | Train Acc: 84.64%\n",
            "\t Val. Loss: 0.644 |  Val. Acc: 77.68%\n",
            "100% 9/9 [00:00<00:00, 140.56it/s]\n",
            "100% 2/2 [00:00<00:00, 333.80it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.427 | Train Acc: 85.44%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 78.82%\n",
            "100% 9/9 [00:00<00:00, 124.66it/s]\n",
            "100% 2/2 [00:00<00:00, 308.05it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.443 | Train Acc: 85.51%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 78.46%\n",
            "100% 9/9 [00:00<00:00, 139.32it/s]\n",
            "100% 2/2 [00:00<00:00, 307.17it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.434 | Train Acc: 85.16%\n",
            "\t Val. Loss: 0.573 |  Val. Acc: 78.25%\n",
            "100% 9/9 [00:00<00:00, 136.41it/s]\n",
            "100% 2/2 [00:00<00:00, 294.34it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.380 | Train Acc: 86.25%\n",
            "\t Val. Loss: 0.853 |  Val. Acc: 77.11%\n",
            "100% 9/9 [00:00<00:00, 132.81it/s]\n",
            "100% 2/2 [00:00<00:00, 309.09it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.395 | Train Acc: 86.70%\n",
            "\t Val. Loss: 0.770 |  Val. Acc: 74.20%\n",
            "100% 9/9 [00:00<00:00, 136.03it/s]\n",
            "100% 2/2 [00:00<00:00, 295.26it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.402 | Train Acc: 86.18%\n",
            "\t Val. Loss: 0.582 |  Val. Acc: 82.16%\n",
            "100% 9/9 [00:00<00:00, 137.74it/s]\n",
            "100% 2/2 [00:00<00:00, 313.22it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.338 | Train Acc: 90.39%\n",
            "\t Val. Loss: 0.637 |  Val. Acc: 78.46%\n",
            "100% 9/9 [00:00<00:00, 134.84it/s]\n",
            "100% 2/2 [00:00<00:00, 312.65it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.361 | Train Acc: 88.33%\n",
            "\t Val. Loss: 0.621 |  Val. Acc: 81.17%\n",
            "100% 9/9 [00:00<00:00, 132.98it/s]\n",
            "100% 2/2 [00:00<00:00, 303.00it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.329 | Train Acc: 89.41%\n",
            "\t Val. Loss: 0.663 |  Val. Acc: 83.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szDjOF303wC",
        "colab_type": "text"
      },
      "source": [
        "## CNNClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVWP8U5kpmj",
        "colab_type": "text"
      },
      "source": [
        "### CNN2D Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2otPOBv06rZ",
        "colab_type": "code",
        "outputId": "b64bf9f2-8503-495a-8887-dce186ec5c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN2dClassifier --freeze-embeddings 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='CNN2dClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 117,515 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] CNN2dClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 64, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 64, kernel_size=(2, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 64, kernel_size=(3, 300), stride=(1, 1))\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 106.04it/s]\n",
            "100% 2/2 [00:00<00:00, 292.33it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.512 | Train Acc: 16.73%\n",
            "\t Val. Loss: 1.978 |  Val. Acc: 28.93%\n",
            "100% 9/9 [00:00<00:00, 125.24it/s]\n",
            "100% 2/2 [00:00<00:00, 243.90it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.151 | Train Acc: 23.50%\n",
            "\t Val. Loss: 1.839 |  Val. Acc: 33.97%\n",
            "100% 9/9 [00:00<00:00, 123.03it/s]\n",
            "100% 2/2 [00:00<00:00, 224.00it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.016 | Train Acc: 29.23%\n",
            "\t Val. Loss: 1.759 |  Val. Acc: 37.25%\n",
            "100% 9/9 [00:00<00:00, 132.64it/s]\n",
            "100% 2/2 [00:00<00:00, 298.23it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.862 | Train Acc: 32.94%\n",
            "\t Val. Loss: 1.637 |  Val. Acc: 47.91%\n",
            "100% 9/9 [00:00<00:00, 133.21it/s]\n",
            "100% 2/2 [00:00<00:00, 296.02it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.781 | Train Acc: 35.32%\n",
            "\t Val. Loss: 1.645 |  Val. Acc: 41.15%\n",
            "100% 9/9 [00:00<00:00, 132.98it/s]\n",
            "100% 2/2 [00:00<00:00, 306.70it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.706 | Train Acc: 40.18%\n",
            "\t Val. Loss: 1.578 |  Val. Acc: 44.64%\n",
            "100% 9/9 [00:00<00:00, 128.00it/s]\n",
            "100% 2/2 [00:00<00:00, 309.45it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.650 | Train Acc: 38.86%\n",
            "\t Val. Loss: 1.499 |  Val. Acc: 50.68%\n",
            "100% 9/9 [00:00<00:00, 129.25it/s]\n",
            "100% 2/2 [00:00<00:00, 297.82it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.540 | Train Acc: 45.74%\n",
            "\t Val. Loss: 1.429 |  Val. Acc: 53.38%\n",
            "100% 9/9 [00:00<00:00, 129.67it/s]\n",
            "100% 2/2 [00:00<00:00, 291.39it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.456 | Train Acc: 48.04%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 51.46%\n",
            "100% 9/9 [00:00<00:00, 126.57it/s]\n",
            "100% 2/2 [00:00<00:00, 290.65it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.366 | Train Acc: 53.07%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 53.59%\n",
            "100% 9/9 [00:00<00:00, 123.97it/s]\n",
            "100% 2/2 [00:00<00:00, 267.18it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.285 | Train Acc: 55.05%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 56.50%\n",
            "100% 9/9 [00:00<00:00, 130.34it/s]\n",
            "100% 2/2 [00:00<00:00, 305.90it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.262 | Train Acc: 56.50%\n",
            "\t Val. Loss: 1.249 |  Val. Acc: 59.42%\n",
            "100% 9/9 [00:00<00:00, 129.84it/s]\n",
            "100% 2/2 [00:00<00:00, 247.76it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.161 | Train Acc: 62.55%\n",
            "\t Val. Loss: 1.173 |  Val. Acc: 59.84%\n",
            "100% 9/9 [00:00<00:00, 126.39it/s]\n",
            "100% 2/2 [00:00<00:00, 158.49it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.145 | Train Acc: 62.80%\n",
            "\t Val. Loss: 1.146 |  Val. Acc: 58.49%\n",
            "100% 9/9 [00:00<00:00, 128.71it/s]\n",
            "100% 2/2 [00:00<00:00, 306.50it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.051 | Train Acc: 66.87%\n",
            "\t Val. Loss: 1.065 |  Val. Acc: 63.32%\n",
            "100% 9/9 [00:00<00:00, 129.01it/s]\n",
            "100% 2/2 [00:00<00:00, 295.20it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.018 | Train Acc: 68.17%\n",
            "\t Val. Loss: 1.026 |  Val. Acc: 61.40%\n",
            "100% 9/9 [00:00<00:00, 131.21it/s]\n",
            "100% 2/2 [00:00<00:00, 281.96it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.940 | Train Acc: 69.26%\n",
            "\t Val. Loss: 0.969 |  Val. Acc: 66.45%\n",
            "100% 9/9 [00:00<00:00, 131.54it/s]\n",
            "100% 2/2 [00:00<00:00, 302.15it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.918 | Train Acc: 69.84%\n",
            "\t Val. Loss: 0.920 |  Val. Acc: 72.07%\n",
            "100% 9/9 [00:00<00:00, 128.10it/s]\n",
            "100% 2/2 [00:00<00:00, 287.62it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.865 | Train Acc: 70.75%\n",
            "\t Val. Loss: 0.883 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 128.16it/s]\n",
            "100% 2/2 [00:00<00:00, 307.01it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.788 | Train Acc: 76.46%\n",
            "\t Val. Loss: 0.854 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 126.15it/s]\n",
            "100% 2/2 [00:00<00:00, 288.38it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.776 | Train Acc: 74.51%\n",
            "\t Val. Loss: 0.823 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 126.39it/s]\n",
            "100% 2/2 [00:00<00:00, 288.35it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.742 | Train Acc: 74.35%\n",
            "\t Val. Loss: 0.777 |  Val. Acc: 76.54%\n",
            "100% 9/9 [00:00<00:00, 129.52it/s]\n",
            "100% 2/2 [00:00<00:00, 298.12it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.692 | Train Acc: 79.28%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 77.32%\n",
            "100% 9/9 [00:00<00:00, 126.88it/s]\n",
            "100% 2/2 [00:00<00:00, 161.02it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.700 | Train Acc: 79.24%\n",
            "\t Val. Loss: 0.738 |  Val. Acc: 77.32%\n",
            "100% 9/9 [00:00<00:00, 124.94it/s]\n",
            "100% 2/2 [00:00<00:00, 268.12it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.620 | Train Acc: 81.78%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 79.46%\n",
            "100% 9/9 [00:00<00:00, 125.42it/s]\n",
            "100% 2/2 [00:00<00:00, 272.40it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.609 | Train Acc: 82.38%\n",
            "\t Val. Loss: 0.679 |  Val. Acc: 78.89%\n",
            "100% 9/9 [00:00<00:00, 128.62it/s]\n",
            "100% 2/2 [00:00<00:00, 219.14it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.552 | Train Acc: 83.81%\n",
            "\t Val. Loss: 0.646 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 126.02it/s]\n",
            "100% 2/2 [00:00<00:00, 274.96it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.566 | Train Acc: 82.54%\n",
            "\t Val. Loss: 0.625 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 130.33it/s]\n",
            "100% 2/2 [00:00<00:00, 308.82it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.538 | Train Acc: 84.79%\n",
            "\t Val. Loss: 0.624 |  Val. Acc: 79.46%\n",
            "100% 9/9 [00:00<00:00, 130.97it/s]\n",
            "100% 2/2 [00:00<00:00, 299.64it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.546 | Train Acc: 83.99%\n",
            "\t Val. Loss: 0.607 |  Val. Acc: 80.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6JUYoTTk19_",
        "colab_type": "text"
      },
      "source": [
        "### CNN1DClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukseycMBk4NK",
        "colab_type": "code",
        "outputId": "2cdc7d99-93fb-440d-fc22-ebd75138558e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN1dClassifier --freeze-embeddings 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=256, l2_regularization=0.001, learning_rate=0.001, model='CNN1dClassifier', model_location=None, n_layers=2, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 561,911 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] CNN1dClassifier(\n",
            "  (embedding): Embedding(661, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 118.72it/s]\n",
            "100% 2/2 [00:00<00:00, 374.46it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.518 | Train Acc: 20.98%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 18.12%\n",
            "100% 9/9 [00:00<00:00, 137.71it/s]\n",
            "100% 2/2 [00:00<00:00, 413.86it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.073 | Train Acc: 26.60%\n",
            "\t Val. Loss: 1.735 |  Val. Acc: 22.38%\n",
            "100% 9/9 [00:00<00:00, 141.31it/s]\n",
            "100% 2/2 [00:00<00:00, 415.11it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.816 | Train Acc: 36.30%\n",
            "\t Val. Loss: 1.721 |  Val. Acc: 26.86%\n",
            "100% 9/9 [00:00<00:00, 149.37it/s]\n",
            "100% 2/2 [00:00<00:00, 427.27it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.717 | Train Acc: 39.73%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 39.80%\n",
            "100% 9/9 [00:00<00:00, 154.58it/s]\n",
            "100% 2/2 [00:00<00:00, 433.16it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.459 | Train Acc: 49.82%\n",
            "\t Val. Loss: 1.466 |  Val. Acc: 47.19%\n",
            "100% 9/9 [00:00<00:00, 149.93it/s]\n",
            "100% 2/2 [00:00<00:00, 352.97it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.310 | Train Acc: 55.05%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 48.75%\n",
            "100% 9/9 [00:00<00:00, 161.52it/s]\n",
            "100% 2/2 [00:00<00:00, 266.94it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.221 | Train Acc: 56.37%\n",
            "\t Val. Loss: 1.184 |  Val. Acc: 61.55%\n",
            "100% 9/9 [00:00<00:00, 159.76it/s]\n",
            "100% 2/2 [00:00<00:00, 444.90it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.144 | Train Acc: 62.06%\n",
            "\t Val. Loss: 1.109 |  Val. Acc: 63.32%\n",
            "100% 9/9 [00:00<00:00, 165.00it/s]\n",
            "100% 2/2 [00:00<00:00, 455.73it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.010 | Train Acc: 67.93%\n",
            "\t Val. Loss: 1.083 |  Val. Acc: 61.55%\n",
            "100% 9/9 [00:00<00:00, 160.78it/s]\n",
            "100% 2/2 [00:00<00:00, 444.29it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.942 | Train Acc: 68.78%\n",
            "\t Val. Loss: 0.946 |  Val. Acc: 71.07%\n",
            "100% 9/9 [00:00<00:00, 163.31it/s]\n",
            "100% 2/2 [00:00<00:00, 450.30it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.866 | Train Acc: 71.84%\n",
            "\t Val. Loss: 0.867 |  Val. Acc: 79.60%\n",
            "100% 9/9 [00:00<00:00, 164.43it/s]\n",
            "100% 2/2 [00:00<00:00, 439.22it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.727 | Train Acc: 77.63%\n",
            "\t Val. Loss: 0.844 |  Val. Acc: 73.99%\n",
            "100% 9/9 [00:00<00:00, 155.38it/s]\n",
            "100% 2/2 [00:00<00:00, 438.16it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.669 | Train Acc: 79.76%\n",
            "\t Val. Loss: 0.758 |  Val. Acc: 74.77%\n",
            "100% 9/9 [00:00<00:00, 158.33it/s]\n",
            "100% 2/2 [00:00<00:00, 432.78it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.585 | Train Acc: 84.19%\n",
            "\t Val. Loss: 0.704 |  Val. Acc: 80.17%\n",
            "100% 9/9 [00:00<00:00, 161.87it/s]\n",
            "100% 2/2 [00:00<00:00, 447.97it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.559 | Train Acc: 85.08%\n",
            "\t Val. Loss: 0.632 |  Val. Acc: 82.88%\n",
            "100% 9/9 [00:00<00:00, 162.41it/s]\n",
            "100% 2/2 [00:00<00:00, 327.00it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.494 | Train Acc: 87.24%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 82.88%\n",
            "100% 9/9 [00:00<00:00, 157.42it/s]\n",
            "100% 2/2 [00:00<00:00, 384.92it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.469 | Train Acc: 86.53%\n",
            "\t Val. Loss: 0.586 |  Val. Acc: 81.74%\n",
            "100% 9/9 [00:00<00:00, 161.84it/s]\n",
            "100% 2/2 [00:00<00:00, 406.92it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.405 | Train Acc: 89.24%\n",
            "\t Val. Loss: 0.532 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 162.57it/s]\n",
            "100% 2/2 [00:00<00:00, 444.69it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Acc: 88.33%\n",
            "\t Val. Loss: 0.514 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 155.00it/s]\n",
            "100% 2/2 [00:00<00:00, 441.74it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.354 | Train Acc: 89.95%\n",
            "\t Val. Loss: 0.485 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 154.75it/s]\n",
            "100% 2/2 [00:00<00:00, 421.22it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.334 | Train Acc: 89.91%\n",
            "\t Val. Loss: 0.486 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 165.33it/s]\n",
            "100% 2/2 [00:00<00:00, 315.82it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.312 | Train Acc: 91.22%\n",
            "\t Val. Loss: 0.468 |  Val. Acc: 86.00%\n",
            "100% 9/9 [00:00<00:00, 160.77it/s]\n",
            "100% 2/2 [00:00<00:00, 431.93it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train Acc: 92.21%\n",
            "\t Val. Loss: 0.430 |  Val. Acc: 89.27%\n",
            "100% 9/9 [00:00<00:00, 161.05it/s]\n",
            "100% 2/2 [00:00<00:00, 450.81it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.298 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.420 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 156.48it/s]\n",
            "100% 2/2 [00:00<00:00, 441.32it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.311 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.409 |  Val. Acc: 90.05%\n",
            "100% 9/9 [00:00<00:00, 163.79it/s]\n",
            "100% 2/2 [00:00<00:00, 451.24it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train Acc: 94.05%\n",
            "\t Val. Loss: 0.386 |  Val. Acc: 91.41%\n",
            "100% 9/9 [00:00<00:00, 161.52it/s]\n",
            "100% 2/2 [00:00<00:00, 445.44it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train Acc: 93.47%\n",
            "\t Val. Loss: 0.371 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 151.40it/s]\n",
            "100% 2/2 [00:00<00:00, 445.35it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train Acc: 94.16%\n",
            "\t Val. Loss: 0.383 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 164.58it/s]\n",
            "100% 2/2 [00:00<00:00, 449.24it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train Acc: 94.53%\n",
            "\t Val. Loss: 0.375 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 161.51it/s]\n",
            "100% 2/2 [00:00<00:00, 342.22it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train Acc: 94.73%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 90.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm70iQTraxZP",
        "colab_type": "text"
      },
      "source": [
        "# File Editor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBdj4zha63F",
        "colab_type": "text"
      },
      "source": [
        "## helperfunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn_oZuLhKqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb5428e1-9880-49fe-8002-df29c275be29"
      },
      "source": [
        "%%writefile helperfunctions.py\n",
        "\"\"\"\n",
        "Helper Functions containing training and evaluation methods\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from utility import categorical_accuracy\n",
        "from config.root import device\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, dataset_tag):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths = get_batch_data(batch, dataset_tag)\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def get_batch_data(batch, dataset_tag):\n",
        "\n",
        "    if dataset_tag == \"multi\":\n",
        "        (question, question_len), (key, key_len), (answer, answer_len) = (\n",
        "            batch.question,\n",
        "            batch.key,\n",
        "            batch.answer,\n",
        "        )\n",
        "\n",
        "        text = torch.cat((question, key, answer), dim=0)\n",
        "        text_lengths = question_len + key_len + answer_len\n",
        "    else:\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "    return text, text_lengths\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, dataset_tag):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "            text, text_lengths = get_batch_data(batch, dataset_tag)\n",
        "\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def train_tag_model(model, iterator, optimizer, criterion, tag_field):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths, tag = get_batch_data_and_tag(batch, tag_field)\n",
        "\n",
        "        predictions = model(text, text_lengths, tag).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def get_batch_data_and_tag(batch, tag_field):\n",
        "\n",
        "    (question, question_len), (key, key_len), (answer, answer_len) = (\n",
        "        batch.question,\n",
        "        batch.key,\n",
        "        batch.answer,\n",
        "    )\n",
        "\n",
        "    question_tag = torch.full_like(\n",
        "        question, tag_field.vocab.stoi[\"Q\"], dtype=torch.long, device=device\n",
        "    )\n",
        "    key_tag = torch.full_like(\n",
        "        key, tag_field.vocab.stoi[\"K\"], dtype=torch.long, device=device\n",
        "    )\n",
        "    answer_tag = torch.full_like(\n",
        "        answer, tag_field.vocab.stoi[\"A\"], dtype=torch.long, device=device\n",
        "    )\n",
        "\n",
        "    tag = torch.cat((question_tag, key_tag, answer_tag), dim=0)\n",
        "\n",
        "    text = torch.cat((question, key, answer), dim=0)\n",
        "\n",
        "    text_lengths = question_len + key_len + answer_len\n",
        "\n",
        "    return text, text_lengths, tag\n",
        "\n",
        "\n",
        "def evaluate_tag_model(model, iterator, criterion, tag_field):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "            text, text_lengths, tag = get_batch_data_and_tag(batch, tag_field)\n",
        "\n",
        "            predictions = model(text, text_lengths, tag).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting helperfunctions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t16HDjub6S2",
        "colab_type": "text"
      },
      "source": [
        "## RNNClassifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKaxVGQ_bKtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38139f79-1e73-4682-b9e1-7bde4edb4541"
      },
      "source": [
        "%%writefile model/RNNClassifiers.py\n",
        "\"\"\"\n",
        "Model Architectures of RNN based Classifiers\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from config.root import LOGGING_FORMAT, LOGGING_LEVEL\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "class RNNHiddenClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This classifier concatenates the last hidden \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        n_layers,\n",
        "        bidirectional,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        if self.bidirectional:\n",
        "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = self.dropout(\n",
        "                torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "            )\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "class RNNMaxpoolClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This classifier max pools the hidden states\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        n_layers,\n",
        "        bidirectional,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        hidden, _ = torch.max(hidden, dim=0)\n",
        "\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "class RNNFieldClassifer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        output_dim,\n",
        "        n_layers,\n",
        "        bidirectional,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "        tag_vocab,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.tag_embedding = nn.Embedding(\n",
        "            len(tag_vocab.vocab),\n",
        "            50,\n",
        "            padding_idx=tag_vocab.vocab.stoi[tag_vocab.pad_token],\n",
        "        )\n",
        "\n",
        "        self.tag_embedding.weight.requires_grad = False\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim + 50,\n",
        "            hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        if self.bidirectional:\n",
        "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths, tag):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        tag_embedded = self.dropout(self.tag_embedding(tag))\n",
        "\n",
        "        embed = torch.cat((embedded, tag_embedded), -1)\n",
        "\n",
        "        # packed_embedded = nn.utils.rnn.pack_padded_sequence(embed, text_lengths)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(embed)\n",
        "\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            hidden = self.dropout(\n",
        "                torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "            )\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1, :, :])\n",
        "\n",
        "        return self.fc(hidden)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model/RNNClassifiers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P2okybFbW2e",
        "colab_type": "code",
        "outputId": "1ea7eefd-4d17-4f45-9e92-a63810cd2780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m RNNMaxpoolClassifier  --freeze-embeddings 0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNMaxpoolClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 633,739 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] RNNMaxpoolClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 45.96it/s]\n",
            "100% 2/2 [00:00<00:00, 128.16it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.359 | Train Acc: 14.19%\n",
            "\t Val. Loss: 2.261 |  Val. Acc: 21.75%\n",
            "100% 9/9 [00:00<00:00, 59.39it/s]\n",
            "100% 2/2 [00:00<00:00, 128.92it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.261 | Train Acc: 19.05%\n",
            "\t Val. Loss: 2.185 |  Val. Acc: 26.44%\n",
            "100% 9/9 [00:00<00:00, 61.31it/s]\n",
            "100% 2/2 [00:00<00:00, 111.88it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.183 | Train Acc: 22.81%\n",
            "\t Val. Loss: 2.153 |  Val. Acc: 18.12%\n",
            "100% 9/9 [00:00<00:00, 60.23it/s]\n",
            "100% 2/2 [00:00<00:00, 132.06it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.138 | Train Acc: 24.63%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 24.51%\n",
            "100% 9/9 [00:00<00:00, 59.33it/s]\n",
            "100% 2/2 [00:00<00:00, 129.69it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.071 | Train Acc: 36.28%\n",
            "\t Val. Loss: 2.018 |  Val. Acc: 33.04%\n",
            "100% 9/9 [00:00<00:00, 56.95it/s]\n",
            "100% 2/2 [00:00<00:00, 130.43it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.978 | Train Acc: 38.02%\n",
            "\t Val. Loss: 1.898 |  Val. Acc: 33.61%\n",
            "100% 9/9 [00:00<00:00, 60.96it/s]\n",
            "100% 2/2 [00:00<00:00, 133.89it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.852 | Train Acc: 42.64%\n",
            "\t Val. Loss: 1.771 |  Val. Acc: 39.23%\n",
            "100% 9/9 [00:00<00:00, 62.05it/s]\n",
            "100% 2/2 [00:00<00:00, 137.62it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.704 | Train Acc: 44.63%\n",
            "\t Val. Loss: 1.613 |  Val. Acc: 44.64%\n",
            "100% 9/9 [00:00<00:00, 61.60it/s]\n",
            "100% 2/2 [00:00<00:00, 135.85it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.522 | Train Acc: 49.88%\n",
            "\t Val. Loss: 1.482 |  Val. Acc: 50.89%\n",
            "100% 9/9 [00:00<00:00, 60.74it/s]\n",
            "100% 2/2 [00:00<00:00, 136.99it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.362 | Train Acc: 56.00%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 55.93%\n",
            "100% 9/9 [00:00<00:00, 58.66it/s]\n",
            "100% 2/2 [00:00<00:00, 131.28it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.194 | Train Acc: 62.49%\n",
            "\t Val. Loss: 1.290 |  Val. Acc: 58.07%\n",
            "100% 9/9 [00:00<00:00, 60.97it/s]\n",
            "100% 2/2 [00:00<00:00, 131.78it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.994 | Train Acc: 69.76%\n",
            "\t Val. Loss: 1.124 |  Val. Acc: 56.93%\n",
            "100% 9/9 [00:00<00:00, 61.20it/s]\n",
            "100% 2/2 [00:00<00:00, 136.04it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.835 | Train Acc: 76.70%\n",
            "\t Val. Loss: 1.074 |  Val. Acc: 63.11%\n",
            "100% 9/9 [00:00<00:00, 61.56it/s]\n",
            "100% 2/2 [00:00<00:00, 130.80it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.698 | Train Acc: 81.06%\n",
            "\t Val. Loss: 0.943 |  Val. Acc: 68.16%\n",
            "100% 9/9 [00:00<00:00, 60.74it/s]\n",
            "100% 2/2 [00:00<00:00, 128.97it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.580 | Train Acc: 83.58%\n",
            "\t Val. Loss: 0.970 |  Val. Acc: 63.68%\n",
            "100% 9/9 [00:00<00:00, 62.34it/s]\n",
            "100% 2/2 [00:00<00:00, 135.35it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.547 | Train Acc: 84.62%\n",
            "\t Val. Loss: 1.049 |  Val. Acc: 65.24%\n",
            "100% 9/9 [00:00<00:00, 62.47it/s]\n",
            "100% 2/2 [00:00<00:00, 137.99it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.550 | Train Acc: 83.67%\n",
            "\t Val. Loss: 0.862 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 56.97it/s]\n",
            "100% 2/2 [00:00<00:00, 138.13it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.446 | Train Acc: 88.15%\n",
            "\t Val. Loss: 0.884 |  Val. Acc: 67.95%\n",
            "100% 9/9 [00:00<00:00, 61.13it/s]\n",
            "100% 2/2 [00:00<00:00, 127.91it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.432 | Train Acc: 86.64%\n",
            "\t Val. Loss: 0.787 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 66.66it/s]\n",
            "100% 2/2 [00:00<00:00, 141.37it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train Acc: 88.96%\n",
            "\t Val. Loss: 0.798 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 65.58it/s]\n",
            "100% 2/2 [00:00<00:00, 136.74it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.347 | Train Acc: 91.45%\n",
            "\t Val. Loss: 0.813 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 66.60it/s]\n",
            "100% 2/2 [00:00<00:00, 147.67it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.284 | Train Acc: 95.25%\n",
            "\t Val. Loss: 0.791 |  Val. Acc: 75.13%\n",
            "100% 9/9 [00:00<00:00, 65.68it/s]\n",
            "100% 2/2 [00:00<00:00, 150.11it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train Acc: 92.60%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 74.56%\n",
            "100% 9/9 [00:00<00:00, 62.09it/s]\n",
            "100% 2/2 [00:00<00:00, 144.34it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.291 | Train Acc: 92.21%\n",
            "\t Val. Loss: 0.823 |  Val. Acc: 72.78%\n",
            "100% 9/9 [00:00<00:00, 67.12it/s]\n",
            "100% 2/2 [00:00<00:00, 147.93it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train Acc: 94.71%\n",
            "\t Val. Loss: 0.803 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 65.78it/s]\n",
            "100% 2/2 [00:00<00:00, 151.76it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train Acc: 93.80%\n",
            "\t Val. Loss: 0.767 |  Val. Acc: 73.78%\n",
            "100% 9/9 [00:00<00:00, 65.63it/s]\n",
            "100% 2/2 [00:00<00:00, 148.93it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train Acc: 94.82%\n",
            "\t Val. Loss: 0.855 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 68.89it/s]\n",
            "100% 2/2 [00:00<00:00, 145.39it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train Acc: 94.79%\n",
            "\t Val. Loss: 0.769 |  Val. Acc: 74.13%\n",
            "100% 9/9 [00:00<00:00, 67.76it/s]\n",
            "100% 2/2 [00:00<00:00, 146.70it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train Acc: 95.42%\n",
            "\t Val. Loss: 0.818 |  Val. Acc: 75.13%\n",
            "100% 9/9 [00:00<00:00, 67.80it/s]\n",
            "100% 2/2 [00:00<00:00, 147.92it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train Acc: 96.14%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 77.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szDjOF303wC",
        "colab_type": "text"
      },
      "source": [
        "## CNNClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVWP8U5kpmj",
        "colab_type": "text"
      },
      "source": [
        "### CNN2D Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2otPOBv06rZ",
        "colab_type": "code",
        "outputId": "b64bf9f2-8503-495a-8887-dce186ec5c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN2dClassifier --freeze-embeddings 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='CNN2dClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 117,515 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] CNN2dClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 64, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 64, kernel_size=(2, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 64, kernel_size=(3, 300), stride=(1, 1))\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 106.04it/s]\n",
            "100% 2/2 [00:00<00:00, 292.33it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.512 | Train Acc: 16.73%\n",
            "\t Val. Loss: 1.978 |  Val. Acc: 28.93%\n",
            "100% 9/9 [00:00<00:00, 125.24it/s]\n",
            "100% 2/2 [00:00<00:00, 243.90it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.151 | Train Acc: 23.50%\n",
            "\t Val. Loss: 1.839 |  Val. Acc: 33.97%\n",
            "100% 9/9 [00:00<00:00, 123.03it/s]\n",
            "100% 2/2 [00:00<00:00, 224.00it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.016 | Train Acc: 29.23%\n",
            "\t Val. Loss: 1.759 |  Val. Acc: 37.25%\n",
            "100% 9/9 [00:00<00:00, 132.64it/s]\n",
            "100% 2/2 [00:00<00:00, 298.23it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.862 | Train Acc: 32.94%\n",
            "\t Val. Loss: 1.637 |  Val. Acc: 47.91%\n",
            "100% 9/9 [00:00<00:00, 133.21it/s]\n",
            "100% 2/2 [00:00<00:00, 296.02it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.781 | Train Acc: 35.32%\n",
            "\t Val. Loss: 1.645 |  Val. Acc: 41.15%\n",
            "100% 9/9 [00:00<00:00, 132.98it/s]\n",
            "100% 2/2 [00:00<00:00, 306.70it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.706 | Train Acc: 40.18%\n",
            "\t Val. Loss: 1.578 |  Val. Acc: 44.64%\n",
            "100% 9/9 [00:00<00:00, 128.00it/s]\n",
            "100% 2/2 [00:00<00:00, 309.45it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.650 | Train Acc: 38.86%\n",
            "\t Val. Loss: 1.499 |  Val. Acc: 50.68%\n",
            "100% 9/9 [00:00<00:00, 129.25it/s]\n",
            "100% 2/2 [00:00<00:00, 297.82it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.540 | Train Acc: 45.74%\n",
            "\t Val. Loss: 1.429 |  Val. Acc: 53.38%\n",
            "100% 9/9 [00:00<00:00, 129.67it/s]\n",
            "100% 2/2 [00:00<00:00, 291.39it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.456 | Train Acc: 48.04%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 51.46%\n",
            "100% 9/9 [00:00<00:00, 126.57it/s]\n",
            "100% 2/2 [00:00<00:00, 290.65it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.366 | Train Acc: 53.07%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 53.59%\n",
            "100% 9/9 [00:00<00:00, 123.97it/s]\n",
            "100% 2/2 [00:00<00:00, 267.18it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.285 | Train Acc: 55.05%\n",
            "\t Val. Loss: 1.304 |  Val. Acc: 56.50%\n",
            "100% 9/9 [00:00<00:00, 130.34it/s]\n",
            "100% 2/2 [00:00<00:00, 305.90it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.262 | Train Acc: 56.50%\n",
            "\t Val. Loss: 1.249 |  Val. Acc: 59.42%\n",
            "100% 9/9 [00:00<00:00, 129.84it/s]\n",
            "100% 2/2 [00:00<00:00, 247.76it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.161 | Train Acc: 62.55%\n",
            "\t Val. Loss: 1.173 |  Val. Acc: 59.84%\n",
            "100% 9/9 [00:00<00:00, 126.39it/s]\n",
            "100% 2/2 [00:00<00:00, 158.49it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.145 | Train Acc: 62.80%\n",
            "\t Val. Loss: 1.146 |  Val. Acc: 58.49%\n",
            "100% 9/9 [00:00<00:00, 128.71it/s]\n",
            "100% 2/2 [00:00<00:00, 306.50it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.051 | Train Acc: 66.87%\n",
            "\t Val. Loss: 1.065 |  Val. Acc: 63.32%\n",
            "100% 9/9 [00:00<00:00, 129.01it/s]\n",
            "100% 2/2 [00:00<00:00, 295.20it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.018 | Train Acc: 68.17%\n",
            "\t Val. Loss: 1.026 |  Val. Acc: 61.40%\n",
            "100% 9/9 [00:00<00:00, 131.21it/s]\n",
            "100% 2/2 [00:00<00:00, 281.96it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.940 | Train Acc: 69.26%\n",
            "\t Val. Loss: 0.969 |  Val. Acc: 66.45%\n",
            "100% 9/9 [00:00<00:00, 131.54it/s]\n",
            "100% 2/2 [00:00<00:00, 302.15it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.918 | Train Acc: 69.84%\n",
            "\t Val. Loss: 0.920 |  Val. Acc: 72.07%\n",
            "100% 9/9 [00:00<00:00, 128.10it/s]\n",
            "100% 2/2 [00:00<00:00, 287.62it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.865 | Train Acc: 70.75%\n",
            "\t Val. Loss: 0.883 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 128.16it/s]\n",
            "100% 2/2 [00:00<00:00, 307.01it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.788 | Train Acc: 76.46%\n",
            "\t Val. Loss: 0.854 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 126.15it/s]\n",
            "100% 2/2 [00:00<00:00, 288.38it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.776 | Train Acc: 74.51%\n",
            "\t Val. Loss: 0.823 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 126.39it/s]\n",
            "100% 2/2 [00:00<00:00, 288.35it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.742 | Train Acc: 74.35%\n",
            "\t Val. Loss: 0.777 |  Val. Acc: 76.54%\n",
            "100% 9/9 [00:00<00:00, 129.52it/s]\n",
            "100% 2/2 [00:00<00:00, 298.12it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.692 | Train Acc: 79.28%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 77.32%\n",
            "100% 9/9 [00:00<00:00, 126.88it/s]\n",
            "100% 2/2 [00:00<00:00, 161.02it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.700 | Train Acc: 79.24%\n",
            "\t Val. Loss: 0.738 |  Val. Acc: 77.32%\n",
            "100% 9/9 [00:00<00:00, 124.94it/s]\n",
            "100% 2/2 [00:00<00:00, 268.12it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.620 | Train Acc: 81.78%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 79.46%\n",
            "100% 9/9 [00:00<00:00, 125.42it/s]\n",
            "100% 2/2 [00:00<00:00, 272.40it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.609 | Train Acc: 82.38%\n",
            "\t Val. Loss: 0.679 |  Val. Acc: 78.89%\n",
            "100% 9/9 [00:00<00:00, 128.62it/s]\n",
            "100% 2/2 [00:00<00:00, 219.14it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.552 | Train Acc: 83.81%\n",
            "\t Val. Loss: 0.646 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 126.02it/s]\n",
            "100% 2/2 [00:00<00:00, 274.96it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.566 | Train Acc: 82.54%\n",
            "\t Val. Loss: 0.625 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 130.33it/s]\n",
            "100% 2/2 [00:00<00:00, 308.82it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.538 | Train Acc: 84.79%\n",
            "\t Val. Loss: 0.624 |  Val. Acc: 79.46%\n",
            "100% 9/9 [00:00<00:00, 130.97it/s]\n",
            "100% 2/2 [00:00<00:00, 299.64it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.546 | Train Acc: 83.99%\n",
            "\t Val. Loss: 0.607 |  Val. Acc: 80.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6JUYoTTk19_",
        "colab_type": "text"
      },
      "source": [
        "### CNN1DClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukseycMBk4NK",
        "colab_type": "code",
        "outputId": "2cdc7d99-93fb-440d-fc22-ebd75138558e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN1dClassifier --freeze-embeddings 0"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:268 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=256, l2_regularization=0.001, learning_rate=0.001, model='CNN1dClassifier', model_location=None, n_layers=2, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:269 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:271 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:81 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:95 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:103 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:278 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:68 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:134 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:140 - initialize_new_model() ] Model Initialized with 561,911 trainiable parameters\n",
            "[DEBUG | train.py:152 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:303 -             <module>() ] CNN1dClassifier(\n",
            "  (embedding): Embedding(661, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=300, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 118.72it/s]\n",
            "100% 2/2 [00:00<00:00, 374.46it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.518 | Train Acc: 20.98%\n",
            "\t Val. Loss: 2.094 |  Val. Acc: 18.12%\n",
            "100% 9/9 [00:00<00:00, 137.71it/s]\n",
            "100% 2/2 [00:00<00:00, 413.86it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.073 | Train Acc: 26.60%\n",
            "\t Val. Loss: 1.735 |  Val. Acc: 22.38%\n",
            "100% 9/9 [00:00<00:00, 141.31it/s]\n",
            "100% 2/2 [00:00<00:00, 415.11it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.816 | Train Acc: 36.30%\n",
            "\t Val. Loss: 1.721 |  Val. Acc: 26.86%\n",
            "100% 9/9 [00:00<00:00, 149.37it/s]\n",
            "100% 2/2 [00:00<00:00, 427.27it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.717 | Train Acc: 39.73%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 39.80%\n",
            "100% 9/9 [00:00<00:00, 154.58it/s]\n",
            "100% 2/2 [00:00<00:00, 433.16it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.459 | Train Acc: 49.82%\n",
            "\t Val. Loss: 1.466 |  Val. Acc: 47.19%\n",
            "100% 9/9 [00:00<00:00, 149.93it/s]\n",
            "100% 2/2 [00:00<00:00, 352.97it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.310 | Train Acc: 55.05%\n",
            "\t Val. Loss: 1.313 |  Val. Acc: 48.75%\n",
            "100% 9/9 [00:00<00:00, 161.52it/s]\n",
            "100% 2/2 [00:00<00:00, 266.94it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.221 | Train Acc: 56.37%\n",
            "\t Val. Loss: 1.184 |  Val. Acc: 61.55%\n",
            "100% 9/9 [00:00<00:00, 159.76it/s]\n",
            "100% 2/2 [00:00<00:00, 444.90it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.144 | Train Acc: 62.06%\n",
            "\t Val. Loss: 1.109 |  Val. Acc: 63.32%\n",
            "100% 9/9 [00:00<00:00, 165.00it/s]\n",
            "100% 2/2 [00:00<00:00, 455.73it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.010 | Train Acc: 67.93%\n",
            "\t Val. Loss: 1.083 |  Val. Acc: 61.55%\n",
            "100% 9/9 [00:00<00:00, 160.78it/s]\n",
            "100% 2/2 [00:00<00:00, 444.29it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.942 | Train Acc: 68.78%\n",
            "\t Val. Loss: 0.946 |  Val. Acc: 71.07%\n",
            "100% 9/9 [00:00<00:00, 163.31it/s]\n",
            "100% 2/2 [00:00<00:00, 450.30it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.866 | Train Acc: 71.84%\n",
            "\t Val. Loss: 0.867 |  Val. Acc: 79.60%\n",
            "100% 9/9 [00:00<00:00, 164.43it/s]\n",
            "100% 2/2 [00:00<00:00, 439.22it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.727 | Train Acc: 77.63%\n",
            "\t Val. Loss: 0.844 |  Val. Acc: 73.99%\n",
            "100% 9/9 [00:00<00:00, 155.38it/s]\n",
            "100% 2/2 [00:00<00:00, 438.16it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.669 | Train Acc: 79.76%\n",
            "\t Val. Loss: 0.758 |  Val. Acc: 74.77%\n",
            "100% 9/9 [00:00<00:00, 158.33it/s]\n",
            "100% 2/2 [00:00<00:00, 432.78it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.585 | Train Acc: 84.19%\n",
            "\t Val. Loss: 0.704 |  Val. Acc: 80.17%\n",
            "100% 9/9 [00:00<00:00, 161.87it/s]\n",
            "100% 2/2 [00:00<00:00, 447.97it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.559 | Train Acc: 85.08%\n",
            "\t Val. Loss: 0.632 |  Val. Acc: 82.88%\n",
            "100% 9/9 [00:00<00:00, 162.41it/s]\n",
            "100% 2/2 [00:00<00:00, 327.00it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.494 | Train Acc: 87.24%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 82.88%\n",
            "100% 9/9 [00:00<00:00, 157.42it/s]\n",
            "100% 2/2 [00:00<00:00, 384.92it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.469 | Train Acc: 86.53%\n",
            "\t Val. Loss: 0.586 |  Val. Acc: 81.74%\n",
            "100% 9/9 [00:00<00:00, 161.84it/s]\n",
            "100% 2/2 [00:00<00:00, 406.92it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.405 | Train Acc: 89.24%\n",
            "\t Val. Loss: 0.532 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 162.57it/s]\n",
            "100% 2/2 [00:00<00:00, 444.69it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Acc: 88.33%\n",
            "\t Val. Loss: 0.514 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 155.00it/s]\n",
            "100% 2/2 [00:00<00:00, 441.74it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.354 | Train Acc: 89.95%\n",
            "\t Val. Loss: 0.485 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 154.75it/s]\n",
            "100% 2/2 [00:00<00:00, 421.22it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.334 | Train Acc: 89.91%\n",
            "\t Val. Loss: 0.486 |  Val. Acc: 84.44%\n",
            "100% 9/9 [00:00<00:00, 165.33it/s]\n",
            "100% 2/2 [00:00<00:00, 315.82it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.312 | Train Acc: 91.22%\n",
            "\t Val. Loss: 0.468 |  Val. Acc: 86.00%\n",
            "100% 9/9 [00:00<00:00, 160.77it/s]\n",
            "100% 2/2 [00:00<00:00, 431.93it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train Acc: 92.21%\n",
            "\t Val. Loss: 0.430 |  Val. Acc: 89.27%\n",
            "100% 9/9 [00:00<00:00, 161.05it/s]\n",
            "100% 2/2 [00:00<00:00, 450.81it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.298 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.420 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 156.48it/s]\n",
            "100% 2/2 [00:00<00:00, 441.32it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.311 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.409 |  Val. Acc: 90.05%\n",
            "100% 9/9 [00:00<00:00, 163.79it/s]\n",
            "100% 2/2 [00:00<00:00, 451.24it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train Acc: 94.05%\n",
            "\t Val. Loss: 0.386 |  Val. Acc: 91.41%\n",
            "100% 9/9 [00:00<00:00, 161.52it/s]\n",
            "100% 2/2 [00:00<00:00, 445.44it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train Acc: 93.47%\n",
            "\t Val. Loss: 0.371 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 151.40it/s]\n",
            "100% 2/2 [00:00<00:00, 445.35it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train Acc: 94.16%\n",
            "\t Val. Loss: 0.383 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 164.58it/s]\n",
            "100% 2/2 [00:00<00:00, 449.24it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train Acc: 94.53%\n",
            "\t Val. Loss: 0.375 |  Val. Acc: 90.62%\n",
            "100% 9/9 [00:00<00:00, 161.51it/s]\n",
            "100% 2/2 [00:00<00:00, 342.22it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train Acc: 94.73%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 90.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn_oZuLhKqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}