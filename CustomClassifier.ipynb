{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomClassifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1PQCpSDGct4BTv3x9oH8GD1HIdNihgWj0",
      "authorship_tag": "ABX9TyP+yjOTY5FgbRQ/O0AMgOl/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/QuestionGenerator/blob/master/CustomClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaCToCs_b4d1",
        "colab_type": "text"
      },
      "source": [
        "# Testing Classifier Deep Learning Architecture Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9CNpxNblqe",
        "colab_type": "code",
        "outputId": "b3dd7297-d9da-422a-f65e-8d161ad3cc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!rm -rf QuestionGenerator\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "from subprocess import Popen\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone --single-branch --branch master  https://{0}:{1}@github.com/{0}/{2}.git'.format(user, password, 'QuestionGenerator')\n",
        "print(Popen(cmd_string, shell=True))\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: shivammehta007\n",
            "Password: ··········\n",
            "<subprocess.Popen object at 0x7f105557e080>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW_iNs97b9e9",
        "colab_type": "code",
        "outputId": "5a3bbbd5-531a-49ac-a64b-d32e6f319a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -rf QuestionGenerator/FromScratch\n",
        "%cd QuestionGenerator/classifier/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxftkthaPIGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -rf data/processed/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAatXS64cdOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python preprocessdata.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0OzHqrpJEx",
        "colab_type": "code",
        "outputId": "2d647fcb-b121-4b7f-c2b4-5f5d1424a45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%ls -a"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34mconfig\u001b[0m/  datasetloader.py    \u001b[01;34mmodel\u001b[0m/             \u001b[01;34mtrained\u001b[0m/  utility.py\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mdata\u001b[0m/    helperfunctions.py  preprocessdata.py  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "608c9eae-266f-4c61-e9c0-984fd0793253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2657  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "f61c4977-ea8d-4f72-dcf6-8702f1ecc184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/QuestionGenerator/classifier\n",
            " 97% 375M/386M [00:01<00:00, 239MB/s]\n",
            "100% 386M/386M [00:01<00:00, 215MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "bd4d9a16-88c6-4be3-a71f-126fb8a708cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nhgkJx_ne2G",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIWmWDYBxNI7",
        "colab_type": "code",
        "outputId": "9caf206f-3854-4757-f4a7-53161d8d1324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!python train.py -h"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [-s SEED] [-loc MODEL_LOCATION] [-b BIDIRECTIONAL]\n",
            "                [-d DROPOUT] [-e EMBEDDING_DIM] [-hd HIDDEN_DIM] [-l N_LAYERS]\n",
            "                [-lr LEARNING_RATE] [-n EPOCHS] [-batch BATCH_SIZE]\n",
            "                [-f FREEZE_EMBEDDINGS] [-t {multi,answeronly}]\n",
            "                [-l2 L2_REGULARIZATION]\n",
            "                [-m {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer,CNN1dExtraLayerClassifier}]\n",
            "                [-lhd LINEAR_HIDDEN_DIM]\n",
            "\n",
            "Utility to train the Model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -s SEED, --seed SEED  Set custom seed for reproducibility\n",
            "  -loc MODEL_LOCATION, --model-location MODEL_LOCATION\n",
            "                        Give an already trained model location to use and\n",
            "                        train more epochs on it\n",
            "  -b BIDIRECTIONAL, --bidirectional BIDIRECTIONAL\n",
            "                        Makes the model Bidirectional\n",
            "  -d DROPOUT, --dropout DROPOUT\n",
            "                        Dropout count for the model\n",
            "  -e EMBEDDING_DIM, --embedding-dim EMBEDDING_DIM\n",
            "                        Embedding Dimensions\n",
            "  -hd HIDDEN_DIM, --hidden-dim HIDDEN_DIM\n",
            "                        Hidden dimensions of the RNN\n",
            "  -l N_LAYERS, --n-layers N_LAYERS\n",
            "                        Number of layers in RNN\n",
            "  -lr LEARNING_RATE, --learning-rate LEARNING_RATE\n",
            "                        Learning rate of Adam Optimizer\n",
            "  -n EPOCHS, --epochs EPOCHS\n",
            "                        Number of Epochs to train model\n",
            "  -batch BATCH_SIZE, --batch_size BATCH_SIZE\n",
            "                        Number of Epochs to train model\n",
            "  -f FREEZE_EMBEDDINGS, --freeze-embeddings FREEZE_EMBEDDINGS\n",
            "                        Freeze Embeddings of Model\n",
            "  -t {multi,answeronly}, --tag {multi,answeronly}\n",
            "                        Use two different dataset type, multi type and single\n",
            "                        type where all are merged into same key\n",
            "  -l2 L2_REGULARIZATION, --l2-regularization L2_REGULARIZATION\n",
            "                        Value of alpha in l2 regularization 0 means no\n",
            "                        regularization\n",
            "  -m {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer,CNN1dExtraLayerClassifier}, --model {RNNHiddenClassifier,RNNMaxpoolClassifier,RNNFieldClassifier,CNN2dClassifier,CNN1dClassifier,RNNFieldClassifer,CNN1dExtraLayerClassifier}\n",
            "                        select the classifier to train on\n",
            "  -lhd LINEAR_HIDDEN_DIM, --linear-hidden-dim LINEAR_HIDDEN_DIM\n",
            "                        Freeze Embeddings of Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd6-Cp6UiuT8",
        "colab_type": "text"
      },
      "source": [
        "## RNNClassifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adzPuBh7NbLC",
        "colab_type": "text"
      },
      "source": [
        "### RNNHiddenClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHpPoyGcz9H",
        "colab_type": "code",
        "outputId": "890e16b2-3ff8-4dda-8fe8-ced79aaa68b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "!python train.py -n 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:329 -                cache() ] Loading vectors from glove.6B.300d.txt\n",
            " 18% 73395/400000 [00:06<00:28, 11479.04it/s]Exception ignored in: <bound method tqdm.__del__ of  18% 73395/400000 [00:06<00:28, 11479.04it/s]>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1049, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1231, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 541, in _decr_instances\n",
            "    cls._instances.remove(instance)\n",
            "  File \"/usr/lib/python3.6/_weakrefset.py\", line 109, in remove\n",
            "    self.data.remove(ref(item))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1058, in __hash__\n",
            "    def __hash__(self):\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 313, in <module>\n",
            "    dataset = GrammarDasetMultiTag.get_iterators(args.batch_size)\n",
            "  File \"/content/QuestionGenerator/classifier/datasetloader.py\", line 89, in get_iterators\n",
            "    unk_init=torch.Tensor.normal_,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\", line 273, in build_vocab\n",
            "    self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/vocab.py\", line 88, in __init__\n",
            "    self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/vocab.py\", line 147, in load_vectors\n",
            "    vectors[idx] = pretrained_aliases[vector](**kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/vocab.py\", line 401, in __init__\n",
            "    super(GloVe, self).__init__(name, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/vocab.py\", line 280, in __init__\n",
            "    self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/vocab.py\", line 347, in cache\n",
            "    entries = line.rstrip().split(b\" \")\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hQXMDrqis75",
        "colab_type": "text"
      },
      "source": [
        "### RNNMaxpoolClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7QpzdKfir48",
        "colab_type": "code",
        "outputId": "50150f79-ccd3-4cec-bb9e-5e68c848ea96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m RNNMaxpoolClassifier  --freeze-embeddings 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:296 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNMaxpoolClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:297 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:299 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:306 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:71 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:159 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:165 - initialize_new_model() ] Model Initialized with 441,739 trainiable parameters\n",
            "[DEBUG | train.py:177 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:331 -             <module>() ] RNNMaxpoolClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 98.01it/s]\n",
            "100% 2/2 [00:00<00:00, 216.90it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.358 | Train Acc: 13.86%\n",
            "\t Val. Loss: 2.262 |  Val. Acc: 21.75%\n",
            "100% 9/9 [00:00<00:00, 149.68it/s]\n",
            "100% 2/2 [00:00<00:00, 275.56it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.256 | Train Acc: 19.23%\n",
            "\t Val. Loss: 2.186 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 149.65it/s]\n",
            "100% 2/2 [00:00<00:00, 284.81it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.177 | Train Acc: 24.74%\n",
            "\t Val. Loss: 2.160 |  Val. Acc: 9.38%\n",
            "100% 9/9 [00:00<00:00, 144.78it/s]\n",
            "100% 2/2 [00:00<00:00, 280.67it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.132 | Train Acc: 26.04%\n",
            "\t Val. Loss: 2.102 |  Val. Acc: 27.64%\n",
            "100% 9/9 [00:00<00:00, 158.20it/s]\n",
            "100% 2/2 [00:00<00:00, 284.62it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.061 | Train Acc: 37.32%\n",
            "\t Val. Loss: 2.015 |  Val. Acc: 33.04%\n",
            "100% 9/9 [00:00<00:00, 150.07it/s]\n",
            "100% 2/2 [00:00<00:00, 200.18it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.975 | Train Acc: 38.49%\n",
            "\t Val. Loss: 1.904 |  Val. Acc: 35.18%\n",
            "100% 9/9 [00:00<00:00, 157.03it/s]\n",
            "100% 2/2 [00:00<00:00, 294.23it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.840 | Train Acc: 41.03%\n",
            "\t Val. Loss: 1.777 |  Val. Acc: 40.01%\n",
            "100% 9/9 [00:00<00:00, 138.50it/s]\n",
            "100% 2/2 [00:00<00:00, 264.40it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.713 | Train Acc: 43.29%\n",
            "\t Val. Loss: 1.591 |  Val. Acc: 51.25%\n",
            "100% 9/9 [00:00<00:00, 160.02it/s]\n",
            "100% 2/2 [00:00<00:00, 290.08it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.536 | Train Acc: 50.43%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 55.15%\n",
            "100% 9/9 [00:00<00:00, 151.09it/s]\n",
            "100% 2/2 [00:00<00:00, 281.89it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.377 | Train Acc: 56.52%\n",
            "\t Val. Loss: 1.338 |  Val. Acc: 56.14%\n",
            "100% 9/9 [00:00<00:00, 155.21it/s]\n",
            "100% 2/2 [00:00<00:00, 296.81it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.174 | Train Acc: 64.92%\n",
            "\t Val. Loss: 1.276 |  Val. Acc: 56.71%\n",
            "100% 9/9 [00:00<00:00, 159.64it/s]\n",
            "100% 2/2 [00:00<00:00, 287.30it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.030 | Train Acc: 67.61%\n",
            "\t Val. Loss: 1.146 |  Val. Acc: 61.76%\n",
            "100% 9/9 [00:00<00:00, 152.95it/s]\n",
            "100% 2/2 [00:00<00:00, 282.81it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.892 | Train Acc: 75.81%\n",
            "\t Val. Loss: 1.126 |  Val. Acc: 58.28%\n",
            "100% 9/9 [00:00<00:00, 135.51it/s]\n",
            "100% 2/2 [00:00<00:00, 265.18it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.766 | Train Acc: 78.33%\n",
            "\t Val. Loss: 1.037 |  Val. Acc: 66.60%\n",
            "100% 9/9 [00:00<00:00, 159.85it/s]\n",
            "100% 2/2 [00:00<00:00, 293.40it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.715 | Train Acc: 79.13%\n",
            "\t Val. Loss: 0.980 |  Val. Acc: 68.94%\n",
            "100% 9/9 [00:00<00:00, 155.40it/s]\n",
            "100% 2/2 [00:00<00:00, 276.92it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.646 | Train Acc: 81.48%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 62.33%\n",
            "100% 9/9 [00:00<00:00, 149.70it/s]\n",
            "100% 2/2 [00:00<00:00, 282.11it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.591 | Train Acc: 82.97%\n",
            "\t Val. Loss: 0.901 |  Val. Acc: 71.43%\n",
            "100% 9/9 [00:00<00:00, 157.73it/s]\n",
            "100% 2/2 [00:00<00:00, 291.69it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.533 | Train Acc: 85.77%\n",
            "\t Val. Loss: 0.862 |  Val. Acc: 69.30%\n",
            "100% 9/9 [00:00<00:00, 159.04it/s]\n",
            "100% 2/2 [00:00<00:00, 217.98it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.502 | Train Acc: 85.16%\n",
            "\t Val. Loss: 0.794 |  Val. Acc: 68.73%\n",
            "100% 9/9 [00:00<00:00, 140.03it/s]\n",
            "100% 2/2 [00:00<00:00, 257.76it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.445 | Train Acc: 88.63%\n",
            "\t Val. Loss: 0.874 |  Val. Acc: 67.38%\n",
            "100% 9/9 [00:00<00:00, 145.07it/s]\n",
            "100% 2/2 [00:00<00:00, 289.81it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.443 | Train Acc: 87.92%\n",
            "\t Val. Loss: 0.756 |  Val. Acc: 71.07%\n",
            "100% 9/9 [00:00<00:00, 157.00it/s]\n",
            "100% 2/2 [00:00<00:00, 288.76it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.373 | Train Acc: 89.46%\n",
            "\t Val. Loss: 0.769 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 163.28it/s]\n",
            "100% 2/2 [00:00<00:00, 285.29it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.349 | Train Acc: 91.13%\n",
            "\t Val. Loss: 0.759 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 150.10it/s]\n",
            "100% 2/2 [00:00<00:00, 255.57it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.336 | Train Acc: 91.34%\n",
            "\t Val. Loss: 0.685 |  Val. Acc: 71.64%\n",
            "100% 9/9 [00:00<00:00, 153.48it/s]\n",
            "100% 2/2 [00:00<00:00, 290.78it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.306 | Train Acc: 92.28%\n",
            "\t Val. Loss: 0.777 |  Val. Acc: 73.78%\n",
            "100% 9/9 [00:00<00:00, 162.88it/s]\n",
            "100% 2/2 [00:00<00:00, 277.85it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.297 | Train Acc: 92.93%\n",
            "\t Val. Loss: 0.765 |  Val. Acc: 72.64%\n",
            "100% 9/9 [00:00<00:00, 154.04it/s]\n",
            "100% 2/2 [00:00<00:00, 216.05it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.269 | Train Acc: 92.86%\n",
            "\t Val. Loss: 0.826 |  Val. Acc: 72.99%\n",
            "100% 9/9 [00:00<00:00, 140.94it/s]\n",
            "100% 2/2 [00:00<00:00, 277.87it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train Acc: 93.99%\n",
            "\t Val. Loss: 0.668 |  Val. Acc: 76.27%\n",
            "100% 9/9 [00:00<00:00, 155.32it/s]\n",
            "100% 2/2 [00:00<00:00, 258.02it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train Acc: 93.97%\n",
            "\t Val. Loss: 0.813 |  Val. Acc: 74.56%\n",
            "100% 9/9 [00:00<00:00, 156.46it/s]\n",
            "100% 2/2 [00:00<00:00, 274.51it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train Acc: 93.64%\n",
            "\t Val. Loss: 0.747 |  Val. Acc: 75.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPuuJNZ3YefO",
        "colab_type": "text"
      },
      "source": [
        "### RNNFieldClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWCPNwHPYhN2",
        "colab_type": "code",
        "outputId": "1411756a-f9c9-4256-ab69-4de444a15496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50 -m RNNFieldClassifer --freeze-embeddings 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:296 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=0, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='RNNFieldClassifer', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:297 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:299 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:306 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:71 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:159 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:165 - initialize_new_model() ] Model Initialized with 686,347 trainiable parameters\n",
            "[DEBUG | train.py:177 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:331 -             <module>() ] RNNFieldClassifer(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (tag_embedding): Embedding(5, 50, padding_idx=1)\n",
            "  (rnn): LSTM(350, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 84.36it/s]\n",
            "100% 2/2 [00:00<00:00, 293.12it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.356 | Train Acc: 13.15%\n",
            "\t Val. Loss: 2.215 |  Val. Acc: 17.63%\n",
            "100% 9/9 [00:00<00:00, 118.54it/s]\n",
            "100% 2/2 [00:00<00:00, 316.48it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.259 | Train Acc: 16.88%\n",
            "\t Val. Loss: 2.237 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 123.77it/s]\n",
            "100% 2/2 [00:00<00:00, 307.55it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.235 | Train Acc: 17.79%\n",
            "\t Val. Loss: 2.271 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 115.25it/s]\n",
            "100% 2/2 [00:00<00:00, 319.83it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.246 | Train Acc: 17.23%\n",
            "\t Val. Loss: 2.184 |  Val. Acc: 15.56%\n",
            "100% 9/9 [00:00<00:00, 125.60it/s]\n",
            "100% 2/2 [00:00<00:00, 314.05it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.200 | Train Acc: 20.31%\n",
            "\t Val. Loss: 2.151 |  Val. Acc: 20.04%\n",
            "100% 9/9 [00:00<00:00, 125.60it/s]\n",
            "100% 2/2 [00:00<00:00, 335.89it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.163 | Train Acc: 20.11%\n",
            "\t Val. Loss: 2.105 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 118.55it/s]\n",
            "100% 2/2 [00:00<00:00, 265.93it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.146 | Train Acc: 21.98%\n",
            "\t Val. Loss: 2.035 |  Val. Acc: 20.82%\n",
            "100% 9/9 [00:00<00:00, 122.99it/s]\n",
            "100% 2/2 [00:00<00:00, 333.69it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.064 | Train Acc: 21.29%\n",
            "\t Val. Loss: 1.882 |  Val. Acc: 34.54%\n",
            "100% 9/9 [00:00<00:00, 129.80it/s]\n",
            "100% 2/2 [00:00<00:00, 327.91it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.993 | Train Acc: 28.79%\n",
            "\t Val. Loss: 1.801 |  Val. Acc: 33.76%\n",
            "100% 9/9 [00:00<00:00, 121.41it/s]\n",
            "100% 2/2 [00:00<00:00, 326.23it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.914 | Train Acc: 29.03%\n",
            "\t Val. Loss: 1.796 |  Val. Acc: 33.97%\n",
            "100% 9/9 [00:00<00:00, 126.95it/s]\n",
            "100% 2/2 [00:00<00:00, 313.85it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.864 | Train Acc: 29.57%\n",
            "\t Val. Loss: 1.731 |  Val. Acc: 35.33%\n",
            "100% 9/9 [00:00<00:00, 123.37it/s]\n",
            "100% 2/2 [00:00<00:00, 284.39it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.832 | Train Acc: 29.92%\n",
            "\t Val. Loss: 1.702 |  Val. Acc: 36.11%\n",
            "100% 9/9 [00:00<00:00, 123.14it/s]\n",
            "100% 2/2 [00:00<00:00, 209.63it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.815 | Train Acc: 32.65%\n",
            "\t Val. Loss: 1.630 |  Val. Acc: 47.91%\n",
            "100% 9/9 [00:00<00:00, 128.91it/s]\n",
            "100% 2/2 [00:00<00:00, 329.06it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.721 | Train Acc: 37.41%\n",
            "\t Val. Loss: 1.602 |  Val. Acc: 43.86%\n",
            "100% 9/9 [00:00<00:00, 128.67it/s]\n",
            "100% 2/2 [00:00<00:00, 329.39it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.680 | Train Acc: 39.77%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 45.99%\n",
            "100% 9/9 [00:00<00:00, 127.03it/s]\n",
            "100% 2/2 [00:00<00:00, 333.03it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.588 | Train Acc: 42.61%\n",
            "\t Val. Loss: 1.463 |  Val. Acc: 45.99%\n",
            "100% 9/9 [00:00<00:00, 129.16it/s]\n",
            "100% 2/2 [00:00<00:00, 322.12it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.518 | Train Acc: 46.80%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 49.68%\n",
            "100% 9/9 [00:00<00:00, 128.76it/s]\n",
            "100% 2/2 [00:00<00:00, 332.45it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.398 | Train Acc: 53.18%\n",
            "\t Val. Loss: 1.411 |  Val. Acc: 50.25%\n",
            "100% 9/9 [00:00<00:00, 119.57it/s]\n",
            "100% 2/2 [00:00<00:00, 321.98it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.394 | Train Acc: 50.38%\n",
            "\t Val. Loss: 1.275 |  Val. Acc: 55.09%\n",
            "100% 9/9 [00:00<00:00, 127.95it/s]\n",
            "100% 2/2 [00:00<00:00, 327.82it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.361 | Train Acc: 51.64%\n",
            "\t Val. Loss: 1.134 |  Val. Acc: 55.51%\n",
            "100% 9/9 [00:00<00:00, 119.93it/s]\n",
            "100% 2/2 [00:00<00:00, 339.40it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.273 | Train Acc: 55.24%\n",
            "\t Val. Loss: 1.288 |  Val. Acc: 56.08%\n",
            "100% 9/9 [00:00<00:00, 128.96it/s]\n",
            "100% 2/2 [00:00<00:00, 224.27it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.153 | Train Acc: 61.21%\n",
            "\t Val. Loss: 1.214 |  Val. Acc: 54.73%\n",
            "100% 9/9 [00:00<00:00, 129.03it/s]\n",
            "100% 2/2 [00:00<00:00, 331.40it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.190 | Train Acc: 58.19%\n",
            "\t Val. Loss: 1.042 |  Val. Acc: 58.78%\n",
            "100% 9/9 [00:00<00:00, 124.18it/s]\n",
            "100% 2/2 [00:00<00:00, 330.20it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.042 | Train Acc: 62.66%\n",
            "\t Val. Loss: 1.015 |  Val. Acc: 59.78%\n",
            "100% 9/9 [00:00<00:00, 122.16it/s]\n",
            "100% 2/2 [00:00<00:00, 307.31it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.031 | Train Acc: 63.10%\n",
            "\t Val. Loss: 1.198 |  Val. Acc: 54.73%\n",
            "100% 9/9 [00:00<00:00, 123.48it/s]\n",
            "100% 2/2 [00:00<00:00, 314.89it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.964 | Train Acc: 66.37%\n",
            "\t Val. Loss: 0.970 |  Val. Acc: 62.90%\n",
            "100% 9/9 [00:00<00:00, 129.27it/s]\n",
            "100% 2/2 [00:00<00:00, 323.11it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.899 | Train Acc: 68.52%\n",
            "\t Val. Loss: 1.163 |  Val. Acc: 59.99%\n",
            "100% 9/9 [00:00<00:00, 127.23it/s]\n",
            "100% 2/2 [00:00<00:00, 329.69it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.903 | Train Acc: 70.04%\n",
            "\t Val. Loss: 0.868 |  Val. Acc: 65.24%\n",
            "100% 9/9 [00:00<00:00, 127.34it/s]\n",
            "100% 2/2 [00:00<00:00, 313.67it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.844 | Train Acc: 71.52%\n",
            "\t Val. Loss: 0.973 |  Val. Acc: 59.84%\n",
            "100% 9/9 [00:00<00:00, 124.51it/s]\n",
            "100% 2/2 [00:00<00:00, 278.36it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.760 | Train Acc: 74.34%\n",
            "\t Val. Loss: 0.870 |  Val. Acc: 64.61%\n",
            "100% 9/9 [00:00<00:00, 122.12it/s]\n",
            "100% 2/2 [00:00<00:00, 320.05it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.693 | Train Acc: 75.70%\n",
            "\t Val. Loss: 0.916 |  Val. Acc: 64.82%\n",
            "100% 9/9 [00:00<00:00, 128.27it/s]\n",
            "100% 2/2 [00:00<00:00, 322.70it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.662 | Train Acc: 77.70%\n",
            "\t Val. Loss: 0.753 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 120.98it/s]\n",
            "100% 2/2 [00:00<00:00, 334.10it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.693 | Train Acc: 74.49%\n",
            "\t Val. Loss: 1.090 |  Val. Acc: 62.33%\n",
            "100% 9/9 [00:00<00:00, 126.49it/s]\n",
            "100% 2/2 [00:00<00:00, 199.93it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.677 | Train Acc: 77.03%\n",
            "\t Val. Loss: 0.711 |  Val. Acc: 75.13%\n",
            "100% 9/9 [00:00<00:00, 125.87it/s]\n",
            "100% 2/2 [00:00<00:00, 313.83it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.584 | Train Acc: 79.39%\n",
            "\t Val. Loss: 0.756 |  Val. Acc: 69.51%\n",
            "100% 9/9 [00:00<00:00, 123.60it/s]\n",
            "100% 2/2 [00:00<00:00, 327.19it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.586 | Train Acc: 78.89%\n",
            "\t Val. Loss: 0.731 |  Val. Acc: 75.70%\n",
            "100% 9/9 [00:00<00:00, 123.62it/s]\n",
            "100% 2/2 [00:00<00:00, 323.72it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.542 | Train Acc: 79.33%\n",
            "\t Val. Loss: 0.758 |  Val. Acc: 70.86%\n",
            "100% 9/9 [00:00<00:00, 124.85it/s]\n",
            "100% 2/2 [00:00<00:00, 326.93it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.526 | Train Acc: 80.56%\n",
            "\t Val. Loss: 0.726 |  Val. Acc: 73.21%\n",
            "100% 9/9 [00:00<00:00, 128.91it/s]\n",
            "100% 2/2 [00:00<00:00, 338.36it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.525 | Train Acc: 79.61%\n",
            "\t Val. Loss: 0.737 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 130.12it/s]\n",
            "100% 2/2 [00:00<00:00, 300.70it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.470 | Train Acc: 83.99%\n",
            "\t Val. Loss: 0.641 |  Val. Acc: 78.82%\n",
            "100% 9/9 [00:00<00:00, 128.23it/s]\n",
            "100% 2/2 [00:00<00:00, 324.75it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.482 | Train Acc: 83.71%\n",
            "\t Val. Loss: 0.706 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 128.96it/s]\n",
            "100% 2/2 [00:00<00:00, 316.80it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.437 | Train Acc: 83.92%\n",
            "\t Val. Loss: 0.683 |  Val. Acc: 76.69%\n",
            "100% 9/9 [00:00<00:00, 119.22it/s]\n",
            "100% 2/2 [00:00<00:00, 314.86it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.420 | Train Acc: 84.58%\n",
            "\t Val. Loss: 0.668 |  Val. Acc: 80.17%\n",
            "100% 9/9 [00:00<00:00, 128.75it/s]\n",
            "100% 2/2 [00:00<00:00, 328.80it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.430 | Train Acc: 85.21%\n",
            "\t Val. Loss: 0.762 |  Val. Acc: 73.21%\n",
            "100% 9/9 [00:00<00:00, 130.40it/s]\n",
            "100% 2/2 [00:00<00:00, 336.45it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.436 | Train Acc: 85.19%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 76.12%\n",
            "100% 9/9 [00:00<00:00, 121.94it/s]\n",
            "100% 2/2 [00:00<00:00, 322.76it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.434 | Train Acc: 85.36%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 76.12%\n",
            "100% 9/9 [00:00<00:00, 131.11it/s]\n",
            "100% 2/2 [00:00<00:00, 329.65it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.417 | Train Acc: 86.85%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 78.46%\n",
            "100% 9/9 [00:00<00:00, 127.42it/s]\n",
            "100% 2/2 [00:00<00:00, 312.66it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.412 | Train Acc: 84.73%\n",
            "\t Val. Loss: 0.612 |  Val. Acc: 78.82%\n",
            "100% 9/9 [00:00<00:00, 121.22it/s]\n",
            "100% 2/2 [00:00<00:00, 335.58it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.387 | Train Acc: 86.88%\n",
            "\t Val. Loss: 0.725 |  Val. Acc: 72.42%\n",
            "100% 9/9 [00:00<00:00, 126.22it/s]\n",
            "100% 2/2 [00:00<00:00, 320.14it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.364 | Train Acc: 88.05%\n",
            "\t Val. Loss: 0.510 |  Val. Acc: 80.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szDjOF303wC",
        "colab_type": "text"
      },
      "source": [
        "## CNNClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVWP8U5kpmj",
        "colab_type": "text"
      },
      "source": [
        "### CNN2D Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2otPOBv06rZ",
        "colab_type": "code",
        "outputId": "14a0ae0b-f4a6-48ca-c118-477e08b2f7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN2dClassifier --freeze-embeddings 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:296 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, model='CNN2dClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:297 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:299 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:306 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:71 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:159 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:165 - initialize_new_model() ] Model Initialized with 367,115 trainiable parameters\n",
            "[DEBUG | train.py:177 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:331 -             <module>() ] CNN2dClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 64, kernel_size=(1, 300), stride=(1, 1))\n",
            "    (1): Conv2d(1, 64, kernel_size=(3, 300), stride=(1, 1))\n",
            "    (2): Conv2d(1, 64, kernel_size=(5, 300), stride=(1, 1))\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 28.81it/s]\n",
            "100% 2/2 [00:00<00:00, 361.30it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.503 | Train Acc: 16.36%\n",
            "\t Val. Loss: 1.956 |  Val. Acc: 32.05%\n",
            "100% 9/9 [00:00<00:00, 32.39it/s]\n",
            "100% 2/2 [00:00<00:00, 427.92it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.059 | Train Acc: 25.91%\n",
            "\t Val. Loss: 1.757 |  Val. Acc: 44.07%\n",
            "100% 9/9 [00:00<00:00, 32.22it/s]\n",
            "100% 2/2 [00:00<00:00, 264.27it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.911 | Train Acc: 29.92%\n",
            "\t Val. Loss: 1.641 |  Val. Acc: 41.15%\n",
            "100% 9/9 [00:00<00:00, 32.00it/s]\n",
            "100% 2/2 [00:00<00:00, 413.17it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.848 | Train Acc: 33.63%\n",
            "\t Val. Loss: 1.557 |  Val. Acc: 48.33%\n",
            "100% 9/9 [00:00<00:00, 31.20it/s]\n",
            "100% 2/2 [00:00<00:00, 377.39it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.574 | Train Acc: 45.91%\n",
            "\t Val. Loss: 1.558 |  Val. Acc: 44.49%\n",
            "100% 9/9 [00:00<00:00, 31.52it/s]\n",
            "100% 2/2 [00:00<00:00, 371.92it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.533 | Train Acc: 47.84%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 54.16%\n",
            "100% 9/9 [00:00<00:00, 31.92it/s]\n",
            "100% 2/2 [00:00<00:00, 361.17it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.421 | Train Acc: 51.51%\n",
            "\t Val. Loss: 1.395 |  Val. Acc: 59.63%\n",
            "100% 9/9 [00:00<00:00, 31.97it/s]\n",
            "100% 2/2 [00:00<00:00, 440.58it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.378 | Train Acc: 51.40%\n",
            "\t Val. Loss: 1.289 |  Val. Acc: 59.42%\n",
            "100% 9/9 [00:00<00:00, 32.58it/s]\n",
            "100% 2/2 [00:00<00:00, 391.97it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.255 | Train Acc: 57.72%\n",
            "\t Val. Loss: 1.254 |  Val. Acc: 61.76%\n",
            "100% 9/9 [00:00<00:00, 31.88it/s]\n",
            "100% 2/2 [00:00<00:00, 346.52it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.107 | Train Acc: 65.22%\n",
            "\t Val. Loss: 1.177 |  Val. Acc: 64.10%\n",
            "100% 9/9 [00:00<00:00, 31.49it/s]\n",
            "100% 2/2 [00:00<00:00, 434.17it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.083 | Train Acc: 62.45%\n",
            "\t Val. Loss: 1.080 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 32.27it/s]\n",
            "100% 2/2 [00:00<00:00, 424.98it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.054 | Train Acc: 65.83%\n",
            "\t Val. Loss: 1.041 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 32.18it/s]\n",
            "100% 2/2 [00:00<00:00, 422.30it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.966 | Train Acc: 70.28%\n",
            "\t Val. Loss: 0.969 |  Val. Acc: 71.49%\n",
            "100% 9/9 [00:00<00:00, 31.46it/s]\n",
            "100% 2/2 [00:00<00:00, 441.41it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.888 | Train Acc: 72.77%\n",
            "\t Val. Loss: 0.933 |  Val. Acc: 75.19%\n",
            "100% 9/9 [00:00<00:00, 32.38it/s]\n",
            "100% 2/2 [00:00<00:00, 461.72it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.835 | Train Acc: 73.55%\n",
            "\t Val. Loss: 0.852 |  Val. Acc: 73.84%\n",
            "100% 9/9 [00:00<00:00, 32.21it/s]\n",
            "100% 2/2 [00:00<00:00, 416.18it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.783 | Train Acc: 75.51%\n",
            "\t Val. Loss: 0.815 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 31.76it/s]\n",
            "100% 2/2 [00:00<00:00, 444.90it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.750 | Train Acc: 77.59%\n",
            "\t Val. Loss: 0.782 |  Val. Acc: 75.19%\n",
            "100% 9/9 [00:00<00:00, 32.02it/s]\n",
            "100% 2/2 [00:00<00:00, 426.45it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.644 | Train Acc: 79.26%\n",
            "\t Val. Loss: 0.733 |  Val. Acc: 78.10%\n",
            "100% 9/9 [00:00<00:00, 31.72it/s]\n",
            "100% 2/2 [00:00<00:00, 411.11it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.675 | Train Acc: 80.58%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 78.89%\n",
            "100% 9/9 [00:00<00:00, 31.77it/s]\n",
            "100% 2/2 [00:00<00:00, 424.46it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.661 | Train Acc: 79.37%\n",
            "\t Val. Loss: 0.662 |  Val. Acc: 81.59%\n",
            "100% 9/9 [00:00<00:00, 31.40it/s]\n",
            "100% 2/2 [00:00<00:00, 368.31it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.531 | Train Acc: 86.09%\n",
            "\t Val. Loss: 0.618 |  Val. Acc: 80.81%\n",
            "100% 9/9 [00:00<00:00, 32.40it/s]\n",
            "100% 2/2 [00:00<00:00, 427.10it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.523 | Train Acc: 84.40%\n",
            "\t Val. Loss: 0.571 |  Val. Acc: 81.59%\n",
            "100% 9/9 [00:00<00:00, 32.17it/s]\n",
            "100% 2/2 [00:00<00:00, 427.97it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.492 | Train Acc: 87.13%\n",
            "\t Val. Loss: 0.565 |  Val. Acc: 83.93%\n",
            "100% 9/9 [00:00<00:00, 31.83it/s]\n",
            "100% 2/2 [00:00<00:00, 440.23it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.494 | Train Acc: 83.71%\n",
            "\t Val. Loss: 0.561 |  Val. Acc: 84.50%\n",
            "100% 9/9 [00:00<00:00, 31.95it/s]\n",
            "100% 2/2 [00:00<00:00, 444.43it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.487 | Train Acc: 85.75%\n",
            "\t Val. Loss: 0.533 |  Val. Acc: 85.85%\n",
            "100% 9/9 [00:00<00:00, 32.28it/s]\n",
            "100% 2/2 [00:00<00:00, 455.63it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.396 | Train Acc: 88.54%\n",
            "\t Val. Loss: 0.515 |  Val. Acc: 86.42%\n",
            "100% 9/9 [00:00<00:00, 32.58it/s]\n",
            "100% 2/2 [00:00<00:00, 441.02it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.426 | Train Acc: 87.79%\n",
            "\t Val. Loss: 0.496 |  Val. Acc: 86.42%\n",
            "100% 9/9 [00:00<00:00, 32.10it/s]\n",
            "100% 2/2 [00:00<00:00, 445.21it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.392 | Train Acc: 89.54%\n",
            "\t Val. Loss: 0.481 |  Val. Acc: 88.56%\n",
            "100% 9/9 [00:00<00:00, 31.93it/s]\n",
            "100% 2/2 [00:00<00:00, 454.64it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.375 | Train Acc: 89.46%\n",
            "\t Val. Loss: 0.473 |  Val. Acc: 87.99%\n",
            "100% 9/9 [00:00<00:00, 32.34it/s]\n",
            "100% 2/2 [00:00<00:00, 433.99it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.361 | Train Acc: 89.41%\n",
            "\t Val. Loss: 0.437 |  Val. Acc: 88.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6JUYoTTk19_",
        "colab_type": "text"
      },
      "source": [
        "### CNN1DClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukseycMBk4NK",
        "colab_type": "code",
        "outputId": "d6052299-b0bc-4b5d-f8b6-2158608ea7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 30 -m CNN1dClassifier --freeze-embeddings 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=30, freeze_embeddings=0, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='CNN1dClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:329 -                cache() ] Loading vectors from glove.6B.300d.txt\n",
            "100% 399709/400000 [00:34<00:00, 11865.85it/s][INFO | vocab.py:381 -                cache() ] Saving vectors to .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 0: True\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 367,115 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] CNN1dClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(300, 64, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(300, 64, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 27]), torch.Size([64, 64, 25]), torch.Size([64, 64, 23])]\n",
            "\n",
            " 11% 1/9 [00:00<00:02,  2.70it/s]\u001b[A[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 34]), torch.Size([64, 64, 32]), torch.Size([64, 64, 30])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "100% 9/9 [00:00<00:00, 21.34it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 447.54it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.495 | Train Acc: 18.62%\n",
            "\t Val. Loss: 1.955 |  Val. Acc: 27.22%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 31]), torch.Size([64, 64, 29]), torch.Size([64, 64, 27])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "100% 9/9 [00:00<00:00, 156.88it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 433.70it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.045 | Train Acc: 28.34%\n",
            "\t Val. Loss: 1.756 |  Val. Acc: 44.07%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "100% 9/9 [00:00<00:00, 159.77it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 465.00it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.905 | Train Acc: 32.09%\n",
            "\t Val. Loss: 1.630 |  Val. Acc: 45.42%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "100% 9/9 [00:00<00:00, 154.90it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 432.80it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.842 | Train Acc: 34.65%\n",
            "\t Val. Loss: 1.551 |  Val. Acc: 51.25%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "100% 9/9 [00:00<00:00, 156.71it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 472.28it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.594 | Train Acc: 44.91%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 50.11%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "100% 9/9 [00:00<00:00, 158.39it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 472.65it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.532 | Train Acc: 48.06%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 57.28%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "100% 9/9 [00:00<00:00, 167.35it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 465.36it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.405 | Train Acc: 48.82%\n",
            "\t Val. Loss: 1.357 |  Val. Acc: 60.98%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "100% 9/9 [00:00<00:00, 159.17it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 467.25it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.371 | Train Acc: 53.16%\n",
            "\t Val. Loss: 1.274 |  Val. Acc: 63.68%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "100% 9/9 [00:00<00:00, 164.84it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 433.16it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.254 | Train Acc: 59.58%\n",
            "\t Val. Loss: 1.259 |  Val. Acc: 61.76%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "100% 9/9 [00:00<00:00, 158.94it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 465.57it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.117 | Train Acc: 63.68%\n",
            "\t Val. Loss: 1.167 |  Val. Acc: 64.10%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "100% 9/9 [00:00<00:00, 154.43it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 464.64it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.087 | Train Acc: 63.60%\n",
            "\t Val. Loss: 1.079 |  Val. Acc: 68.58%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "100% 9/9 [00:00<00:00, 158.42it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 408.80it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.035 | Train Acc: 67.20%\n",
            "\t Val. Loss: 1.044 |  Val. Acc: 68.37%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "100% 9/9 [00:00<00:00, 152.93it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 422.00it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.953 | Train Acc: 68.28%\n",
            "\t Val. Loss: 0.982 |  Val. Acc: 69.36%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "100% 9/9 [00:00<00:00, 165.96it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 462.85it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.857 | Train Acc: 74.53%\n",
            "\t Val. Loss: 0.957 |  Val. Acc: 69.36%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "100% 9/9 [00:00<00:00, 140.14it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 457.94it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.813 | Train Acc: 76.20%\n",
            "\t Val. Loss: 0.864 |  Val. Acc: 70.92%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "100% 9/9 [00:00<00:00, 165.18it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 471.54it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.753 | Train Acc: 75.33%\n",
            "\t Val. Loss: 0.813 |  Val. Acc: 74.41%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "100% 9/9 [00:00<00:00, 161.92it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 472.15it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.757 | Train Acc: 77.70%\n",
            "\t Val. Loss: 0.769 |  Val. Acc: 75.19%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "100% 9/9 [00:00<00:00, 165.39it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 472.86it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.616 | Train Acc: 82.47%\n",
            "\t Val. Loss: 0.729 |  Val. Acc: 78.10%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "100% 9/9 [00:00<00:00, 160.54it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 469.82it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.668 | Train Acc: 78.48%\n",
            "\t Val. Loss: 0.700 |  Val. Acc: 78.10%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "100% 9/9 [00:00<00:00, 167.79it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 461.57it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.656 | Train Acc: 79.54%\n",
            "\t Val. Loss: 0.679 |  Val. Acc: 80.24%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 34]), torch.Size([64, 64, 32]), torch.Size([64, 64, 30])]\n",
            "100% 9/9 [00:00<00:00, 163.16it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 375.92it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.545 | Train Acc: 85.12%\n",
            "\t Val. Loss: 0.625 |  Val. Acc: 81.02%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "100% 9/9 [00:00<00:00, 160.08it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 471.46it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.526 | Train Acc: 85.64%\n",
            "\t Val. Loss: 0.577 |  Val. Acc: 81.02%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "100% 9/9 [00:00<00:00, 162.33it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 445.37it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.461 | Train Acc: 86.90%\n",
            "\t Val. Loss: 0.568 |  Val. Acc: 82.37%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 25]), torch.Size([64, 64, 23]), torch.Size([64, 64, 21])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "100% 9/9 [00:00<00:00, 161.92it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 473.69it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.473 | Train Acc: 86.98%\n",
            "\t Val. Loss: 0.560 |  Val. Acc: 81.59%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 24]), torch.Size([64, 64, 22]), torch.Size([64, 64, 20])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 27]), torch.Size([64, 64, 25]), torch.Size([64, 64, 23])]\n",
            "[torch.Size([64, 64, 35]), torch.Size([64, 64, 33]), torch.Size([64, 64, 31])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "100% 9/9 [00:00<00:00, 154.98it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 466.86it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.473 | Train Acc: 86.11%\n",
            "\t Val. Loss: 0.536 |  Val. Acc: 85.07%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 21]), torch.Size([64, 64, 19]), torch.Size([64, 64, 17])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 36]), torch.Size([64, 64, 34]), torch.Size([64, 64, 32])]\n",
            "100% 9/9 [00:00<00:00, 161.98it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 468.90it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.422 | Train Acc: 87.98%\n",
            "\t Val. Loss: 0.509 |  Val. Acc: 85.07%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 40]), torch.Size([64, 64, 38]), torch.Size([64, 64, 36])]\n",
            "[torch.Size([64, 64, 28]), torch.Size([64, 64, 26]), torch.Size([64, 64, 24])]\n",
            "[torch.Size([64, 64, 32]), torch.Size([64, 64, 30]), torch.Size([64, 64, 28])]\n",
            "[torch.Size([64, 64, 22]), torch.Size([64, 64, 20]), torch.Size([64, 64, 18])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "[torch.Size([64, 64, 34]), torch.Size([64, 64, 32]), torch.Size([64, 64, 30])]\n",
            "100% 9/9 [00:00<00:00, 167.68it/s]\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([37, 64, 43]), torch.Size([37, 64, 41]), torch.Size([37, 64, 39])]\n",
            "100% 2/2 [00:00<00:00, 445.26it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.411 | Train Acc: 88.18%\n",
            "\t Val. Loss: 0.487 |  Val. Acc: 86.42%\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A[torch.Size([64, 64, 30]), torch.Size([64, 64, 28]), torch.Size([64, 64, 26])]\n",
            "[torch.Size([64, 64, 33]), torch.Size([64, 64, 31]), torch.Size([64, 64, 29])]\n",
            "[torch.Size([64, 64, 26]), torch.Size([64, 64, 24]), torch.Size([64, 64, 22])]\n",
            "[torch.Size([64, 64, 23]), torch.Size([64, 64, 21]), torch.Size([64, 64, 19])]\n",
            "[torch.Size([57, 64, 55]), torch.Size([57, 64, 53]), torch.Size([57, 64, 51])]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 363, in <module>\n",
            "  File \"/content/QuestionGenerator/classifier/helperfunctions.py\", line 18, in train\n",
            "    for batch in tqdm(iterator, total=len(iterator)):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/std.py\", line 1091, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
            "    yield Batch(minibatch, self.dataset, self.device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\", line 34, in __init__\n",
            "    setattr(self, name, field.process(batch, device=device))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\", line 201, in process\n",
            "    tensor = self.numericalize(padded, device=device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\", line 296, in numericalize\n",
            "    lengths = torch.tensor(lengths, dtype=self.dtype, device=device)\n",
            "KeyboardInterrupt\n",
            "\n",
            "100% 399709/400000 [00:48<00:00, 8246.96it/s] \n",
            "  0% 0/9 [00:00<?, ?it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSwiNN0VrRt4",
        "colab_type": "text"
      },
      "source": [
        "### CNN1dExtraLayerClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_EQkEynrcAL",
        "colab_type": "code",
        "outputId": "2957026e-b083-4f87-a688-f438f3fcf5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50 -m CNN1dExtraLayerClassifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='CNN1dExtraLayerClassifier', model_location=None, n_layers=1, seed=1234, tag='multi')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:83 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:99 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:107 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 199,115 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] CNN1dExtraLayerClassifier(\n",
            "  (embedding): Embedding(640, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (1): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (2): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    )\n",
            "  )\n",
            "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "  0% 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "100% 9/9 [00:00<00:00, 125.55it/s]\n",
            "100% 2/2 [00:00<00:00, 300.66it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.289 | Train Acc: 17.25%\n",
            "\t Val. Loss: 2.063 |  Val. Acc: 28.15%\n",
            "100% 9/9 [00:00<00:00, 148.31it/s]\n",
            "100% 2/2 [00:00<00:00, 283.10it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.116 | Train Acc: 23.74%\n",
            "\t Val. Loss: 1.961 |  Val. Acc: 38.66%\n",
            "100% 9/9 [00:00<00:00, 132.74it/s]\n",
            "100% 2/2 [00:00<00:00, 260.54it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.984 | Train Acc: 31.94%\n",
            "\t Val. Loss: 1.869 |  Val. Acc: 36.89%\n",
            "100% 9/9 [00:00<00:00, 145.38it/s]\n",
            "100% 2/2 [00:00<00:00, 297.00it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.940 | Train Acc: 27.97%\n",
            "\t Val. Loss: 1.770 |  Val. Acc: 42.86%\n",
            "100% 9/9 [00:00<00:00, 145.81it/s]\n",
            "100% 2/2 [00:00<00:00, 272.03it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.838 | Train Acc: 36.30%\n",
            "\t Val. Loss: 1.737 |  Val. Acc: 49.68%\n",
            "100% 9/9 [00:00<00:00, 145.64it/s]\n",
            "100% 2/2 [00:00<00:00, 300.77it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.747 | Train Acc: 39.27%\n",
            "\t Val. Loss: 1.677 |  Val. Acc: 46.77%\n",
            "100% 9/9 [00:00<00:00, 129.38it/s]\n",
            "100% 2/2 [00:00<00:00, 227.70it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.714 | Train Acc: 39.75%\n",
            "\t Val. Loss: 1.635 |  Val. Acc: 53.38%\n",
            "100% 9/9 [00:00<00:00, 145.92it/s]\n",
            "100% 2/2 [00:00<00:00, 293.32it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.635 | Train Acc: 45.74%\n",
            "\t Val. Loss: 1.547 |  Val. Acc: 51.82%\n",
            "100% 9/9 [00:00<00:00, 143.79it/s]\n",
            "100% 2/2 [00:00<00:00, 292.98it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.518 | Train Acc: 52.31%\n",
            "\t Val. Loss: 1.505 |  Val. Acc: 59.99%\n",
            "100% 9/9 [00:00<00:00, 141.85it/s]\n",
            "100% 2/2 [00:00<00:00, 297.61it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.458 | Train Acc: 56.16%\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 59.99%\n",
            "100% 9/9 [00:00<00:00, 144.49it/s]\n",
            "100% 2/2 [00:00<00:00, 303.33it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.360 | Train Acc: 59.52%\n",
            "\t Val. Loss: 1.381 |  Val. Acc: 59.99%\n",
            "100% 9/9 [00:00<00:00, 148.95it/s]\n",
            "100% 2/2 [00:00<00:00, 302.61it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.277 | Train Acc: 62.97%\n",
            "\t Val. Loss: 1.348 |  Val. Acc: 60.77%\n",
            "100% 9/9 [00:00<00:00, 136.96it/s]\n",
            "100% 2/2 [00:00<00:00, 214.66it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.196 | Train Acc: 66.27%\n",
            "\t Val. Loss: 1.255 |  Val. Acc: 62.33%\n",
            "100% 9/9 [00:00<00:00, 144.47it/s]\n",
            "100% 2/2 [00:00<00:00, 293.29it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.081 | Train Acc: 66.70%\n",
            "\t Val. Loss: 1.210 |  Val. Acc: 62.33%\n",
            "100% 9/9 [00:00<00:00, 131.71it/s]\n",
            "100% 2/2 [00:00<00:00, 292.88it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.031 | Train Acc: 68.87%\n",
            "\t Val. Loss: 1.149 |  Val. Acc: 64.46%\n",
            "100% 9/9 [00:00<00:00, 136.29it/s]\n",
            "100% 2/2 [00:00<00:00, 273.02it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.956 | Train Acc: 71.91%\n",
            "\t Val. Loss: 1.102 |  Val. Acc: 63.68%\n",
            "100% 9/9 [00:00<00:00, 142.48it/s]\n",
            "100% 2/2 [00:00<00:00, 303.30it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.889 | Train Acc: 75.64%\n",
            "\t Val. Loss: 1.061 |  Val. Acc: 68.16%\n",
            "100% 9/9 [00:00<00:00, 133.66it/s]\n",
            "100% 2/2 [00:00<00:00, 300.37it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.812 | Train Acc: 77.53%\n",
            "\t Val. Loss: 1.018 |  Val. Acc: 66.60%\n",
            "100% 9/9 [00:00<00:00, 136.10it/s]\n",
            "100% 2/2 [00:00<00:00, 268.73it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.759 | Train Acc: 78.91%\n",
            "\t Val. Loss: 0.982 |  Val. Acc: 68.94%\n",
            "100% 9/9 [00:00<00:00, 144.93it/s]\n",
            "100% 2/2 [00:00<00:00, 295.82it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.709 | Train Acc: 82.80%\n",
            "\t Val. Loss: 0.933 |  Val. Acc: 67.38%\n",
            "100% 9/9 [00:00<00:00, 143.18it/s]\n",
            "100% 2/2 [00:00<00:00, 286.49it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.650 | Train Acc: 82.93%\n",
            "\t Val. Loss: 0.894 |  Val. Acc: 68.94%\n",
            "100% 9/9 [00:00<00:00, 130.48it/s]\n",
            "100% 2/2 [00:00<00:00, 298.48it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.609 | Train Acc: 85.01%\n",
            "\t Val. Loss: 0.876 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 131.83it/s]\n",
            "100% 2/2 [00:00<00:00, 296.10it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.579 | Train Acc: 84.55%\n",
            "\t Val. Loss: 0.841 |  Val. Acc: 68.94%\n",
            "100% 9/9 [00:00<00:00, 147.46it/s]\n",
            "100% 2/2 [00:00<00:00, 302.17it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.522 | Train Acc: 87.57%\n",
            "\t Val. Loss: 0.814 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 140.24it/s]\n",
            "100% 2/2 [00:00<00:00, 300.83it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.478 | Train Acc: 88.09%\n",
            "\t Val. Loss: 0.786 |  Val. Acc: 72.85%\n",
            "100% 9/9 [00:00<00:00, 142.70it/s]\n",
            "100% 2/2 [00:00<00:00, 254.63it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.495 | Train Acc: 86.74%\n",
            "\t Val. Loss: 0.765 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 139.94it/s]\n",
            "100% 2/2 [00:00<00:00, 292.29it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.430 | Train Acc: 90.06%\n",
            "\t Val. Loss: 0.755 |  Val. Acc: 71.28%\n",
            "100% 9/9 [00:00<00:00, 137.69it/s]\n",
            "100% 2/2 [00:00<00:00, 292.55it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.421 | Train Acc: 90.02%\n",
            "\t Val. Loss: 0.736 |  Val. Acc: 72.85%\n",
            "100% 9/9 [00:00<00:00, 142.94it/s]\n",
            "100% 2/2 [00:00<00:00, 195.19it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.396 | Train Acc: 89.74%\n",
            "\t Val. Loss: 0.727 |  Val. Acc: 72.07%\n",
            "100% 9/9 [00:00<00:00, 148.36it/s]\n",
            "100% 2/2 [00:00<00:00, 289.92it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.355 | Train Acc: 92.19%\n",
            "\t Val. Loss: 0.713 |  Val. Acc: 72.85%\n",
            "100% 9/9 [00:00<00:00, 143.81it/s]\n",
            "100% 2/2 [00:00<00:00, 264.62it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.353 | Train Acc: 91.45%\n",
            "\t Val. Loss: 0.718 |  Val. Acc: 71.28%\n",
            "100% 9/9 [00:00<00:00, 146.51it/s]\n",
            "100% 2/2 [00:00<00:00, 270.04it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.341 | Train Acc: 92.54%\n",
            "\t Val. Loss: 0.701 |  Val. Acc: 72.85%\n",
            "100% 9/9 [00:00<00:00, 132.69it/s]\n",
            "100% 2/2 [00:00<00:00, 311.31it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.325 | Train Acc: 93.27%\n",
            "\t Val. Loss: 0.663 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 147.32it/s]\n",
            "100% 2/2 [00:00<00:00, 307.80it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train Acc: 94.16%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 148.40it/s]\n",
            "100% 2/2 [00:00<00:00, 310.29it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train Acc: 94.36%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 72.07%\n",
            "100% 9/9 [00:00<00:00, 142.26it/s]\n",
            "100% 2/2 [00:00<00:00, 305.11it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train Acc: 94.49%\n",
            "\t Val. Loss: 0.691 |  Val. Acc: 72.85%\n",
            "100% 9/9 [00:00<00:00, 148.96it/s]\n",
            "100% 2/2 [00:00<00:00, 297.10it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train Acc: 95.81%\n",
            "\t Val. Loss: 0.684 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 150.57it/s]\n",
            "100% 2/2 [00:00<00:00, 309.59it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train Acc: 95.25%\n",
            "\t Val. Loss: 0.648 |  Val. Acc: 76.33%\n",
            "100% 9/9 [00:00<00:00, 134.91it/s]\n",
            "100% 2/2 [00:00<00:00, 316.89it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train Acc: 95.53%\n",
            "\t Val. Loss: 0.651 |  Val. Acc: 75.55%\n",
            "100% 9/9 [00:00<00:00, 134.84it/s]\n",
            "100% 2/2 [00:00<00:00, 310.64it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train Acc: 94.40%\n",
            "\t Val. Loss: 0.666 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 143.89it/s]\n",
            "100% 2/2 [00:00<00:00, 311.74it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train Acc: 96.29%\n",
            "\t Val. Loss: 0.645 |  Val. Acc: 78.25%\n",
            "100% 9/9 [00:00<00:00, 135.54it/s]\n",
            "100% 2/2 [00:00<00:00, 301.92it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train Acc: 96.14%\n",
            "\t Val. Loss: 0.672 |  Val. Acc: 74.77%\n",
            "100% 9/9 [00:00<00:00, 149.99it/s]\n",
            "100% 2/2 [00:00<00:00, 301.23it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train Acc: 97.31%\n",
            "\t Val. Loss: 0.668 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 133.83it/s]\n",
            "100% 2/2 [00:00<00:00, 294.94it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train Acc: 96.46%\n",
            "\t Val. Loss: 0.637 |  Val. Acc: 76.12%\n",
            "100% 9/9 [00:00<00:00, 146.82it/s]\n",
            "100% 2/2 [00:00<00:00, 313.41it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train Acc: 95.44%\n",
            "\t Val. Loss: 0.659 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 143.95it/s]\n",
            "100% 2/2 [00:00<00:00, 290.01it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train Acc: 96.66%\n",
            "\t Val. Loss: 0.624 |  Val. Acc: 75.55%\n",
            "100% 9/9 [00:00<00:00, 150.89it/s]\n",
            "100% 2/2 [00:00<00:00, 306.51it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train Acc: 96.81%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 73.42%\n",
            "100% 9/9 [00:00<00:00, 147.71it/s]\n",
            "100% 2/2 [00:00<00:00, 297.29it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train Acc: 96.10%\n",
            "\t Val. Loss: 0.638 |  Val. Acc: 74.77%\n",
            "100% 9/9 [00:00<00:00, 147.00it/s]\n",
            "100% 2/2 [00:00<00:00, 300.29it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.142 | Train Acc: 98.39%\n",
            "\t Val. Loss: 0.629 |  Val. Acc: 76.90%\n",
            "100% 9/9 [00:00<00:00, 151.54it/s]\n",
            "100% 2/2 [00:00<00:00, 306.79it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.164 | Train Acc: 96.44%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 76.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqw9ZXEu0_kY",
        "colab_type": "text"
      },
      "source": [
        "# Classification Based on Only Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSZLyqt_1bvu",
        "colab_type": "text"
      },
      "source": [
        "## RNN Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To_4q_3a1d1X",
        "colab_type": "text"
      },
      "source": [
        "### RNNHiddenClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emVJTfar1ftY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09928d9e-5919-4633-fd4c-39d5a2b6877f"
      },
      "source": [
        "!python train.py -n 50 --tag answeronly"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234, tag='answeronly')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:161 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:172 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:180 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 443,147 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] RNNHiddenClassifier(\n",
            "  (embedding): Embedding(741, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 142.94it/s]\n",
            "100% 2/2 [00:00<00:00, 380.78it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.374 | Train Acc: 12.56%\n",
            "\t Val. Loss: 2.265 |  Val. Acc: 17.27%\n",
            "100% 9/9 [00:00<00:00, 146.50it/s]\n",
            "100% 2/2 [00:00<00:00, 273.63it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.294 | Train Acc: 16.75%\n",
            "\t Val. Loss: 2.233 |  Val. Acc: 17.48%\n",
            "100% 9/9 [00:00<00:00, 186.10it/s]\n",
            "100% 2/2 [00:00<00:00, 376.56it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.232 | Train Acc: 19.16%\n",
            "\t Val. Loss: 2.269 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 159.00it/s]\n",
            "100% 2/2 [00:00<00:00, 356.29it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.253 | Train Acc: 18.34%\n",
            "\t Val. Loss: 2.234 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 175.28it/s]\n",
            "100% 2/2 [00:00<00:00, 380.30it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.228 | Train Acc: 19.55%\n",
            "\t Val. Loss: 2.227 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 179.69it/s]\n",
            "100% 2/2 [00:00<00:00, 377.20it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.194 | Train Acc: 20.36%\n",
            "\t Val. Loss: 2.219 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 181.78it/s]\n",
            "100% 2/2 [00:00<00:00, 336.93it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.221 | Train Acc: 21.09%\n",
            "\t Val. Loss: 2.215 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 177.93it/s]\n",
            "100% 2/2 [00:00<00:00, 393.37it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.199 | Train Acc: 21.98%\n",
            "\t Val. Loss: 2.138 |  Val. Acc: 24.87%\n",
            "100% 9/9 [00:00<00:00, 184.52it/s]\n",
            "100% 2/2 [00:00<00:00, 373.76it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.162 | Train Acc: 24.95%\n",
            "\t Val. Loss: 2.096 |  Val. Acc: 24.09%\n",
            "100% 9/9 [00:00<00:00, 160.60it/s]\n",
            "100% 2/2 [00:00<00:00, 379.78it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.110 | Train Acc: 26.60%\n",
            "\t Val. Loss: 2.073 |  Val. Acc: 18.26%\n",
            "100% 9/9 [00:00<00:00, 177.02it/s]\n",
            "100% 2/2 [00:00<00:00, 376.19it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.075 | Train Acc: 27.58%\n",
            "\t Val. Loss: 1.963 |  Val. Acc: 29.35%\n",
            "100% 9/9 [00:00<00:00, 180.02it/s]\n",
            "100% 2/2 [00:00<00:00, 258.52it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.986 | Train Acc: 31.48%\n",
            "\t Val. Loss: 1.859 |  Val. Acc: 32.62%\n",
            "100% 9/9 [00:00<00:00, 187.75it/s]\n",
            "100% 2/2 [00:00<00:00, 393.11it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.875 | Train Acc: 35.54%\n",
            "\t Val. Loss: 1.773 |  Val. Acc: 28.36%\n",
            "100% 9/9 [00:00<00:00, 187.00it/s]\n",
            "100% 2/2 [00:00<00:00, 408.78it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.734 | Train Acc: 37.82%\n",
            "\t Val. Loss: 1.613 |  Val. Acc: 35.90%\n",
            "100% 9/9 [00:00<00:00, 191.22it/s]\n",
            "100% 2/2 [00:00<00:00, 355.13it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.584 | Train Acc: 44.85%\n",
            "\t Val. Loss: 1.593 |  Val. Acc: 34.54%\n",
            "100% 9/9 [00:00<00:00, 187.03it/s]\n",
            "100% 2/2 [00:00<00:00, 388.70it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.494 | Train Acc: 46.41%\n",
            "\t Val. Loss: 1.575 |  Val. Acc: 28.57%\n",
            "100% 9/9 [00:00<00:00, 177.76it/s]\n",
            "100% 2/2 [00:00<00:00, 397.92it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.425 | Train Acc: 47.26%\n",
            "\t Val. Loss: 1.226 |  Val. Acc: 45.48%\n",
            "100% 9/9 [00:00<00:00, 190.68it/s]\n",
            "100% 2/2 [00:00<00:00, 358.96it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.268 | Train Acc: 51.83%\n",
            "\t Val. Loss: 1.238 |  Val. Acc: 44.70%\n",
            "100% 9/9 [00:00<00:00, 191.08it/s]\n",
            "100% 2/2 [00:00<00:00, 386.82it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.202 | Train Acc: 57.19%\n",
            "\t Val. Loss: 1.246 |  Val. Acc: 46.41%\n",
            "100% 9/9 [00:00<00:00, 189.05it/s]\n",
            "100% 2/2 [00:00<00:00, 382.87it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.134 | Train Acc: 58.13%\n",
            "\t Val. Loss: 1.041 |  Val. Acc: 56.78%\n",
            "100% 9/9 [00:00<00:00, 181.70it/s]\n",
            "100% 2/2 [00:00<00:00, 351.87it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.189 | Train Acc: 57.67%\n",
            "\t Val. Loss: 1.111 |  Val. Acc: 50.53%\n",
            "100% 9/9 [00:00<00:00, 173.09it/s]\n",
            "100% 2/2 [00:00<00:00, 397.92it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.055 | Train Acc: 61.34%\n",
            "\t Val. Loss: 0.885 |  Val. Acc: 59.27%\n",
            "100% 9/9 [00:00<00:00, 170.67it/s]\n",
            "100% 2/2 [00:00<00:00, 332.91it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.979 | Train Acc: 62.62%\n",
            "\t Val. Loss: 0.894 |  Val. Acc: 60.05%\n",
            "100% 9/9 [00:00<00:00, 182.19it/s]\n",
            "100% 2/2 [00:00<00:00, 386.25it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.904 | Train Acc: 66.63%\n",
            "\t Val. Loss: 0.902 |  Val. Acc: 59.06%\n",
            "100% 9/9 [00:00<00:00, 179.40it/s]\n",
            "100% 2/2 [00:00<00:00, 347.70it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.840 | Train Acc: 72.71%\n",
            "\t Val. Loss: 0.818 |  Val. Acc: 61.40%\n",
            "100% 9/9 [00:00<00:00, 166.54it/s]\n",
            "100% 2/2 [00:00<00:00, 385.95it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.784 | Train Acc: 72.75%\n",
            "\t Val. Loss: 0.789 |  Val. Acc: 61.04%\n",
            "100% 9/9 [00:00<00:00, 170.37it/s]\n",
            "100% 2/2 [00:00<00:00, 388.45it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.767 | Train Acc: 72.66%\n",
            "\t Val. Loss: 0.739 |  Val. Acc: 65.10%\n",
            "100% 9/9 [00:00<00:00, 178.10it/s]\n",
            "100% 2/2 [00:00<00:00, 394.39it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.716 | Train Acc: 76.05%\n",
            "\t Val. Loss: 0.732 |  Val. Acc: 62.96%\n",
            "100% 9/9 [00:00<00:00, 187.83it/s]\n",
            "100% 2/2 [00:00<00:00, 381.20it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.710 | Train Acc: 75.57%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 65.88%\n",
            "100% 9/9 [00:00<00:00, 184.32it/s]\n",
            "100% 2/2 [00:00<00:00, 398.89it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.665 | Train Acc: 75.98%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 173.39it/s]\n",
            "100% 2/2 [00:00<00:00, 388.54it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.608 | Train Acc: 78.15%\n",
            "\t Val. Loss: 0.703 |  Val. Acc: 69.15%\n",
            "100% 9/9 [00:00<00:00, 193.42it/s]\n",
            "100% 2/2 [00:00<00:00, 372.23it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.635 | Train Acc: 77.48%\n",
            "\t Val. Loss: 0.616 |  Val. Acc: 74.20%\n",
            "100% 9/9 [00:00<00:00, 187.25it/s]\n",
            "100% 2/2 [00:00<00:00, 401.93it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.578 | Train Acc: 79.67%\n",
            "\t Val. Loss: 0.599 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 183.43it/s]\n",
            "100% 2/2 [00:00<00:00, 365.69it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.581 | Train Acc: 78.85%\n",
            "\t Val. Loss: 0.655 |  Val. Acc: 70.50%\n",
            "100% 9/9 [00:00<00:00, 184.68it/s]\n",
            "100% 2/2 [00:00<00:00, 390.62it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.561 | Train Acc: 80.24%\n",
            "\t Val. Loss: 0.546 |  Val. Acc: 78.82%\n",
            "100% 9/9 [00:00<00:00, 165.36it/s]\n",
            "100% 2/2 [00:00<00:00, 380.18it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.523 | Train Acc: 81.17%\n",
            "\t Val. Loss: 0.555 |  Val. Acc: 77.11%\n",
            "100% 9/9 [00:00<00:00, 181.96it/s]\n",
            "100% 2/2 [00:00<00:00, 310.02it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.575 | Train Acc: 80.00%\n",
            "\t Val. Loss: 0.561 |  Val. Acc: 77.11%\n",
            "100% 9/9 [00:00<00:00, 182.71it/s]\n",
            "100% 2/2 [00:00<00:00, 381.23it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.546 | Train Acc: 79.78%\n",
            "\t Val. Loss: 0.563 |  Val. Acc: 76.12%\n",
            "100% 9/9 [00:00<00:00, 157.79it/s]\n",
            "100% 2/2 [00:00<00:00, 352.52it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.513 | Train Acc: 81.38%\n",
            "\t Val. Loss: 0.573 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 180.74it/s]\n",
            "100% 2/2 [00:00<00:00, 365.01it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.483 | Train Acc: 83.16%\n",
            "\t Val. Loss: 0.543 |  Val. Acc: 78.46%\n",
            "100% 9/9 [00:00<00:00, 183.49it/s]\n",
            "100% 2/2 [00:00<00:00, 371.11it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.516 | Train Acc: 80.62%\n",
            "\t Val. Loss: 0.551 |  Val. Acc: 76.33%\n",
            "100% 9/9 [00:00<00:00, 160.31it/s]\n",
            "100% 2/2 [00:00<00:00, 398.32it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.476 | Train Acc: 82.56%\n",
            "\t Val. Loss: 0.557 |  Val. Acc: 72.07%\n",
            "100% 9/9 [00:00<00:00, 187.72it/s]\n",
            "100% 2/2 [00:00<00:00, 226.81it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.466 | Train Acc: 83.95%\n",
            "\t Val. Loss: 0.556 |  Val. Acc: 79.24%\n",
            "100% 9/9 [00:00<00:00, 155.01it/s]\n",
            "100% 2/2 [00:00<00:00, 346.14it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.452 | Train Acc: 83.60%\n",
            "\t Val. Loss: 0.622 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 174.61it/s]\n",
            "100% 2/2 [00:00<00:00, 393.20it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.410 | Train Acc: 85.03%\n",
            "\t Val. Loss: 0.587 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 185.27it/s]\n",
            "100% 2/2 [00:00<00:00, 375.73it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.409 | Train Acc: 85.44%\n",
            "\t Val. Loss: 0.577 |  Val. Acc: 73.42%\n",
            "100% 9/9 [00:00<00:00, 191.89it/s]\n",
            "100% 2/2 [00:00<00:00, 390.10it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.384 | Train Acc: 87.24%\n",
            "\t Val. Loss: 0.558 |  Val. Acc: 74.98%\n",
            "100% 9/9 [00:00<00:00, 192.97it/s]\n",
            "100% 2/2 [00:00<00:00, 372.31it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.413 | Train Acc: 84.99%\n",
            "\t Val. Loss: 0.585 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 187.57it/s]\n",
            "100% 2/2 [00:00<00:00, 382.88it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.365 | Train Acc: 87.48%\n",
            "\t Val. Loss: 0.538 |  Val. Acc: 73.42%\n",
            "100% 9/9 [00:00<00:00, 182.74it/s]\n",
            "100% 2/2 [00:00<00:00, 345.11it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.344 | Train Acc: 88.37%\n",
            "\t Val. Loss: 0.507 |  Val. Acc: 77.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajPd7tDZ31w-",
        "colab_type": "text"
      },
      "source": [
        "### RNNMaxPoolClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7czcNgnW33zJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b7eacf5-d17f-470a-f5c1-c1b7c8fde96f"
      },
      "source": [
        "!python train.py -n 50 -m RNNMaxpoolClassifier  --tag answeronly"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNMaxpoolClassifier', model_location=None, n_layers=1, seed=1234, tag='answeronly')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:161 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:172 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:180 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 441,739 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] RNNMaxpoolClassifier(\n",
            "  (embedding): Embedding(741, 300, padding_idx=1)\n",
            "  (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 151.74it/s]\n",
            "100% 2/2 [00:00<00:00, 381.47it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.350 | Train Acc: 11.95%\n",
            "\t Val. Loss: 2.282 |  Val. Acc: 13.79%\n",
            "100% 9/9 [00:00<00:00, 201.18it/s]\n",
            "100% 2/2 [00:00<00:00, 421.16it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.247 | Train Acc: 18.53%\n",
            "\t Val. Loss: 2.239 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 203.89it/s]\n",
            "100% 2/2 [00:00<00:00, 415.67it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.215 | Train Acc: 21.07%\n",
            "\t Val. Loss: 2.269 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 178.76it/s]\n",
            "100% 2/2 [00:00<00:00, 382.10it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.203 | Train Acc: 17.64%\n",
            "\t Val. Loss: 2.240 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 196.92it/s]\n",
            "100% 2/2 [00:00<00:00, 380.83it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.190 | Train Acc: 19.14%\n",
            "\t Val. Loss: 2.218 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 196.90it/s]\n",
            "100% 2/2 [00:00<00:00, 393.31it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.162 | Train Acc: 22.81%\n",
            "\t Val. Loss: 2.191 |  Val. Acc: 7.81%\n",
            "100% 9/9 [00:00<00:00, 197.90it/s]\n",
            "100% 2/2 [00:00<00:00, 407.43it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.151 | Train Acc: 20.88%\n",
            "\t Val. Loss: 2.160 |  Val. Acc: 8.59%\n",
            "100% 9/9 [00:00<00:00, 145.70it/s]\n",
            "100% 2/2 [00:00<00:00, 334.94it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.105 | Train Acc: 28.58%\n",
            "\t Val. Loss: 2.031 |  Val. Acc: 30.70%\n",
            "100% 9/9 [00:00<00:00, 187.21it/s]\n",
            "100% 2/2 [00:00<00:00, 390.55it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.042 | Train Acc: 33.37%\n",
            "\t Val. Loss: 1.928 |  Val. Acc: 37.88%\n",
            "100% 9/9 [00:00<00:00, 187.63it/s]\n",
            "100% 2/2 [00:00<00:00, 409.74it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.954 | Train Acc: 35.57%\n",
            "\t Val. Loss: 1.836 |  Val. Acc: 40.22%\n",
            "100% 9/9 [00:00<00:00, 191.85it/s]\n",
            "100% 2/2 [00:00<00:00, 423.47it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.791 | Train Acc: 43.31%\n",
            "\t Val. Loss: 1.671 |  Val. Acc: 40.01%\n",
            "100% 9/9 [00:00<00:00, 198.87it/s]\n",
            "100% 2/2 [00:00<00:00, 390.29it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.654 | Train Acc: 45.72%\n",
            "\t Val. Loss: 1.494 |  Val. Acc: 36.95%\n",
            "100% 9/9 [00:00<00:00, 169.47it/s]\n",
            "100% 2/2 [00:00<00:00, 392.27it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.487 | Train Acc: 49.03%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 43.71%\n",
            "100% 9/9 [00:00<00:00, 196.27it/s]\n",
            "100% 2/2 [00:00<00:00, 322.78it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.314 | Train Acc: 54.26%\n",
            "\t Val. Loss: 1.217 |  Val. Acc: 46.47%\n",
            "100% 9/9 [00:00<00:00, 195.57it/s]\n",
            "100% 2/2 [00:00<00:00, 396.91it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.220 | Train Acc: 60.86%\n",
            "\t Val. Loss: 1.144 |  Val. Acc: 53.23%\n",
            "100% 9/9 [00:00<00:00, 191.57it/s]\n",
            "100% 2/2 [00:00<00:00, 407.65it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.077 | Train Acc: 66.09%\n",
            "\t Val. Loss: 1.086 |  Val. Acc: 49.18%\n",
            "100% 9/9 [00:00<00:00, 203.47it/s]\n",
            "100% 2/2 [00:00<00:00, 378.56it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.996 | Train Acc: 65.31%\n",
            "\t Val. Loss: 1.006 |  Val. Acc: 52.87%\n",
            "100% 9/9 [00:00<00:00, 196.36it/s]\n",
            "100% 2/2 [00:00<00:00, 403.07it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.908 | Train Acc: 69.23%\n",
            "\t Val. Loss: 0.967 |  Val. Acc: 57.71%\n",
            "100% 9/9 [00:00<00:00, 198.32it/s]\n",
            "100% 2/2 [00:00<00:00, 396.19it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.855 | Train Acc: 70.69%\n",
            "\t Val. Loss: 0.908 |  Val. Acc: 60.83%\n",
            "100% 9/9 [00:00<00:00, 197.89it/s]\n",
            "100% 2/2 [00:00<00:00, 269.80it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.808 | Train Acc: 71.32%\n",
            "\t Val. Loss: 0.850 |  Val. Acc: 67.02%\n",
            "100% 9/9 [00:00<00:00, 197.99it/s]\n",
            "100% 2/2 [00:00<00:00, 403.63it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.740 | Train Acc: 75.01%\n",
            "\t Val. Loss: 0.896 |  Val. Acc: 63.75%\n",
            "100% 9/9 [00:00<00:00, 191.05it/s]\n",
            "100% 2/2 [00:00<00:00, 395.26it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.695 | Train Acc: 78.72%\n",
            "\t Val. Loss: 0.822 |  Val. Acc: 62.75%\n",
            "100% 9/9 [00:00<00:00, 194.35it/s]\n",
            "100% 2/2 [00:00<00:00, 387.88it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.611 | Train Acc: 79.82%\n",
            "\t Val. Loss: 0.754 |  Val. Acc: 67.44%\n",
            "100% 9/9 [00:00<00:00, 180.35it/s]\n",
            "100% 2/2 [00:00<00:00, 362.80it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.606 | Train Acc: 77.59%\n",
            "\t Val. Loss: 0.769 |  Val. Acc: 68.58%\n",
            "100% 9/9 [00:00<00:00, 197.61it/s]\n",
            "100% 2/2 [00:00<00:00, 387.41it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.590 | Train Acc: 79.30%\n",
            "\t Val. Loss: 0.702 |  Val. Acc: 65.67%\n",
            "100% 9/9 [00:00<00:00, 194.32it/s]\n",
            "100% 2/2 [00:00<00:00, 407.67it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.557 | Train Acc: 80.97%\n",
            "\t Val. Loss: 0.725 |  Val. Acc: 69.15%\n",
            "100% 9/9 [00:00<00:00, 185.74it/s]\n",
            "100% 2/2 [00:00<00:00, 340.50it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.542 | Train Acc: 82.64%\n",
            "\t Val. Loss: 0.765 |  Val. Acc: 67.02%\n",
            "100% 9/9 [00:00<00:00, 204.70it/s]\n",
            "100% 2/2 [00:00<00:00, 403.69it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.452 | Train Acc: 87.50%\n",
            "\t Val. Loss: 0.664 |  Val. Acc: 70.14%\n",
            "100% 9/9 [00:00<00:00, 177.93it/s]\n",
            "100% 2/2 [00:00<00:00, 304.43it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.448 | Train Acc: 85.66%\n",
            "\t Val. Loss: 0.717 |  Val. Acc: 69.57%\n",
            "100% 9/9 [00:00<00:00, 191.46it/s]\n",
            "100% 2/2 [00:00<00:00, 368.21it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.442 | Train Acc: 84.75%\n",
            "\t Val. Loss: 0.654 |  Val. Acc: 69.36%\n",
            "100% 9/9 [00:00<00:00, 176.87it/s]\n",
            "100% 2/2 [00:00<00:00, 373.21it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.417 | Train Acc: 86.81%\n",
            "\t Val. Loss: 0.654 |  Val. Acc: 72.49%\n",
            "100% 9/9 [00:00<00:00, 181.23it/s]\n",
            "100% 2/2 [00:00<00:00, 409.88it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.395 | Train Acc: 87.50%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 199.50it/s]\n",
            "100% 2/2 [00:00<00:00, 418.11it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.389 | Train Acc: 86.40%\n",
            "\t Val. Loss: 0.588 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 194.77it/s]\n",
            "100% 2/2 [00:00<00:00, 409.58it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.346 | Train Acc: 89.65%\n",
            "\t Val. Loss: 0.652 |  Val. Acc: 71.71%\n",
            "100% 9/9 [00:00<00:00, 209.36it/s]\n",
            "100% 2/2 [00:00<00:00, 410.84it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.336 | Train Acc: 90.39%\n",
            "\t Val. Loss: 0.583 |  Val. Acc: 73.42%\n",
            "100% 9/9 [00:00<00:00, 197.28it/s]\n",
            "100% 2/2 [00:00<00:00, 402.70it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.340 | Train Acc: 88.07%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 194.15it/s]\n",
            "100% 2/2 [00:00<00:00, 418.51it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.355 | Train Acc: 88.22%\n",
            "\t Val. Loss: 0.604 |  Val. Acc: 70.14%\n",
            "100% 9/9 [00:00<00:00, 206.56it/s]\n",
            "100% 2/2 [00:00<00:00, 427.51it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.317 | Train Acc: 90.15%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 72.49%\n",
            "100% 9/9 [00:00<00:00, 206.99it/s]\n",
            "100% 2/2 [00:00<00:00, 391.57it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.318 | Train Acc: 91.02%\n",
            "\t Val. Loss: 0.566 |  Val. Acc: 74.20%\n",
            "100% 9/9 [00:00<00:00, 201.02it/s]\n",
            "100% 2/2 [00:00<00:00, 422.69it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train Acc: 91.95%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 76.33%\n",
            "100% 9/9 [00:00<00:00, 199.15it/s]\n",
            "100% 2/2 [00:00<00:00, 396.44it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train Acc: 91.91%\n",
            "\t Val. Loss: 0.588 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 200.52it/s]\n",
            "100% 2/2 [00:00<00:00, 396.10it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.280 | Train Acc: 90.28%\n",
            "\t Val. Loss: 0.571 |  Val. Acc: 74.98%\n",
            "100% 9/9 [00:00<00:00, 197.83it/s]\n",
            "100% 2/2 [00:00<00:00, 403.34it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train Acc: 92.64%\n",
            "\t Val. Loss: 0.596 |  Val. Acc: 76.75%\n",
            "100% 9/9 [00:00<00:00, 199.95it/s]\n",
            "100% 2/2 [00:00<00:00, 409.04it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train Acc: 93.88%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 195.16it/s]\n",
            "100% 2/2 [00:00<00:00, 245.18it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train Acc: 92.99%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 193.40it/s]\n",
            "100% 2/2 [00:00<00:00, 395.41it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train Acc: 90.91%\n",
            "\t Val. Loss: 0.657 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 188.06it/s]\n",
            "100% 2/2 [00:00<00:00, 400.35it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train Acc: 93.32%\n",
            "\t Val. Loss: 0.788 |  Val. Acc: 73.63%\n",
            "100% 9/9 [00:00<00:00, 175.68it/s]\n",
            "100% 2/2 [00:00<00:00, 429.44it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train Acc: 89.50%\n",
            "\t Val. Loss: 0.784 |  Val. Acc: 70.14%\n",
            "100% 9/9 [00:00<00:00, 203.15it/s]\n",
            "100% 2/2 [00:00<00:00, 382.64it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.300 | Train Acc: 90.87%\n",
            "\t Val. Loss: 0.655 |  Val. Acc: 72.28%\n",
            "100% 9/9 [00:00<00:00, 201.16it/s]\n",
            "100% 2/2 [00:00<00:00, 413.35it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train Acc: 92.13%\n",
            "\t Val. Loss: 0.572 |  Val. Acc: 78.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYfsqqd23O8A",
        "colab_type": "text"
      },
      "source": [
        "## CNN Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKX9aNsh3RUN",
        "colab_type": "text"
      },
      "source": [
        "### CNN1DClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6vpLJj3UpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d122957-713b-4825-b922-9a42977a63b7"
      },
      "source": [
        "!python train.py -n 50 -m CNN1dClassifier --tag answeronly"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='CNN1dClassifier', model_location=None, n_layers=1, seed=1234, tag='answeronly')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:161 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:172 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:180 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 175,115 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] CNN1dClassifier(\n",
            "  (embedding): Embedding(741, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 64, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(300, 64, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(300, 64, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 207.93it/s]\n",
            "100% 2/2 [00:00<00:00, 544.36it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.412 | Train Acc: 14.45%\n",
            "\t Val. Loss: 2.157 |  Val. Acc: 14.21%\n",
            "100% 9/9 [00:00<00:00, 268.68it/s]\n",
            "100% 2/2 [00:00<00:00, 562.24it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.118 | Train Acc: 27.36%\n",
            "\t Val. Loss: 2.086 |  Val. Acc: 21.54%\n",
            "100% 9/9 [00:00<00:00, 261.69it/s]\n",
            "100% 2/2 [00:00<00:00, 574.29it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.866 | Train Acc: 36.54%\n",
            "\t Val. Loss: 1.972 |  Val. Acc: 23.10%\n",
            "100% 9/9 [00:00<00:00, 271.55it/s]\n",
            "100% 2/2 [00:00<00:00, 562.31it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.732 | Train Acc: 42.16%\n",
            "\t Val. Loss: 1.859 |  Val. Acc: 36.32%\n",
            "100% 9/9 [00:00<00:00, 240.99it/s]\n",
            "100% 2/2 [00:00<00:00, 480.39it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.575 | Train Acc: 50.19%\n",
            "\t Val. Loss: 1.789 |  Val. Acc: 36.53%\n",
            "100% 9/9 [00:00<00:00, 265.29it/s]\n",
            "100% 2/2 [00:00<00:00, 565.73it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.435 | Train Acc: 55.00%\n",
            "\t Val. Loss: 1.690 |  Val. Acc: 38.30%\n",
            "100% 9/9 [00:00<00:00, 269.67it/s]\n",
            "100% 2/2 [00:00<00:00, 547.13it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.289 | Train Acc: 60.60%\n",
            "\t Val. Loss: 1.555 |  Val. Acc: 48.40%\n",
            "100% 9/9 [00:00<00:00, 274.36it/s]\n",
            "100% 2/2 [00:00<00:00, 573.42it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.183 | Train Acc: 64.40%\n",
            "\t Val. Loss: 1.510 |  Val. Acc: 46.05%\n",
            "100% 9/9 [00:00<00:00, 275.72it/s]\n",
            "100% 2/2 [00:00<00:00, 507.29it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.060 | Train Acc: 67.83%\n",
            "\t Val. Loss: 1.419 |  Val. Acc: 45.90%\n",
            "100% 9/9 [00:00<00:00, 276.03it/s]\n",
            "100% 2/2 [00:00<00:00, 598.29it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.992 | Train Acc: 68.72%\n",
            "\t Val. Loss: 1.297 |  Val. Acc: 48.61%\n",
            "100% 9/9 [00:00<00:00, 259.95it/s]\n",
            "100% 2/2 [00:00<00:00, 568.60it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.884 | Train Acc: 72.99%\n",
            "\t Val. Loss: 1.233 |  Val. Acc: 61.97%\n",
            "100% 9/9 [00:00<00:00, 247.51it/s]\n",
            "100% 2/2 [00:00<00:00, 550.65it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.819 | Train Acc: 75.57%\n",
            "\t Val. Loss: 1.146 |  Val. Acc: 61.40%\n",
            "100% 9/9 [00:00<00:00, 256.75it/s]\n",
            "100% 2/2 [00:00<00:00, 602.41it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.719 | Train Acc: 80.02%\n",
            "\t Val. Loss: 1.091 |  Val. Acc: 63.75%\n",
            "100% 9/9 [00:00<00:00, 280.13it/s]\n",
            "100% 2/2 [00:00<00:00, 594.43it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.698 | Train Acc: 80.82%\n",
            "\t Val. Loss: 1.034 |  Val. Acc: 61.61%\n",
            "100% 9/9 [00:00<00:00, 275.21it/s]\n",
            "100% 2/2 [00:00<00:00, 637.14it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.638 | Train Acc: 82.58%\n",
            "\t Val. Loss: 0.976 |  Val. Acc: 65.88%\n",
            "100% 9/9 [00:00<00:00, 278.01it/s]\n",
            "100% 2/2 [00:00<00:00, 576.70it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.593 | Train Acc: 82.73%\n",
            "\t Val. Loss: 1.031 |  Val. Acc: 64.32%\n",
            "100% 9/9 [00:00<00:00, 247.42it/s]\n",
            "100% 2/2 [00:00<00:00, 603.97it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.586 | Train Acc: 80.74%\n",
            "\t Val. Loss: 0.941 |  Val. Acc: 66.87%\n",
            "100% 9/9 [00:00<00:00, 277.35it/s]\n",
            "100% 2/2 [00:00<00:00, 584.45it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.509 | Train Acc: 87.13%\n",
            "\t Val. Loss: 0.933 |  Val. Acc: 70.14%\n",
            "100% 9/9 [00:00<00:00, 261.92it/s]\n",
            "100% 2/2 [00:00<00:00, 592.75it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.483 | Train Acc: 86.85%\n",
            "\t Val. Loss: 0.911 |  Val. Acc: 68.01%\n",
            "100% 9/9 [00:00<00:00, 250.32it/s]\n",
            "100% 2/2 [00:00<00:00, 584.78it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.485 | Train Acc: 84.73%\n",
            "\t Val. Loss: 0.851 |  Val. Acc: 71.71%\n",
            "100% 9/9 [00:00<00:00, 242.57it/s]\n",
            "100% 2/2 [00:00<00:00, 590.83it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.423 | Train Acc: 87.48%\n",
            "\t Val. Loss: 0.829 |  Val. Acc: 68.79%\n",
            "100% 9/9 [00:00<00:00, 283.78it/s]\n",
            "100% 2/2 [00:00<00:00, 619.50it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Acc: 89.72%\n",
            "\t Val. Loss: 0.779 |  Val. Acc: 69.57%\n",
            "100% 9/9 [00:00<00:00, 252.78it/s]\n",
            "100% 2/2 [00:00<00:00, 610.35it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.422 | Train Acc: 87.76%\n",
            "\t Val. Loss: 0.778 |  Val. Acc: 70.92%\n",
            "100% 9/9 [00:00<00:00, 271.03it/s]\n",
            "100% 2/2 [00:00<00:00, 572.68it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.363 | Train Acc: 90.04%\n",
            "\t Val. Loss: 0.822 |  Val. Acc: 70.14%\n",
            "100% 9/9 [00:00<00:00, 281.26it/s]\n",
            "100% 2/2 [00:00<00:00, 598.84it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.354 | Train Acc: 91.32%\n",
            "\t Val. Loss: 0.745 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 280.83it/s]\n",
            "100% 2/2 [00:00<00:00, 582.22it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.341 | Train Acc: 90.93%\n",
            "\t Val. Loss: 0.756 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 267.38it/s]\n",
            "100% 2/2 [00:00<00:00, 565.12it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train Acc: 91.74%\n",
            "\t Val. Loss: 0.743 |  Val. Acc: 72.28%\n",
            "100% 9/9 [00:00<00:00, 263.78it/s]\n",
            "100% 2/2 [00:00<00:00, 614.46it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.294 | Train Acc: 93.08%\n",
            "\t Val. Loss: 0.723 |  Val. Acc: 70.35%\n",
            "100% 9/9 [00:00<00:00, 277.43it/s]\n",
            "100% 2/2 [00:00<00:00, 614.42it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train Acc: 92.41%\n",
            "\t Val. Loss: 0.701 |  Val. Acc: 70.35%\n",
            "100% 9/9 [00:00<00:00, 273.07it/s]\n",
            "100% 2/2 [00:00<00:00, 592.12it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.296 | Train Acc: 92.99%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 70.35%\n",
            "100% 9/9 [00:00<00:00, 264.37it/s]\n",
            "100% 2/2 [00:00<00:00, 542.81it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train Acc: 93.41%\n",
            "\t Val. Loss: 0.670 |  Val. Acc: 71.71%\n",
            "100% 9/9 [00:00<00:00, 247.75it/s]\n",
            "100% 2/2 [00:00<00:00, 441.65it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train Acc: 94.43%\n",
            "\t Val. Loss: 0.655 |  Val. Acc: 76.54%\n",
            "100% 9/9 [00:00<00:00, 256.08it/s]\n",
            "100% 2/2 [00:00<00:00, 610.70it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train Acc: 93.21%\n",
            "\t Val. Loss: 0.680 |  Val. Acc: 72.28%\n",
            "100% 9/9 [00:00<00:00, 277.60it/s]\n",
            "100% 2/2 [00:00<00:00, 633.77it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train Acc: 94.90%\n",
            "\t Val. Loss: 0.656 |  Val. Acc: 71.71%\n",
            "100% 9/9 [00:00<00:00, 277.01it/s]\n",
            "100% 2/2 [00:00<00:00, 604.80it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train Acc: 93.95%\n",
            "\t Val. Loss: 0.659 |  Val. Acc: 74.62%\n",
            "100% 9/9 [00:00<00:00, 282.86it/s]\n",
            "100% 2/2 [00:00<00:00, 608.31it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train Acc: 94.30%\n",
            "\t Val. Loss: 0.659 |  Val. Acc: 70.35%\n",
            "100% 9/9 [00:00<00:00, 268.42it/s]\n",
            "100% 2/2 [00:00<00:00, 612.31it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train Acc: 95.90%\n",
            "\t Val. Loss: 0.631 |  Val. Acc: 72.49%\n",
            "100% 9/9 [00:00<00:00, 275.74it/s]\n",
            "100% 2/2 [00:00<00:00, 327.31it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train Acc: 96.27%\n",
            "\t Val. Loss: 0.634 |  Val. Acc: 73.84%\n",
            "100% 9/9 [00:00<00:00, 280.56it/s]\n",
            "100% 2/2 [00:00<00:00, 641.92it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train Acc: 95.10%\n",
            "\t Val. Loss: 0.635 |  Val. Acc: 73.84%\n",
            "100% 9/9 [00:00<00:00, 269.96it/s]\n",
            "100% 2/2 [00:00<00:00, 430.38it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train Acc: 94.69%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 251.30it/s]\n",
            "100% 2/2 [00:00<00:00, 592.54it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train Acc: 96.14%\n",
            "\t Val. Loss: 0.585 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 235.85it/s]\n",
            "100% 2/2 [00:00<00:00, 625.55it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train Acc: 95.62%\n",
            "\t Val. Loss: 0.580 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 278.30it/s]\n",
            "100% 2/2 [00:00<00:00, 638.99it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.139 | Train Acc: 96.44%\n",
            "\t Val. Loss: 0.587 |  Val. Acc: 75.97%\n",
            "100% 9/9 [00:00<00:00, 262.57it/s]\n",
            "100% 2/2 [00:00<00:00, 565.69it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.154 | Train Acc: 97.37%\n",
            "\t Val. Loss: 0.587 |  Val. Acc: 73.84%\n",
            "100% 9/9 [00:00<00:00, 288.55it/s]\n",
            "100% 2/2 [00:00<00:00, 610.92it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.157 | Train Acc: 96.14%\n",
            "\t Val. Loss: 0.558 |  Val. Acc: 77.32%\n",
            "100% 9/9 [00:00<00:00, 286.29it/s]\n",
            "100% 2/2 [00:00<00:00, 661.67it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train Acc: 93.93%\n",
            "\t Val. Loss: 0.588 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 275.94it/s]\n",
            "100% 2/2 [00:00<00:00, 593.00it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.164 | Train Acc: 97.01%\n",
            "\t Val. Loss: 0.583 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 282.32it/s]\n",
            "100% 2/2 [00:00<00:00, 669.86it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train Acc: 97.37%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 270.98it/s]\n",
            "100% 2/2 [00:00<00:00, 424.87it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.144 | Train Acc: 96.81%\n",
            "\t Val. Loss: 0.594 |  Val. Acc: 73.06%\n",
            "100% 9/9 [00:00<00:00, 274.08it/s]\n",
            "100% 2/2 [00:00<00:00, 652.30it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.125 | Train Acc: 98.24%\n",
            "\t Val. Loss: 0.571 |  Val. Acc: 73.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYeUeHX33hB2",
        "colab_type": "text"
      },
      "source": [
        "### CNN1dExtraLayerClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAuOuMR43kjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a824f0f0-8cc5-42c3-de36-689de24978fd"
      },
      "source": [
        "!python train.py -n 50 -m CNN1dExtraLayerClassifier --tag answeronly"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:307 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='CNN1dExtraLayerClassifier', model_location=None, n_layers=1, seed=1234, tag='answeronly')\n",
            "[DEBUG | train.py:308 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:310 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:161 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:386 -                cache() ] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:172 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:180 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:317 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:73 - initialize_new_model() ] Initializing Model\n",
            "[DEBUG | train.py:162 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:168 - initialize_new_model() ] Model Initialized with 199,115 trainiable parameters\n",
            "[DEBUG | train.py:180 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "[INFO | train.py:343 -             <module>() ] CNN1dExtraLayerClassifier(\n",
            "  (embedding): Embedding(741, 300, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (1): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    )\n",
            "    (2): CustomConv1d(\n",
            "      (convlayer): Conv1d(300, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    )\n",
            "  )\n",
            "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "100% 9/9 [00:00<00:00, 158.73it/s]\n",
            "100% 2/2 [00:00<00:00, 443.54it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.333 | Train Acc: 16.10%\n",
            "\t Val. Loss: 2.252 |  Val. Acc: 9.16%\n",
            "100% 9/9 [00:00<00:00, 227.95it/s]\n",
            "100% 2/2 [00:00<00:00, 465.26it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.186 | Train Acc: 22.68%\n",
            "\t Val. Loss: 2.190 |  Val. Acc: 9.16%\n",
            "100% 9/9 [00:00<00:00, 220.38it/s]\n",
            "100% 2/2 [00:00<00:00, 446.39it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.125 | Train Acc: 27.15%\n",
            "\t Val. Loss: 2.141 |  Val. Acc: 10.16%\n",
            "100% 9/9 [00:00<00:00, 226.07it/s]\n",
            "100% 2/2 [00:00<00:00, 475.71it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 2.010 | Train Acc: 30.18%\n",
            "\t Val. Loss: 2.047 |  Val. Acc: 28.93%\n",
            "100% 9/9 [00:00<00:00, 213.65it/s]\n",
            "100% 2/2 [00:00<00:00, 470.61it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.878 | Train Acc: 35.91%\n",
            "\t Val. Loss: 1.983 |  Val. Acc: 27.79%\n",
            "100% 9/9 [00:00<00:00, 215.14it/s]\n",
            "100% 2/2 [00:00<00:00, 483.55it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.763 | Train Acc: 41.62%\n",
            "\t Val. Loss: 1.862 |  Val. Acc: 34.04%\n",
            "100% 9/9 [00:00<00:00, 230.08it/s]\n",
            "100% 2/2 [00:00<00:00, 482.35it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.649 | Train Acc: 43.05%\n",
            "\t Val. Loss: 1.694 |  Val. Acc: 49.96%\n",
            "100% 9/9 [00:00<00:00, 237.74it/s]\n",
            "100% 2/2 [00:00<00:00, 496.22it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.529 | Train Acc: 49.69%\n",
            "\t Val. Loss: 1.567 |  Val. Acc: 47.04%\n",
            "100% 9/9 [00:00<00:00, 237.37it/s]\n",
            "100% 2/2 [00:00<00:00, 475.17it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.321 | Train Acc: 57.46%\n",
            "\t Val. Loss: 1.417 |  Val. Acc: 52.66%\n",
            "100% 9/9 [00:00<00:00, 221.09it/s]\n",
            "100% 2/2 [00:00<00:00, 486.18it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 1.121 | Train Acc: 66.26%\n",
            "\t Val. Loss: 1.236 |  Val. Acc: 58.13%\n",
            "100% 9/9 [00:00<00:00, 213.20it/s]\n",
            "100% 2/2 [00:00<00:00, 485.54it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.961 | Train Acc: 72.45%\n",
            "\t Val. Loss: 1.114 |  Val. Acc: 60.83%\n",
            "100% 9/9 [00:00<00:00, 225.12it/s]\n",
            "100% 2/2 [00:00<00:00, 486.92it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.824 | Train Acc: 77.35%\n",
            "\t Val. Loss: 0.992 |  Val. Acc: 60.83%\n",
            "100% 9/9 [00:00<00:00, 238.39it/s]\n",
            "100% 2/2 [00:00<00:00, 494.38it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.690 | Train Acc: 80.95%\n",
            "\t Val. Loss: 0.912 |  Val. Acc: 61.61%\n",
            "100% 9/9 [00:00<00:00, 226.13it/s]\n",
            "100% 2/2 [00:00<00:00, 463.33it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.591 | Train Acc: 83.86%\n",
            "\t Val. Loss: 0.803 |  Val. Acc: 70.71%\n",
            "100% 9/9 [00:00<00:00, 204.43it/s]\n",
            "100% 2/2 [00:00<00:00, 350.07it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.495 | Train Acc: 86.77%\n",
            "\t Val. Loss: 0.766 |  Val. Acc: 58.13%\n",
            "100% 9/9 [00:00<00:00, 235.10it/s]\n",
            "100% 2/2 [00:00<00:00, 479.95it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.445 | Train Acc: 89.26%\n",
            "\t Val. Loss: 0.700 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 235.18it/s]\n",
            "100% 2/2 [00:00<00:00, 476.22it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.401 | Train Acc: 87.96%\n",
            "\t Val. Loss: 0.671 |  Val. Acc: 66.09%\n",
            "100% 9/9 [00:00<00:00, 240.09it/s]\n",
            "100% 2/2 [00:00<00:00, 487.65it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.342 | Train Acc: 91.08%\n",
            "\t Val. Loss: 0.611 |  Val. Acc: 74.41%\n",
            "100% 9/9 [00:00<00:00, 214.79it/s]\n",
            "100% 2/2 [00:00<00:00, 476.84it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.350 | Train Acc: 91.80%\n",
            "\t Val. Loss: 0.633 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 230.77it/s]\n",
            "100% 2/2 [00:00<00:00, 475.17it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.272 | Train Acc: 93.32%\n",
            "\t Val. Loss: 0.576 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 219.29it/s]\n",
            "100% 2/2 [00:00<00:00, 484.69it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train Acc: 93.69%\n",
            "\t Val. Loss: 0.527 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 237.06it/s]\n",
            "100% 2/2 [00:00<00:00, 499.44it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train Acc: 93.97%\n",
            "\t Val. Loss: 0.517 |  Val. Acc: 78.10%\n",
            "100% 9/9 [00:00<00:00, 237.34it/s]\n",
            "100% 2/2 [00:00<00:00, 493.97it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train Acc: 95.05%\n",
            "\t Val. Loss: 0.518 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 215.30it/s]\n",
            "100% 2/2 [00:00<00:00, 499.38it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train Acc: 94.88%\n",
            "\t Val. Loss: 0.500 |  Val. Acc: 76.75%\n",
            "100% 9/9 [00:00<00:00, 234.69it/s]\n",
            "100% 2/2 [00:00<00:00, 507.08it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train Acc: 94.69%\n",
            "\t Val. Loss: 0.487 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 236.43it/s]\n",
            "100% 2/2 [00:00<00:00, 504.70it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.161 | Train Acc: 96.98%\n",
            "\t Val. Loss: 0.501 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 239.01it/s]\n",
            "100% 2/2 [00:00<00:00, 498.64it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.150 | Train Acc: 95.71%\n",
            "\t Val. Loss: 0.456 |  Val. Acc: 78.67%\n",
            "100% 9/9 [00:00<00:00, 195.58it/s]\n",
            "100% 2/2 [00:00<00:00, 462.16it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.156 | Train Acc: 96.25%\n",
            "\t Val. Loss: 0.510 |  Val. Acc: 75.19%\n",
            "100% 9/9 [00:00<00:00, 218.85it/s]\n",
            "100% 2/2 [00:00<00:00, 458.82it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.154 | Train Acc: 96.51%\n",
            "\t Val. Loss: 0.465 |  Val. Acc: 80.60%\n",
            "100% 9/9 [00:00<00:00, 213.90it/s]\n",
            "100% 2/2 [00:00<00:00, 474.33it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.135 | Train Acc: 96.64%\n",
            "\t Val. Loss: 0.460 |  Val. Acc: 77.89%\n",
            "100% 9/9 [00:00<00:00, 238.23it/s]\n",
            "100% 2/2 [00:00<00:00, 512.50it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.126 | Train Acc: 97.51%\n",
            "\t Val. Loss: 0.472 |  Val. Acc: 75.76%\n",
            "100% 9/9 [00:00<00:00, 242.15it/s]\n",
            "100% 2/2 [00:00<00:00, 518.33it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.130 | Train Acc: 97.03%\n",
            "\t Val. Loss: 0.453 |  Val. Acc: 76.54%\n",
            "100% 9/9 [00:00<00:00, 236.69it/s]\n",
            "100% 2/2 [00:00<00:00, 503.19it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train Acc: 98.61%\n",
            "\t Val. Loss: 0.476 |  Val. Acc: 76.33%\n",
            "100% 9/9 [00:00<00:00, 230.54it/s]\n",
            "100% 2/2 [00:00<00:00, 485.51it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train Acc: 98.57%\n",
            "\t Val. Loss: 0.442 |  Val. Acc: 79.24%\n",
            "100% 9/9 [00:00<00:00, 232.44it/s]\n",
            "100% 2/2 [00:00<00:00, 525.67it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.111 | Train Acc: 97.20%\n",
            "\t Val. Loss: 0.420 |  Val. Acc: 80.60%\n",
            "100% 9/9 [00:00<00:00, 240.36it/s]\n",
            "100% 2/2 [00:00<00:00, 519.26it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train Acc: 98.74%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 86.21%\n",
            "100% 9/9 [00:00<00:00, 239.73it/s]\n",
            "100% 2/2 [00:00<00:00, 531.77it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train Acc: 98.44%\n",
            "\t Val. Loss: 0.452 |  Val. Acc: 76.54%\n",
            "100% 9/9 [00:00<00:00, 242.50it/s]\n",
            "100% 2/2 [00:00<00:00, 501.35it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train Acc: 98.22%\n",
            "\t Val. Loss: 0.444 |  Val. Acc: 82.73%\n",
            "100% 9/9 [00:00<00:00, 237.32it/s]\n",
            "100% 2/2 [00:00<00:00, 523.90it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train Acc: 97.72%\n",
            "\t Val. Loss: 0.421 |  Val. Acc: 80.03%\n",
            "100% 9/9 [00:00<00:00, 229.45it/s]\n",
            "100% 2/2 [00:00<00:00, 495.87it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train Acc: 98.61%\n",
            "\t Val. Loss: 0.420 |  Val. Acc: 84.29%\n",
            "100% 9/9 [00:00<00:00, 238.15it/s]\n",
            "100% 2/2 [00:00<00:00, 528.72it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train Acc: 98.18%\n",
            "\t Val. Loss: 0.518 |  Val. Acc: 74.98%\n",
            "100% 9/9 [00:00<00:00, 244.37it/s]\n",
            "100% 2/2 [00:00<00:00, 553.30it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train Acc: 98.07%\n",
            "\t Val. Loss: 0.395 |  Val. Acc: 81.38%\n",
            "100% 9/9 [00:00<00:00, 232.85it/s]\n",
            "100% 2/2 [00:00<00:00, 556.57it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train Acc: 98.96%\n",
            "\t Val. Loss: 0.397 |  Val. Acc: 81.38%\n",
            "100% 9/9 [00:00<00:00, 238.09it/s]\n",
            "100% 2/2 [00:00<00:00, 398.96it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train Acc: 98.44%\n",
            "\t Val. Loss: 0.418 |  Val. Acc: 79.24%\n",
            "100% 9/9 [00:00<00:00, 222.51it/s]\n",
            "100% 2/2 [00:00<00:00, 510.13it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train Acc: 98.96%\n",
            "\t Val. Loss: 0.422 |  Val. Acc: 77.89%\n",
            "100% 9/9 [00:00<00:00, 218.93it/s]\n",
            "100% 2/2 [00:00<00:00, 487.45it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.049 | Train Acc: 99.46%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 79.24%\n",
            "100% 9/9 [00:00<00:00, 219.91it/s]\n",
            "100% 2/2 [00:00<00:00, 511.10it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.048 | Train Acc: 99.13%\n",
            "\t Val. Loss: 0.395 |  Val. Acc: 85.64%\n",
            "100% 9/9 [00:00<00:00, 219.91it/s]\n",
            "100% 2/2 [00:00<00:00, 504.43it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.044 | Train Acc: 99.65%\n",
            "\t Val. Loss: 0.430 |  Val. Acc: 79.24%\n",
            "100% 9/9 [00:00<00:00, 219.32it/s]\n",
            "100% 2/2 [00:00<00:00, 501.08it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train Acc: 99.11%\n",
            "\t Val. Loss: 0.377 |  Val. Acc: 81.38%\n",
            "100% 9/9 [00:00<00:00, 219.62it/s]\n",
            "100% 2/2 [00:00<00:00, 499.62it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.050 | Train Acc: 99.26%\n",
            "\t Val. Loss: 0.410 |  Val. Acc: 81.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm70iQTraxZP",
        "colab_type": "text"
      },
      "source": [
        "# File Editor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBdj4zha63F",
        "colab_type": "text"
      },
      "source": [
        "## helperfunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn_oZuLhKqy",
        "colab_type": "code",
        "outputId": "fb5428e1-9880-49fe-8002-df29c275be29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile helperfunctions.py\n",
        "\"\"\"\n",
        "Helper Functions containing training and evaluation methods\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from utility import categorical_accuracy\n",
        "from config.root import device\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, dataset_tag):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths = get_batch_data(batch, dataset_tag)\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def get_batch_data(batch, dataset_tag):\n",
        "\n",
        "    if dataset_tag == \"multi\":\n",
        "        (question, question_len), (key, key_len), (answer, answer_len) = (\n",
        "            batch.question,\n",
        "            batch.key,\n",
        "            batch.answer,\n",
        "        )\n",
        "\n",
        "        text = torch.cat((question, key, answer), dim=0)\n",
        "        text_lengths = question_len + key_len + answer_len\n",
        "    else:\n",
        "\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "    return text, text_lengths\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, dataset_tag):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "            text, text_lengths = get_batch_data(batch, dataset_tag)\n",
        "\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def train_tag_model(model, iterator, optimizer, criterion, tag_field):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, text_lengths, tag = get_batch_data_and_tag(batch, tag_field)\n",
        "\n",
        "        predictions = model(text, text_lengths, tag).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def get_batch_data_and_tag(batch, tag_field):\n",
        "\n",
        "    (question, question_len), (key, key_len), (answer, answer_len) = (\n",
        "        batch.question,\n",
        "        batch.key,\n",
        "        batch.answer,\n",
        "    )\n",
        "\n",
        "    question_tag = torch.full_like(\n",
        "        question, tag_field.vocab.stoi[\"Q\"], dtype=torch.long, device=device\n",
        "    )\n",
        "    key_tag = torch.full_like(\n",
        "        key, tag_field.vocab.stoi[\"K\"], dtype=torch.long, device=device\n",
        "    )\n",
        "    answer_tag = torch.full_like(\n",
        "        answer, tag_field.vocab.stoi[\"A\"], dtype=torch.long, device=device\n",
        "    )\n",
        "\n",
        "    tag = torch.cat((question_tag, key_tag, answer_tag), dim=0)\n",
        "\n",
        "    text = torch.cat((question, key, answer), dim=0)\n",
        "\n",
        "    text_lengths = question_len + key_len + answer_len\n",
        "\n",
        "    return text, text_lengths, tag\n",
        "\n",
        "\n",
        "def evaluate_tag_model(model, iterator, criterion, tag_field):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in tqdm(iterator, total=len(iterator)):\n",
        "\n",
        "            text, text_lengths, tag = get_batch_data_and_tag(batch, tag_field)\n",
        "\n",
        "            predictions = model(text, text_lengths, tag).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting helperfunctions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t16HDjub6S2",
        "colab_type": "text"
      },
      "source": [
        "## RNNClassifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKaxVGQ_bKtH",
        "colab_type": "code",
        "outputId": "38139f79-1e73-4682-b9e1-7bde4edb4541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile model/RNNClassifiers.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CNN2dClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(\n",
        "                    in_channels=1,\n",
        "                    out_channels=n_filters,\n",
        "                    kernel_size=(fs, embedding_dim),\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "class CNN1dClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv1d(\n",
        "                    in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "class CNN1dExtraLayerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv1d(\n",
        "                    in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "\n",
        "        for conv in conved:\n",
        "            print(conv.shape)\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        for pool in pooled:\n",
        "            print(pool.shape)\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model/RNNClassifiers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyAm-Ih3mndd",
        "colab_type": "text"
      },
      "source": [
        "## CNNClassifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P2okybFbW2e",
        "colab_type": "code",
        "outputId": "79eaa6b4-628e-4e0e-e672-a880cebfa858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile model/CNNClassifiers.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from config.root import device\n",
        "\n",
        "\n",
        "class CNN2dClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(\n",
        "                    in_channels=1,\n",
        "                    out_channels=n_filters,\n",
        "                    kernel_size=(fs, embedding_dim),\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "class CNN1dClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv1d(\n",
        "                    in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "\n",
        "        print([conv.shape for conv in conved])\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "class CustomConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.convlayer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "        )\n",
        "\n",
        "    def forward(self, embedded):\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        post_conv = self.convlayer(embedded)\n",
        "\n",
        "        return post_conv.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "class CNN1dExtraLayerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        n_filters,\n",
        "        filter_sizes,\n",
        "        linear_hidden_dim,\n",
        "        output_dim,\n",
        "        dropout,\n",
        "        pad_idx,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                CustomConv1d(\n",
        "                    in_channels=embedding_dim,\n",
        "                    out_channels=n_filters,\n",
        "                    kernel_size=fs,\n",
        "                    padding=((fs - 1) // 2),\n",
        "                )\n",
        "                for fs in filter_sizes\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.hidden_layer = nn.Linear(len(filter_sizes) * n_filters, linear_hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(linear_hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_len):\n",
        "\n",
        "        text = text.permute(1, 0)\n",
        "        max_len = text.shape[1]\n",
        "        # print(max_len)\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # print(\"Embedding Size: {}\".format(embedded.shape))\n",
        "\n",
        "        conved = [conv(embedded) for conv in self.convs]\n",
        "\n",
        "        # print([conv.shape for conv in conved])\n",
        "\n",
        "        cnns = F.relu(torch.cat([conv for conv in conved], -1))\n",
        "        # print(\"Concat CNN shape: {}\".format(cnns.shape))\n",
        "\n",
        "        hidden_output = self.hidden_layer(cnns)\n",
        "\n",
        "        # print(hidden_output.shape)\n",
        "        mask = (\n",
        "            (\n",
        "                torch.arange(max_len, device=device).expand(len(text_len), max_len)\n",
        "                < text_len.unsqueeze(1)\n",
        "            )\n",
        "            .float()\n",
        "            .unsqueeze(2)\n",
        "        )\n",
        "\n",
        "        # print(mask.shape)\n",
        "\n",
        "        vectors = hidden_output - (1.0 - mask) * 1e23\n",
        "\n",
        "        # print(\"Vector shapes : {}\".format(vectors.shape))\n",
        "\n",
        "        # print(vectors[0, :, 0])\n",
        "\n",
        "        vec, _ = torch.max(vectors, dim=1)\n",
        "\n",
        "        # print(vec.shape)\n",
        "\n",
        "        cat = self.dropout(vec)\n",
        "\n",
        "        # print(cat.shape)\n",
        "\n",
        "        output = self.fc(cat)\n",
        "\n",
        "        # print(output.shape)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model/CNNClassifiers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL5nj2r3Xzh3",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mg_Daz3v1mb",
        "colab_type": "code",
        "outputId": "8371d8c3-156e-439e-bc6b-227ffe31490a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile config/hyperparameters.py\n",
        "\"\"\"\n",
        "Hyper Parameters of Model\n",
        "\"\"\"\n",
        "\n",
        "MAX_VOCAB = 10000\n",
        "BATCH_SIZE = 64\n",
        "# Keep it 300 since we are using glove 300d vectors\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "N_LAYERS = 1\n",
        "BIDIRECTION = True\n",
        "DROPOUT = 0.7\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "FREEZE_EMBEDDINGS = 1\n",
        "WEIGHT_DECAY = 0.001\n",
        "CNN_FILTER_SIZES = [1, 3, 5]\n",
        "CNN_N_FILTER = 64\n",
        "LINEAR_HIDDEN_DIM = 128\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config/hyperparameters.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_z50uYBaVkw",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYV_I1g-X9aM",
        "colab_type": "code",
        "outputId": "cba91e47-d288-48c7-9dbc-43389d79f7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Training script for the model\n",
        "\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from config.hyperparameters import (\n",
        "    BATCH_SIZE,\n",
        "    BIDIRECTION,\n",
        "    DROPOUT,\n",
        "    EMBEDDING_DIM,\n",
        "    EPOCHS,\n",
        "    FREEZE_EMBEDDINGS,\n",
        "    HIDDEN_DIM,\n",
        "    LR,\n",
        "    N_LAYERS,\n",
        "    WEIGHT_DECAY,\n",
        "    CNN_N_FILTER,\n",
        "    CNN_FILTER_SIZES,\n",
        "    LINEAR_HIDDEN_DIM,\n",
        ")\n",
        "from config.root import (\n",
        "    LOGGING_FORMAT,\n",
        "    LOGGING_LEVEL,\n",
        "    TRAINED_CLASSIFIER_FOLDER,\n",
        "    TRAINED_CLASSIFIER_RNNHIDDEN,\n",
        "    device,\n",
        "    seed_all,\n",
        "    SEED,\n",
        ")\n",
        "from datasetloader import GrammarDasetMultiTag, GrammarDasetAnswerTag\n",
        "from helperfunctions import evaluate, train, train_tag_model, evaluate_tag_model\n",
        "from model import (\n",
        "    RNNHiddenClassifier,\n",
        "    RNNMaxpoolClassifier,\n",
        "    CNN2dClassifier,\n",
        "    CNN1dClassifier,\n",
        "    RNNFieldClassifer,\n",
        "    CNN1dExtraLayerClassifier,\n",
        ")\n",
        "from utility import categorical_accuracy, epoch_time\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Method to count the number of parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def initialize_new_model(\n",
        "    classifier_type,\n",
        "    dataset,\n",
        "    embedding_dim,\n",
        "    hidden_dim,\n",
        "    n_layers,\n",
        "    bidirectional,\n",
        "    dropout,\n",
        "    freeze_embeddings,\n",
        "    dataset_tag,\n",
        "    linear_hidden_dim,\n",
        "):\n",
        "    \"\"\"Method to initialise new model, takes in dataset object and hyperparameters as parameter\"\"\"\n",
        "    logger.debug(\"Initializing Model\")\n",
        "    if dataset_tag == \"multi\":\n",
        "        VOCAB_SIZE = len(dataset.question.vocab)\n",
        "        PAD_IDX = dataset.question.vocab.stoi[dataset.question.pad_token]\n",
        "        pretrained_embeddings = dataset.question.vocab.vectors\n",
        "        UNK_IDX = dataset.question.vocab.stoi[dataset.question.unk_token]\n",
        "    else:\n",
        "        VOCAB_SIZE = len(dataset.text.vocab)\n",
        "        PAD_IDX = dataset.text.vocab.stoi[dataset.text.pad_token]\n",
        "        pretrained_embeddings = dataset.text.vocab.vectors\n",
        "        UNK_IDX = dataset.text.vocab.stoi[dataset.text.unk_token]\n",
        "\n",
        "    OUTPUT_LAYERS = len(dataset.label.vocab)\n",
        "\n",
        "    if classifier_type == \"RNNHiddenClassifier\":\n",
        "\n",
        "        model = RNNHiddenClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            OUTPUT_LAYERS,\n",
        "            n_layers,\n",
        "            bidirectional,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "\n",
        "    elif classifier_type == \"RNNMaxpoolClassifier\":\n",
        "        model = RNNMaxpoolClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            OUTPUT_LAYERS,\n",
        "            n_layers,\n",
        "            bidirectional,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "    elif classifier_type == \"CNN2dClassifier\":\n",
        "        model = CNN2dClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            CNN_N_FILTER,\n",
        "            CNN_FILTER_SIZES,\n",
        "            OUTPUT_LAYERS,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "    elif classifier_type == \"CNN1dClassifier\":\n",
        "        model = CNN1dClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            CNN_N_FILTER,\n",
        "            CNN_FILTER_SIZES,\n",
        "            OUTPUT_LAYERS,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "    elif classifier_type == \"RNNFieldClassifer\":\n",
        "        model = RNNFieldClassifer(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            OUTPUT_LAYERS,\n",
        "            n_layers,\n",
        "            bidirectional,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "            dataset.tags,\n",
        "        )\n",
        "    elif classifier_type == \"CNN1dExtraLayerClassifier\":\n",
        "        model = CNN1dExtraLayerClassifier(\n",
        "            VOCAB_SIZE,\n",
        "            embedding_dim,\n",
        "            CNN_N_FILTER,\n",
        "            CNN_FILTER_SIZES,\n",
        "            linear_hidden_dim,\n",
        "            OUTPUT_LAYERS,\n",
        "            dropout,\n",
        "            PAD_IDX,\n",
        "        )\n",
        "    else:\n",
        "        raise TypeError(\"Invalid Classifier selected\")\n",
        "\n",
        "    if freeze_embeddings:\n",
        "        model.embedding.weight.requires_grad = False\n",
        "\n",
        "    logger.debug(\n",
        "        \"Freeze Embeddings Value {}: {}\".format(\n",
        "            freeze_embeddings, model.embedding.weight.requires_grad\n",
        "        )\n",
        "    )\n",
        "\n",
        "    logger.info(\n",
        "        \"Model Initialized with {:,} trainiable parameters\".format(\n",
        "            count_parameters(model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Initialize pretrained word embeddings\n",
        "\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "    # Initialize Padding and Unknown as 0\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
        "\n",
        "    logger.debug(\"Copied PreTrained Embeddings\")\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Utility to train the Model\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-s\",\n",
        "        \"--seed\",\n",
        "        default=SEED,\n",
        "        help=\"Set custom seed for reproducibility\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-loc\",\n",
        "        \"--model-location\",\n",
        "        default=None,\n",
        "        help=\"Give an already trained model location to use and train more epochs on it\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-b\",\n",
        "        \"--bidirectional\",\n",
        "        default=BIDIRECTION,\n",
        "        help=\"Makes the model Bidirectional\",\n",
        "        type=bool,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-d\",\n",
        "        \"--dropout\",\n",
        "        default=DROPOUT,\n",
        "        help=\"Dropout count for the model\",\n",
        "        type=float,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-e\",\n",
        "        \"--embedding-dim\",\n",
        "        default=EMBEDDING_DIM,\n",
        "        help=\"Embedding Dimensions\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-hd\",\n",
        "        \"--hidden-dim\",\n",
        "        default=HIDDEN_DIM,\n",
        "        help=\"Hidden dimensions of the RNN\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-l\", \"--n-layers\", default=N_LAYERS, help=\"Number of layers in RNN\", type=int\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-lr\",\n",
        "        \"--learning-rate\",\n",
        "        default=LR,\n",
        "        help=\"Learning rate of Adam Optimizer\",\n",
        "        type=float,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-n\",\n",
        "        \"--epochs\",\n",
        "        default=EPOCHS,\n",
        "        help=\"Number of Epochs to train model\",\n",
        "        type=int,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-batch\",\n",
        "        \"--batch_size\",\n",
        "        default=BATCH_SIZE,\n",
        "        help=\"Number of Epochs to train model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--freeze-embeddings\",\n",
        "        default=FREEZE_EMBEDDINGS,\n",
        "        help=\"Freeze Embeddings of Model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-t\",\n",
        "        \"--tag\",\n",
        "        default=\"multi\",\n",
        "        choices=[\"multi\", \"answeronly\"],\n",
        "        help=\"Use two different dataset type, multi type and single type where all are merged into same key \",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-l2\",\n",
        "        \"--l2-regularization\",\n",
        "        default=WEIGHT_DECAY,\n",
        "        help=\"Value of alpha in l2 regularization 0 means no regularization \",\n",
        "        type=float,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-m\",\n",
        "        \"--model\",\n",
        "        default=\"RNNHiddenClassifier\",\n",
        "        choices=[\n",
        "            \"RNNHiddenClassifier\",\n",
        "            \"RNNMaxpoolClassifier\",\n",
        "            \"RNNFieldClassifier\",\n",
        "            \"CNN2dClassifier\",\n",
        "            \"CNN1dClassifier\",\n",
        "            \"RNNFieldClassifer\",\n",
        "            \"CNN1dExtraLayerClassifier\",\n",
        "        ],\n",
        "        help=\"select the classifier to train on\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-lhd\",\n",
        "        \"--linear-hidden-dim\",\n",
        "        default=LINEAR_HIDDEN_DIM,\n",
        "        help=\"Freeze Embeddings of Model\",\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    seed_all(args.seed)\n",
        "    logger.debug(args)\n",
        "    logger.debug(\"Custom seed set with: {}\".format(args.seed))\n",
        "\n",
        "    logger.info(\"Loading Dataset\")\n",
        "\n",
        "    if args.tag == \"multi\":\n",
        "        dataset = GrammarDasetMultiTag.get_iterators(args.batch_size)\n",
        "    else:\n",
        "        dataset = GrammarDasetAnswerTag.get_iterators(args.batch_size)\n",
        "\n",
        "    logger.info(\"Dataset Loaded Successfully\")\n",
        "\n",
        "    if args.model_location:\n",
        "        model = torch.load(args.model_location)\n",
        "    else:\n",
        "        model = initialize_new_model(\n",
        "            args.model,\n",
        "            dataset,\n",
        "            args.embedding_dim,\n",
        "            args.hidden_dim,\n",
        "            args.n_layers,\n",
        "            args.bidirectional,\n",
        "            args.dropout,\n",
        "            args.freeze_embeddings,\n",
        "            args.tag,\n",
        "            args.linear_hidden_dim,\n",
        "        )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LR, weight_decay=args.l2_regularization\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    logger.info(model)\n",
        "\n",
        "    if not os.path.exists(TRAINED_CLASSIFIER_FOLDER):\n",
        "        os.mkdir(TRAINED_CLASSIFIER_FOLDER)\n",
        "\n",
        "    best_test_loss = float(\"inf\")\n",
        "    \n",
        "    for epoch in range(int(args.epochs)):\n",
        "\n",
        "        start_time = time.time()\n",
        "        if args.model == \"RNNFieldClassifer\":\n",
        "            train_loss, train_acc = train_tag_model(\n",
        "                model, dataset.train_iterator, optimizer, criterion, dataset.tags\n",
        "            )\n",
        "            test_loss, test_acc = evaluate_tag_model(\n",
        "                model, dataset.test_iterator, criterion, dataset.tags\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            train_loss, train_acc = train(\n",
        "                model, dataset.train_iterator, optimizer, criterion, args.tag\n",
        "            )\n",
        "            test_loss, test_acc = evaluate(\n",
        "                model, dataset.test_iterator, criterion, args.tag\n",
        "            )\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if test_loss < best_test_loss:\n",
        "            best_test_loss = test_loss\n",
        "            torch.save(\n",
        "                model,\n",
        "                os.path.join(TRAINED_CLASSIFIER_FOLDER, TRAINED_CLASSIFIER_RNNHIDDEN),\n",
        "            )\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
        "        print(f\"\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcxueuUu2zQ6",
        "colab_type": "text"
      },
      "source": [
        "## datasetloader.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOOETS4faZbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee56d6fc-b7c8-418d-d111-3a2d76966f22"
      },
      "source": [
        "%%writefile datasetloader.py\n",
        "\"\"\"\n",
        "Load Dataset\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext import data, datasets\n",
        "\n",
        "from config.data import (\n",
        "    DATASET_FOLDER,\n",
        "    PROCESSED_DATASET,\n",
        "    PROCESSED_DATASET_FOLDER,\n",
        "    PROCESSED_DATASET_TRAIN_FILENAME,\n",
        "    PROCESSED_DATASET_TEST_FILENAME,\n",
        "    TEMP_DIR,\n",
        ")\n",
        "from config.hyperparameters import BATCH_SIZE, MAX_VOCAB\n",
        "from config.root import LOGGING_FORMAT, LOGGING_LEVEL, device\n",
        "from utility import tokenizer\n",
        "\n",
        "# Initialize logger for this file\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=LOGGING_LEVEL, format=LOGGING_FORMAT)\n",
        "\n",
        "\n",
        "class GrammarDasetMultiTag:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.dataset_location = PROCESSED_DATASET\n",
        "\n",
        "        self.question = data.Field(\n",
        "            tokenize=tokenizer, include_lengths=True, eos_token=\"</q>\", init_token=\"<q>\"\n",
        "        )\n",
        "        self.key = data.Field(\n",
        "            tokenize=tokenizer, include_lengths=True, eos_token=\"</k>\", init_token=\"<k>\"\n",
        "        )\n",
        "        self.answer = data.Field(\n",
        "            tokenize=tokenizer, include_lengths=True, eos_token=\"</a>\", init_token=\"<a>\"\n",
        "        )\n",
        "        self.label = data.LabelField()\n",
        "\n",
        "        self.tags = data.Field(tokenize=tokenizer)\n",
        "\n",
        "        self.fields = None\n",
        "        self.trainset = None\n",
        "        self.testset = None\n",
        "        self.train_iterator, self.test_iterator = None, None\n",
        "\n",
        "    @classmethod\n",
        "    def get_iterators(cls, batch_size):\n",
        "        \"\"\"\n",
        "        Load dataset and return iterators\n",
        "        \"\"\"\n",
        "        grammar_dataset = cls()\n",
        "\n",
        "        grammar_dataset.fields = [\n",
        "            (\"question\", grammar_dataset.question),\n",
        "            (\"key\", grammar_dataset.key),\n",
        "            (\"answer\", grammar_dataset.answer),\n",
        "            (\"label\", grammar_dataset.label),\n",
        "            (None, None),\n",
        "        ]\n",
        "\n",
        "        if not os.path.exists(PROCESSED_DATASET[\"train\"]) or not os.path.exists(\n",
        "            PROCESSED_DATASET[\"test\"]\n",
        "        ):\n",
        "            raise FileNotFoundError(\n",
        "                \"Please run the preprocessdata.py first by executing python preprocessdata.py\"\n",
        "            )\n",
        "\n",
        "        grammar_dataset.trainset, grammar_dataset.testset = data.TabularDataset.splits(\n",
        "            path=os.path.join(DATASET_FOLDER, PROCESSED_DATASET_FOLDER),\n",
        "            train=PROCESSED_DATASET_TRAIN_FILENAME,\n",
        "            test=PROCESSED_DATASET_TEST_FILENAME,\n",
        "            format=\"tsv\",\n",
        "            fields=grammar_dataset.fields,\n",
        "            skip_header=True,\n",
        "        )\n",
        "\n",
        "        logger.debug(\"Data Loaded Successfully!\")\n",
        "\n",
        "        grammar_dataset.question.build_vocab(\n",
        "            grammar_dataset.trainset,\n",
        "            max_size=MAX_VOCAB,\n",
        "            vectors=\"glove.6B.300d\",\n",
        "            unk_init=torch.Tensor.normal_,\n",
        "        )\n",
        "\n",
        "        grammar_dataset.key.vocab = grammar_dataset.question.vocab\n",
        "        grammar_dataset.answer.vocab = grammar_dataset.question.vocab\n",
        "\n",
        "        grammar_dataset.label.build_vocab(grammar_dataset.trainset)\n",
        "\n",
        "        grammar_dataset.tags.build_vocab([\"Q\", \"K\", \"A\"])\n",
        "\n",
        "        logger.debug(\"Vocabulary Loaded\")\n",
        "        grammar_dataset.train_iterator, grammar_dataset.test_iterator = data.BucketIterator.splits(\n",
        "            (grammar_dataset.trainset, grammar_dataset.testset),\n",
        "            batch_size=batch_size,\n",
        "            sort_within_batch=True,\n",
        "            sort_key=lambda x: len(x.question) + len(x.key) + len(x.answer),\n",
        "            device=device,\n",
        "        )\n",
        "        logger.debug(\"Created Iterators\")\n",
        "\n",
        "        return grammar_dataset\n",
        "\n",
        "\n",
        "class GrammarDasetAnswerTag:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.dataset_location = PROCESSED_DATASET\n",
        "\n",
        "        self.text = data.Field(\n",
        "            tokenize=tokenizer,\n",
        "            include_lengths=True,\n",
        "            eos_token=\"<end>\",\n",
        "            init_token=\"<start>\",\n",
        "        )\n",
        "        self.label = data.LabelField()\n",
        "\n",
        "        self.fields = None\n",
        "        self.trainset = None\n",
        "        self.testset = None\n",
        "        self.train_iterator, self.test_iterator = None, None\n",
        "\n",
        "    @classmethod\n",
        "    def get_iterators(cls, batch_size):\n",
        "        \"\"\"\n",
        "        Load dataset and return iterators\n",
        "        \"\"\"\n",
        "        grammar_dataset = cls()\n",
        "\n",
        "        grammar_dataset.fields = [\n",
        "            (None, None),\n",
        "            (None, None),\n",
        "            (\"text\", grammar_dataset.text),\n",
        "            (\"label\", grammar_dataset.label),\n",
        "            (None, None),\n",
        "        ]\n",
        "\n",
        "        if not os.path.exists(PROCESSED_DATASET[\"train\"]) or not os.path.exists(\n",
        "            PROCESSED_DATASET[\"test\"]\n",
        "        ):\n",
        "            raise FileNotFoundError(\n",
        "                \"Please run the preprocessdata.py first by executing python preprocessdata.py\"\n",
        "            )\n",
        "\n",
        "        grammar_dataset.trainset, grammar_dataset.testset = data.TabularDataset.splits(\n",
        "            path=os.path.join(DATASET_FOLDER, PROCESSED_DATASET_FOLDER),\n",
        "            train=PROCESSED_DATASET_TRAIN_FILENAME,\n",
        "            test=PROCESSED_DATASET_TEST_FILENAME,\n",
        "            format=\"tsv\",\n",
        "            fields=grammar_dataset.fields,\n",
        "            skip_header=True,\n",
        "        )\n",
        "\n",
        "        logger.debug(\"Data Loaded Successfully!\")\n",
        "\n",
        "        grammar_dataset.text.build_vocab(\n",
        "            grammar_dataset.trainset,\n",
        "            max_size=MAX_VOCAB,\n",
        "            vectors=\"glove.6B.300d\",\n",
        "            unk_init=torch.Tensor.normal_,\n",
        "        )\n",
        "\n",
        "        grammar_dataset.label.build_vocab(grammar_dataset.trainset)\n",
        "\n",
        "        logger.debug(\"Vocabulary Loaded\")\n",
        "        grammar_dataset.train_iterator, grammar_dataset.test_iterator = data.BucketIterator.splits(\n",
        "            (grammar_dataset.trainset, grammar_dataset.testset),\n",
        "            batch_size=batch_size,\n",
        "            sort_within_batch=True,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            device=device,\n",
        "        )\n",
        "        logger.debug(\"Created Iterators\")\n",
        "\n",
        "        return grammar_dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting datasetloader.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2A0YGhb257k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}