{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generation of Blanks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/shivammehta007/NLPinEnglishLearning/blob/master/FITBGenerator/Generation_of_Blanks.ipynb",
      "authorship_tag": "ABX9TyP2IOY0jtEbJcaLo0EW2c5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivammehta007/NLPinEnglishLearning/blob/master/FITBGenerator/Generation_of_Blanks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU4dAfBwhK3b",
        "colab_type": "text"
      },
      "source": [
        "# Fill in the blank Generator With Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpdPjQ5jSn_",
        "colab_type": "text"
      },
      "source": [
        "## Download Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDrIopZhIaO",
        "colab_type": "code",
        "outputId": "03c14ad1-cf0b-46bd-9741-626672c56c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!rm -rf NLPinEnglishLearning\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import subprocess\n",
        "\n",
        "def run_command(command):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        shell=True,\n",
        "        encoding=\"utf-8\",\n",
        "        errors=\"replace\",\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        realtime_output = process.stdout.readline()\n",
        "\n",
        "        if realtime_output == \"\" and process.poll() is not None:\n",
        "            break\n",
        "\n",
        "        if realtime_output:\n",
        "            print(realtime_output.strip(), flush=True)\n",
        "\n",
        "\n",
        "cmd_string = 'git clone https://github.com/shivammehta007/NLPinEnglishLearning.git'\n",
        "run_command(cmd_string)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLPinEnglishLearning'...\n",
            "Checking out files:  94% (127/134)\n",
            "Checking out files:  95% (128/134)\n",
            "Checking out files:  96% (129/134)\n",
            "Checking out files:  97% (130/134)\n",
            "Checking out files:  98% (132/134)\n",
            "Checking out files:  99% (133/134)\n",
            "Checking out files: 100% (134/134)\n",
            "Checking out files: 100% (134/134), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7g4AhObih2-",
        "colab_type": "code",
        "outputId": "13ff2056-32c2-4abf-f665-7efa39ff5dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd NLPinEnglishLearning/FITBGenerator/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLPinEnglishLearning/FITBGenerator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcDcDeTylq76",
        "colab_type": "code",
        "outputId": "3fea21f2-000d-4d33-e83d-c382d4229075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation_of_Blanks.ipynb  \u001b[0m\u001b[01;34mSequence2Sequence\u001b[0m/  \u001b[01;34mSequenceLabeling\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvlCMl-LJI1H",
        "colab_type": "text"
      },
      "source": [
        "## Download Glove from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMieNSOJIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "kaggle_info = json.load(open(\"/content/drive/My Drive/kaggle.json\"))\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_info[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_info[\"key\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPfYshuJKWLJ",
        "colab_type": "code",
        "outputId": "65b45b4e-2ad4-486b-a8bf-a66e5ea1543f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list --user thanakomsn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                        title               size  lastUpdated          downloadCount  \n",
            "-------------------------  -----------------  -----  -------------------  -------------  \n",
            "thanakomsn/glove6b300dtxt  glove.6B.300d.txt  386MB  2017-11-28 07:19:43           2926  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oksPtO8LpYP",
        "colab_type": "code",
        "outputId": "580854fd-100f-4053-8228-4e8fe895bd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download thanakomsn/glove6b300dtxt "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b300dtxt.zip to /content/NLPinEnglishLearning/FITBGenerator/SequenceClassification\n",
            "100% 385M/386M [00:06<00:00, 94.1MB/s]\n",
            "100% 386M/386M [00:06<00:00, 62.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRjCEyqsL_8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir .vector_cache\n",
        "%mv glove6b300dtxt.zip .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haCNqeqHMUDs",
        "colab_type": "code",
        "outputId": "8d5c0bfa-c37c-497d-c158-4b6d16131aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip .vector_cache/glove6b300dtxt.zip\n",
        "%ls -a .vector_cache/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .vector_cache/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  glove6b300dtxt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm6SEXYlcdrs",
        "colab_type": "text"
      },
      "source": [
        "## Sequence Labeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaeY97szchhh",
        "colab_type": "code",
        "outputId": "30f3d2ac-7ba9-4eeb-ea12-ee395b1b6cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd SequenceLabeling/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLPinEnglishLearning/FITBGenerator/SequenceClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X3GrnD7jOpx",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSBYYV6qHXFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python preprocessdata.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ4_Jw_lp3x",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6sicYFjuV3",
        "colab_type": "code",
        "outputId": "f9d11809-0056-4ba4-f00e-85eecd804785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50 --n-layers 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | train.py:221 -             <module>() ] Namespace(batch_size=64, bidirectional=True, dropout=0.7, embedding_dim=300, epochs=50, freeze_embeddings=1, hidden_dim=128, l2_regularization=0.001, learning_rate=0.001, linear_hidden_dim=128, model='RNNHiddenClassifier', model_location=None, n_layers=1, seed=1234)\n",
            "[DEBUG | train.py:222 -             <module>() ] Custom seed set with: 1234\n",
            "[INFO | train.py:224 -             <module>() ] Loading Dataset\n",
            "[DEBUG | datasetloader.py:74 -        get_iterators() ] Data Loaded Successfully!\n",
            "[INFO | vocab.py:329 -                cache() ] Loading vectors from glove.6B.300d.txt\n",
            "100% 399957/400000 [00:38<00:00, 10478.59it/s][INFO | vocab.py:381 -                cache() ] Saving vectors to .vector_cache/glove.6B.300d.txt.pt\n",
            "[DEBUG | datasetloader.py:85 -        get_iterators() ] Vocabulary Loaded\n",
            "[DEBUG | datasetloader.py:94 -        get_iterators() ] Created Iterators\n",
            "[INFO | train.py:228 -             <module>() ] Dataset Loaded Successfully\n",
            "[DEBUG | train.py:66 - initialize_new_model() ] Initializing Model\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[DEBUG | train.py:92 - initialize_new_model() ] Freeze Embeddings Value 1: False\n",
            "[INFO | train.py:98 - initialize_new_model() ] Model Initialized with 440,577 trainiable parameters\n",
            "[DEBUG | train.py:110 - initialize_new_model() ] Copied PreTrained Embeddings\n",
            "100% 399957/400000 [00:50<00:00, 10478.59it/s][INFO | train.py:253 -             <module>() ] RNNHiddenClassifier(\n",
            "  (embedding): Embedding(564, 300, padding_idx=1)\n",
            "  (rnn): LSTMWithPackPaddedSequences(\n",
            "    (rnn): LSTM(300, 128, dropout=0.7, bidirectional=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.7, inplace=False)\n",
            ")\n",
            "\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "100% 5/5 [00:00<00:00, 10.21it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 164.05it/s]\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.677 | Train Acc: 59.77%\n",
            "\t Val. Loss: 0.683 |  Val. Acc: 51.51%\n",
            "\t Train. F1: 0.35 |  Val. F1: 0.02\n",
            "\t Train. Precision: 0.42 |  Val. Precision: 0.01\n",
            "\t Train. Recall: 0.46 |  Val. Recall: 0.11\n",
            "\n",
            "100% 5/5 [00:00<00:00, 113.79it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 155.60it/s]\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.640 | Train Acc: 65.16%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 52.80%\n",
            "\t Train. F1: 0.14 |  Val. F1: 0.04\n",
            "\t Train. Precision: 0.08 |  Val. Precision: 0.02\n",
            "\t Train. Recall: 0.53 |  Val. Recall: 0.25\n",
            "\n",
            "100% 5/5 [00:00<00:00, 113.66it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 155.82it/s]\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.611 | Train Acc: 67.11%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 57.11%\n",
            "\t Train. F1: 0.22 |  Val. F1: 0.23\n",
            "\t Train. Precision: 0.13 |  Val. Precision: 0.14\n",
            "\t Train. Recall: 0.71 |  Val. Recall: 0.63\n",
            "\n",
            "100% 5/5 [00:00<00:00, 116.93it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 159.72it/s]\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.579 | Train Acc: 71.35%\n",
            "\t Val. Loss: 0.669 |  Val. Acc: 62.93%\n",
            "\t Train. F1: 0.41 |  Val. F1: 0.42\n",
            "\t Train. Precision: 0.30 |  Val. Precision: 0.29\n",
            "\t Train. Recall: 0.72 |  Val. Recall: 0.73\n",
            "\n",
            "100% 5/5 [00:00<00:00, 102.99it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 161.82it/s]\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.546 | Train Acc: 76.55%\n",
            "\t Val. Loss: 0.670 |  Val. Acc: 66.16%\n",
            "\t Train. F1: 0.57 |  Val. F1: 0.51\n",
            "\t Train. Precision: 0.45 |  Val. Precision: 0.38\n",
            "\t Train. Recall: 0.79 |  Val. Recall: 0.75\n",
            "\n",
            "100% 5/5 [00:00<00:00, 115.76it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 152.69it/s]\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.513 | Train Acc: 80.54%\n",
            "\t Val. Loss: 0.648 |  Val. Acc: 71.12%\n",
            "\t Train. F1: 0.69 |  Val. F1: 0.63\n",
            "\t Train. Precision: 0.62 |  Val. Precision: 0.54\n",
            "\t Train. Recall: 0.78 |  Val. Recall: 0.76\n",
            "\n",
            "100% 5/5 [00:00<00:00, 117.39it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 166.76it/s]\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.487 | Train Acc: 81.56%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 72.84%\n",
            "\t Train. F1: 0.72 |  Val. F1: 0.64\n",
            "\t Train. Precision: 0.71 |  Val. Precision: 0.54\n",
            "\t Train. Recall: 0.74 |  Val. Recall: 0.80\n",
            "\n",
            "100% 5/5 [00:00<00:00, 115.75it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 114.74it/s]\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.473 | Train Acc: 82.34%\n",
            "\t Val. Loss: 0.647 |  Val. Acc: 72.41%\n",
            "\t Train. F1: 0.73 |  Val. F1: 0.63\n",
            "\t Train. Precision: 0.70 |  Val. Precision: 0.52\n",
            "\t Train. Recall: 0.77 |  Val. Recall: 0.80\n",
            "\n",
            "100% 5/5 [00:00<00:00, 105.65it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 156.54it/s]\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.454 | Train Acc: 82.23%\n",
            "\t Val. Loss: 0.650 |  Val. Acc: 72.41%\n",
            "\t Train. F1: 0.73 |  Val. F1: 0.63\n",
            "\t Train. Precision: 0.69 |  Val. Precision: 0.52\n",
            "\t Train. Recall: 0.78 |  Val. Recall: 0.81\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.27it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 162.17it/s]\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.451 | Train Acc: 82.59%\n",
            "\t Val. Loss: 0.644 |  Val. Acc: 73.49%\n",
            "\t Train. F1: 0.73 |  Val. F1: 0.65\n",
            "\t Train. Precision: 0.70 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.77 |  Val. Recall: 0.81\n",
            "\n",
            "100% 5/5 [00:00<00:00, 109.94it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 118.62it/s]\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.425 | Train Acc: 83.97%\n",
            "\t Val. Loss: 0.621 |  Val. Acc: 74.57%\n",
            "\t Train. F1: 0.76 |  Val. F1: 0.67\n",
            "\t Train. Precision: 0.75 |  Val. Precision: 0.58\n",
            "\t Train. Recall: 0.79 |  Val. Recall: 0.81\n",
            "\n",
            "100% 5/5 [00:00<00:00, 112.10it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 159.64it/s]\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.415 | Train Acc: 84.92%\n",
            "\t Val. Loss: 0.623 |  Val. Acc: 74.14%\n",
            "\t Train. F1: 0.78 |  Val. F1: 0.66\n",
            "\t Train. Precision: 0.79 |  Val. Precision: 0.56\n",
            "\t Train. Recall: 0.78 |  Val. Recall: 0.81\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.58it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 169.38it/s]\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.420 | Train Acc: 84.02%\n",
            "\t Val. Loss: 0.625 |  Val. Acc: 73.28%\n",
            "\t Train. F1: 0.76 |  Val. F1: 0.64\n",
            "\t Train. Precision: 0.74 |  Val. Precision: 0.53\n",
            "\t Train. Recall: 0.79 |  Val. Recall: 0.82\n",
            "\n",
            "100% 5/5 [00:00<00:00, 94.17it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 162.80it/s]\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.398 | Train Acc: 85.59%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 73.92%\n",
            "\t Train. F1: 0.78 |  Val. F1: 0.66\n",
            "\t Train. Precision: 0.74 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.82\n",
            "\n",
            "100% 5/5 [00:00<00:00, 110.21it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 151.66it/s]\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.399 | Train Acc: 85.42%\n",
            "\t Val. Loss: 0.611 |  Val. Acc: 75.22%\n",
            "\t Train. F1: 0.78 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.74 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.83 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.31it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 159.30it/s]\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.387 | Train Acc: 86.38%\n",
            "\t Val. Loss: 0.612 |  Val. Acc: 75.86%\n",
            "\t Train. F1: 0.80 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.80 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.81 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 103.24it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 145.65it/s]\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.373 | Train Acc: 87.11%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 75.65%\n",
            "\t Train. F1: 0.82 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.81 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.82\n",
            "\n",
            "100% 5/5 [00:00<00:00, 107.06it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 162.49it/s]\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.376 | Train Acc: 86.47%\n",
            "\t Val. Loss: 0.609 |  Val. Acc: 75.22%\n",
            "\t Train. F1: 0.80 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.82 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.79 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.03it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 165.23it/s]\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.378 | Train Acc: 86.06%\n",
            "\t Val. Loss: 0.609 |  Val. Acc: 74.78%\n",
            "\t Train. F1: 0.78 |  Val. F1: 0.66\n",
            "\t Train. Precision: 0.76 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 113.62it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 158.45it/s]\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train Acc: 85.51%\n",
            "\t Val. Loss: 0.594 |  Val. Acc: 75.43%\n",
            "\t Train. F1: 0.78 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.76 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.81 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 111.83it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 161.30it/s]\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.372 | Train Acc: 86.52%\n",
            "\t Val. Loss: 0.619 |  Val. Acc: 74.35%\n",
            "\t Train. F1: 0.80 |  Val. F1: 0.66\n",
            "\t Train. Precision: 0.78 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.27it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 166.73it/s]\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.361 | Train Acc: 86.99%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 76.51%\n",
            "\t Train. F1: 0.81 |  Val. F1: 0.70\n",
            "\t Train. Precision: 0.82 |  Val. Precision: 0.62\n",
            "\t Train. Recall: 0.81 |  Val. Recall: 0.82\n",
            "\n",
            "100% 5/5 [00:00<00:00, 112.96it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 163.17it/s]\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.354 | Train Acc: 87.83%\n",
            "\t Val. Loss: 0.606 |  Val. Acc: 75.43%\n",
            "\t Train. F1: 0.82 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 116.88it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 168.49it/s]\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.348 | Train Acc: 87.59%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.22%\n",
            "\t Train. F1: 0.81 |  Val. F1: 0.67\n",
            "\t Train. Precision: 0.80 |  Val. Precision: 0.56\n",
            "\t Train. Recall: 0.83 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.93it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 166.94it/s]\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.348 | Train Acc: 88.11%\n",
            "\t Val. Loss: 0.621 |  Val. Acc: 74.78%\n",
            "\t Train. F1: 0.82 |  Val. F1: 0.67\n",
            "\t Train. Precision: 0.78 |  Val. Precision: 0.56\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.83\n",
            "\n",
            "100% 5/5 [00:00<00:00, 120.68it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 148.85it/s]\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.350 | Train Acc: 87.67%\n",
            "\t Val. Loss: 0.594 |  Val. Acc: 75.65%\n",
            "\t Train. F1: 0.82 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.60\n",
            "\t Train. Recall: 0.82 |  Val. Recall: 0.82\n",
            "\n",
            "100% 5/5 [00:00<00:00, 117.64it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 161.51it/s]\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.335 | Train Acc: 88.90%\n",
            "\t Val. Loss: 0.611 |  Val. Acc: 74.78%\n",
            "\t Train. F1: 0.84 |  Val. F1: 0.66\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.90it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 161.47it/s]\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.351 | Train Acc: 87.38%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.00%\n",
            "\t Train. F1: 0.80 |  Val. F1: 0.67\n",
            "\t Train. Precision: 0.75 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 104.41it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 157.10it/s]\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.337 | Train Acc: 88.19%\n",
            "\t Val. Loss: 0.574 |  Val. Acc: 76.94%\n",
            "\t Train. F1: 0.83 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.64\n",
            "\t Train. Recall: 0.84 |  Val. Recall: 0.81\n",
            "\n",
            "100% 5/5 [00:00<00:00, 104.91it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 155.44it/s]\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.337 | Train Acc: 88.42%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 75.86%\n",
            "\t Train. F1: 0.83 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.85 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.83 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.33it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 161.13it/s]\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.329 | Train Acc: 88.83%\n",
            "\t Val. Loss: 0.598 |  Val. Acc: 75.86%\n",
            "\t Train. F1: 0.83 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.80 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.08it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 112.42it/s]\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.332 | Train Acc: 88.32%\n",
            "\t Val. Loss: 0.589 |  Val. Acc: 76.29%\n",
            "\t Train. F1: 0.83 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.83 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 116.41it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 153.55it/s]\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.324 | Train Acc: 89.07%\n",
            "\t Val. Loss: 0.592 |  Val. Acc: 75.86%\n",
            "\t Train. F1: 0.84 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 107.64it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 154.93it/s]\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train Acc: 89.46%\n",
            "\t Val. Loss: 0.616 |  Val. Acc: 75.22%\n",
            "\t Train. F1: 0.84 |  Val. F1: 0.67\n",
            "\t Train. Precision: 0.82 |  Val. Precision: 0.55\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 112.02it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 148.72it/s]\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.314 | Train Acc: 89.70%\n",
            "\t Val. Loss: 0.585 |  Val. Acc: 77.59%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.63\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.14it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 159.30it/s]\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.326 | Train Acc: 89.39%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 75.86%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.68\n",
            "\t Train. Precision: 0.85 |  Val. Precision: 0.57\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.71it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 158.87it/s]\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train Acc: 89.06%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 76.94%\n",
            "\t Train. F1: 0.84 |  Val. F1: 0.70\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.60\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 111.01it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 136.14it/s]\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train Acc: 89.77%\n",
            "\t Val. Loss: 0.583 |  Val. Acc: 77.16%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.71\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.60\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 111.59it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 153.07it/s]\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.312 | Train Acc: 90.00%\n",
            "\t Val. Loss: 0.603 |  Val. Acc: 76.72%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.70\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.86\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.56it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 170.44it/s]\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.315 | Train Acc: 89.05%\n",
            "\t Val. Loss: 0.603 |  Val. Acc: 76.94%\n",
            "\t Train. F1: 0.84 |  Val. F1: 0.70\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.60\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.05it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 145.77it/s]\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.299 | Train Acc: 90.74%\n",
            "\t Val. Loss: 0.596 |  Val. Acc: 76.72%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.70\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.61\n",
            "\t Train. Recall: 0.89 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 115.17it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 155.60it/s]\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.305 | Train Acc: 89.44%\n",
            "\t Val. Loss: 0.592 |  Val. Acc: 77.59%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.71\n",
            "\t Train. Precision: 0.85 |  Val. Precision: 0.60\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.86\n",
            "\n",
            "100% 5/5 [00:00<00:00, 119.16it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 155.49it/s]\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train Acc: 90.19%\n",
            "\t Val. Loss: 0.575 |  Val. Acc: 78.45%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.85 |  Val. Precision: 0.61\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.88\n",
            "\n",
            "100% 5/5 [00:00<00:00, 106.71it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 162.39it/s]\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.308 | Train Acc: 89.95%\n",
            "\t Val. Loss: 0.577 |  Val. Acc: 78.66%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.91\n",
            "\n",
            "100% 5/5 [00:00<00:00, 121.12it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 150.83it/s]\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.299 | Train Acc: 90.68%\n",
            "\t Val. Loss: 0.586 |  Val. Acc: 77.59%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.71\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.61\n",
            "\t Train. Recall: 0.90 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 118.25it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 169.53it/s]\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train Acc: 89.52%\n",
            "\t Val. Loss: 0.608 |  Val. Acc: 77.80%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.63\n",
            "\t Train. Recall: 0.85 |  Val. Recall: 0.85\n",
            "\n",
            "100% 5/5 [00:00<00:00, 120.05it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 157.07it/s]\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train Acc: 90.01%\n",
            "\t Val. Loss: 0.591 |  Val. Acc: 76.94%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.71\n",
            "\t Train. Precision: 0.84 |  Val. Precision: 0.61\n",
            "\t Train. Recall: 0.87 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 110.57it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 130.76it/s]\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train Acc: 90.21%\n",
            "\t Val. Loss: 0.599 |  Val. Acc: 76.29%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.86 |  Val. Precision: 0.59\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.84\n",
            "\n",
            "100% 5/5 [00:00<00:00, 102.69it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 127.91it/s]\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.301 | Train Acc: 90.37%\n",
            "\t Val. Loss: 0.610 |  Val. Acc: 76.51%\n",
            "\t Train. F1: 0.85 |  Val. F1: 0.69\n",
            "\t Train. Precision: 0.83 |  Val. Precision: 0.58\n",
            "\t Train. Recall: 0.89 |  Val. Recall: 0.86\n",
            "\n",
            "100% 5/5 [00:00<00:00, 113.73it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00, 154.07it/s]\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train Acc: 90.30%\n",
            "\t Val. Loss: 0.556 |  Val. Acc: 77.80%\n",
            "\t Train. F1: 0.86 |  Val. F1: 0.72\n",
            "\t Train. Precision: 0.87 |  Val. Precision: 0.64\n",
            "\t Train. Recall: 0.86 |  Val. Recall: 0.84\n",
            "100% 399957/400000 [00:53<00:00, 7477.53it/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXHK1gyh2ok9",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq To generate Blanks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7EmnZGfKhiO",
        "colab_type": "code",
        "outputId": "a1ad846f-a6b8-4465-8c48-4d9eb2ea86cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd Sequence2Sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/QuestionGenerator/FITBGenerator/Sequence2Sequence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnoJOcu07JrL",
        "colab_type": "code",
        "outputId": "f0843d2e-86ce-4c6c-c75c-59b0cb265a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/  \u001b[01;34mdata\u001b[0m/  preprocess.py  train.py  utility.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqx_D6V3X8ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-0VBxvCLHnZ",
        "colab_type": "text"
      },
      "source": [
        "## PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2uX6T1BMvya",
        "colab_type": "code",
        "outputId": "62a45461-69ab-4123-ab7d-d31fc9f88434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mconfig\u001b[0m/  \u001b[01;34mdata\u001b[0m/  preprocess.py  train.py  utility.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzvxpA7xLK7h",
        "colab_type": "code",
        "outputId": "50c6276d-5344-45ad-d0d9-fda96ca5f60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!python preprocess.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DEBUG | preprocess.py:66 -           preprocess() ] DataSet Preprocessed Successfully!\n",
            "[DEBUG | preprocess.py:120 -           preprocess() ] Trainset Size: (260, 2), Validset Size: (29, 2), Tesetset Size: (51, 2)\n",
            "[DEBUG | preprocess.py:126 -           preprocess() ] Saving the file preprocessed files to : processed\n",
            "[INFO | preprocess.py:131 -           preprocess() ] Running FairSeq Preprocessing to convert files into fairseq binaries\n",
            "[DEBUG | preprocess.py:135 -           preprocess() ] Old Binaries present deleting them\n",
            "[DEBUG | preprocess.py:137 -           preprocess() ] Deleted old binaries now generating new one's\n",
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/fairseq_binaries', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=5000, nwordstgt=5000, only_source=False, optimizer='nag', padding_factor=8, seed=1234, source_lang='sentence', srcdict=None, target_lang='question', task='translation', tensorboard_logdir='', testpref='data/processed/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='data/processed/train', user_dir=None, validpref='data/processed/valid', workers=1)\n",
            "| [sentence] Dictionary: 663 types\n",
            "| [sentence] data/processed/train.sentence: 260 sents, 2232 tokens, 0.0% replaced by <unk>\n",
            "| [sentence] Dictionary: 663 types\n",
            "| [sentence] data/processed/valid.sentence: 29 sents, 222 tokens, 17.6% replaced by <unk>\n",
            "| [sentence] Dictionary: 663 types\n",
            "| [sentence] data/processed/test.sentence: 51 sents, 437 tokens, 19.2% replaced by <unk>\n",
            "| [question] Dictionary: 551 types\n",
            "| [question] data/processed/train.question: 260 sents, 1930 tokens, 0.0% replaced by <unk>\n",
            "| [question] Dictionary: 551 types\n",
            "| [question] data/processed/valid.question: 29 sents, 194 tokens, 17.5% replaced by <unk>\n",
            "| [question] Dictionary: 551 types\n",
            "| [question] data/processed/test.question: 51 sents, 376 tokens, 19.1% replaced by <unk>\n",
            "| Wrote preprocessed data to data/fairseq_binaries\n",
            "[DEBUG | preprocess.py:173 -             <module>() ] Utility Finished Execution in: 1.1623ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28w7ziS1k96C",
        "colab_type": "text"
      },
      "source": [
        "## RNN Sequence2Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8naS7GM2LWJx",
        "colab_type": "code",
        "outputId": "36954f1e-d7f2-47ed-c9af-aed50b93d17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', arch='lstm', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=5.0, cpu=False, criterion='cross_entropy', curriculum=0, data='data/fairseq_binaries', dataset_impl=None, ddp_backend='c10d', decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, decoder_embed_dim=300, decoder_embed_path='glove.6B.300d.txt', decoder_freeze_embed=False, decoder_hidden_size=258, decoder_layers=2, decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_bidirectional=True, encoder_dropout_in=0.3, encoder_dropout_out=0.3, encoder_embed_dim=300, encoder_embed_path='glove.6B.300d.txt', encoder_freeze_embed=False, encoder_hidden_size=258, encoder_layers=2, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=50, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/lstm', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [sentence] dictionary: 664 types\n",
            "| [question] dictionary: 552 types\n",
            "| loaded 29 examples from: data/fairseq_binaries/valid.sentence-question.sentence\n",
            "| loaded 29 examples from: data/fairseq_binaries/valid.sentence-question.question\n",
            "| data/fairseq_binaries valid sentence-question 29 examples\n",
            "| Found 335/664 types in embedding file.\n",
            "| Found 247/552 types in embedding file.\n",
            "LSTMModel(\n",
            "(encoder): LSTMEncoder(\n",
            "(embed_tokens): Embedding(664, 300, padding_idx=1)\n",
            "(lstm): LSTM(300, 258, num_layers=2, dropout=0.3, bidirectional=True)\n",
            ")\n",
            "(decoder): LSTMDecoder(\n",
            "(embed_tokens): Embedding(552, 300, padding_idx=1)\n",
            "(encoder_hidden_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "(encoder_cell_proj): Linear(in_features=516, out_features=258, bias=True)\n",
            "(layers): ModuleList(\n",
            "(0): LSTMCell(558, 258)\n",
            "(1): LSTMCell(258, 258)\n",
            ")\n",
            "(attention): AttentionLayer(\n",
            "(input_proj): Linear(in_features=258, out_features=516, bias=False)\n",
            "(output_proj): Linear(in_features=774, out_features=258, bias=False)\n",
            ")\n",
            "(additional_fc): Linear(in_features=258, out_features=300, bias=True)\n",
            "(fc_out): Linear(in_features=300, out_features=552, bias=True)\n",
            ")\n",
            ")\n",
            "| model lstm, criterion CrossEntropyCriterion\n",
            "| num. model params: 5344500 (num. trained: 5344500)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| loaded checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 15 @ 75 updates)\n",
            "| loading train data for epoch 15\n",
            "| loaded 260 examples from: data/fairseq_binaries/train.sentence-question.sentence\n",
            "| loaded 260 examples from: data/fairseq_binaries/train.sentence-question.question\n",
            "| data/fairseq_binaries train sentence-question 260 examples\n",
            "| epoch 016 | loss 4.846 | ppl 28.76 | wps 9991 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 80 | lr 0.001 | gnorm 1.462 | clip 0.000 | oom 0.000 | wall 0 | train_wall 3\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "add_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "add_(Tensor other, *, Number alpha)\n",
            "| epoch 016 | valid on 'valid' subset | loss 5.596 | ppl 48.36 | num_updates 80 | best_loss 5.59581\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 16 @ 80 updates) (writing took 0.17329621315002441 seconds)\n",
            "| epoch 017 | loss 4.671 | ppl 25.48 | wps 10492 | ups 22 | wpb 386.000 | bsz 52.000 | num_updates 85 | lr 0.001 | gnorm 1.732 | clip 0.000 | oom 0.000 | wall 1 | train_wall 3\n",
            "| epoch 017 | valid on 'valid' subset | loss 5.706 | ppl 52.21 | num_updates 85 | best_loss 5.59581\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 17 @ 85 updates) (writing took 3.112016201019287 seconds)\n",
            "| epoch 018 | loss 4.623 | ppl 24.64 | wps 7420 | ups 19 | wpb 386.000 | bsz 52.000 | num_updates 90 | lr 0.001 | gnorm 2.199 | clip 0.000 | oom 0.000 | wall 5 | train_wall 3\n",
            "| epoch 018 | valid on 'valid' subset | loss 5.627 | ppl 49.41 | num_updates 90 | best_loss 5.59581\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 18 @ 90 updates) (writing took 0.10401129722595215 seconds)\n",
            "| epoch 019 | loss 4.472 | ppl 22.19 | wps 8302 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 95 | lr 0.001 | gnorm 2.224 | clip 0.200 | oom 0.000 | wall 5 | train_wall 4\n",
            "| epoch 019 | valid on 'valid' subset | loss 5.551 | ppl 46.9 | num_updates 95 | best_loss 5.55137\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 19 @ 95 updates) (writing took 0.43891143798828125 seconds)\n",
            "| epoch 020 | loss 4.252 | ppl 19.06 | wps 8663 | ups 19 | wpb 386.000 | bsz 52.000 | num_updates 100 | lr 0.001 | gnorm 1.960 | clip 0.000 | oom 0.000 | wall 6 | train_wall 4\n",
            "| epoch 020 | valid on 'valid' subset | loss 5.602 | ppl 48.58 | num_updates 100 | best_loss 5.55137\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 20 @ 100 updates) (writing took 0.1009531021118164 seconds)\n",
            "| epoch 021 | loss 4.018 | ppl 16.2 | wps 8188 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 105 | lr 0.001 | gnorm 1.533 | clip 0.000 | oom 0.000 | wall 7 | train_wall 4\n",
            "| epoch 021 | valid on 'valid' subset | loss 5.428 | ppl 43.06 | num_updates 105 | best_loss 5.42824\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 21 @ 105 updates) (writing took 1.4758620262145996 seconds)\n",
            "| epoch 022 | loss 3.790 | ppl 13.83 | wps 9897 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 110 | lr 0.001 | gnorm 1.506 | clip 0.000 | oom 0.000 | wall 8 | train_wall 4\n",
            "| epoch 022 | valid on 'valid' subset | loss 5.370 | ppl 41.35 | num_updates 110 | best_loss 5.3699\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 22 @ 110 updates) (writing took 1.3218023777008057 seconds)\n",
            "| epoch 023 | loss 3.609 | ppl 12.2 | wps 9628 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 115 | lr 0.001 | gnorm 1.660 | clip 0.000 | oom 0.000 | wall 10 | train_wall 4\n",
            "| epoch 023 | valid on 'valid' subset | loss 5.404 | ppl 42.34 | num_updates 115 | best_loss 5.3699\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 23 @ 115 updates) (writing took 0.26860499382019043 seconds)\n",
            "| epoch 024 | loss 3.369 | ppl 10.33 | wps 8072 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 120 | lr 0.001 | gnorm 1.439 | clip 0.000 | oom 0.000 | wall 11 | train_wall 5\n",
            "| epoch 024 | valid on 'valid' subset | loss 5.312 | ppl 39.73 | num_updates 120 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_best.pt (epoch 24 @ 120 updates) (writing took 1.3987176418304443 seconds)\n",
            "| epoch 025 | loss 3.124 | ppl 8.72 | wps 10445 | ups 22 | wpb 386.000 | bsz 52.000 | num_updates 125 | lr 0.001 | gnorm 1.363 | clip 0.000 | oom 0.000 | wall 13 | train_wall 5\n",
            "| epoch 025 | valid on 'valid' subset | loss 5.383 | ppl 41.72 | num_updates 125 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 25 @ 125 updates) (writing took 1.2570409774780273 seconds)\n",
            "| epoch 026 | loss 2.978 | ppl 7.88 | wps 6920 | ups 19 | wpb 386.000 | bsz 52.000 | num_updates 130 | lr 0.001 | gnorm 1.468 | clip 0.000 | oom 0.000 | wall 15 | train_wall 5\n",
            "| epoch 026 | valid on 'valid' subset | loss 5.500 | ppl 45.26 | num_updates 130 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 26 @ 130 updates) (writing took 0.09743905067443848 seconds)\n",
            "| epoch 027 | loss 2.746 | ppl 6.71 | wps 7252 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 135 | lr 0.001 | gnorm 1.413 | clip 0.000 | oom 0.000 | wall 15 | train_wall 5\n",
            "| epoch 027 | valid on 'valid' subset | loss 5.323 | ppl 40.03 | num_updates 135 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 27 @ 135 updates) (writing took 1.3399834632873535 seconds)\n",
            "| epoch 028 | loss 2.635 | ppl 6.21 | wps 7322 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 140 | lr 0.001 | gnorm 1.509 | clip 0.000 | oom 0.000 | wall 17 | train_wall 5\n",
            "| epoch 028 | valid on 'valid' subset | loss 5.438 | ppl 43.35 | num_updates 140 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 28 @ 140 updates) (writing took 1.3056042194366455 seconds)\n",
            "| epoch 029 | loss 2.418 | ppl 5.35 | wps 6889 | ups 22 | wpb 386.000 | bsz 52.000 | num_updates 145 | lr 0.001 | gnorm 1.563 | clip 0.000 | oom 0.000 | wall 19 | train_wall 5\n",
            "| epoch 029 | valid on 'valid' subset | loss 5.565 | ppl 47.33 | num_updates 145 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 29 @ 145 updates) (writing took 0.4707512855529785 seconds)\n",
            "| epoch 030 | loss 2.430 | ppl 5.39 | wps 7537 | ups 19 | wpb 386.000 | bsz 52.000 | num_updates 150 | lr 0.001 | gnorm 1.561 | clip 0.000 | oom 0.000 | wall 20 | train_wall 6\n",
            "| epoch 030 | valid on 'valid' subset | loss 5.507 | ppl 45.47 | num_updates 150 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 30 @ 150 updates) (writing took 0.09855031967163086 seconds)\n",
            "| epoch 031 | loss 2.255 | ppl 4.77 | wps 10043 | ups 22 | wpb 386.000 | bsz 52.000 | num_updates 155 | lr 0.001 | gnorm 1.633 | clip 0.000 | oom 0.000 | wall 20 | train_wall 6\n",
            "| epoch 031 | valid on 'valid' subset | loss 5.547 | ppl 46.76 | num_updates 155 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 31 @ 155 updates) (writing took 0.7572627067565918 seconds)\n",
            "| epoch 032 | loss 2.095 | ppl 4.27 | wps 7736 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 160 | lr 0.001 | gnorm 1.668 | clip 0.000 | oom 0.000 | wall 21 | train_wall 6\n",
            "| epoch 032 | valid on 'valid' subset | loss 5.512 | ppl 45.64 | num_updates 160 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 32 @ 160 updates) (writing took 0.29756593704223633 seconds)\n",
            "| epoch 033 | loss 1.959 | ppl 3.89 | wps 7950 | ups 19 | wpb 386.000 | bsz 52.000 | num_updates 165 | lr 0.001 | gnorm 1.479 | clip 0.000 | oom 0.000 | wall 22 | train_wall 6\n",
            "| epoch 033 | valid on 'valid' subset | loss 5.554 | ppl 46.99 | num_updates 165 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 33 @ 165 updates) (writing took 0.4665513038635254 seconds)\n",
            "| epoch 034 | loss 1.836 | ppl 3.57 | wps 7219 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 170 | lr 0.001 | gnorm 1.524 | clip 0.000 | oom 0.000 | wall 23 | train_wall 6\n",
            "| epoch 034 | valid on 'valid' subset | loss 5.539 | ppl 46.51 | num_updates 170 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 34 @ 170 updates) (writing took 0.09613513946533203 seconds)\n",
            "| epoch 035 | loss 1.754 | ppl 3.37 | wps 7057 | ups 23 | wpb 386.000 | bsz 52.000 | num_updates 175 | lr 0.001 | gnorm 1.511 | clip 0.000 | oom 0.000 | wall 24 | train_wall 6\n",
            "| epoch 035 | valid on 'valid' subset | loss 5.580 | ppl 47.85 | num_updates 175 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 35 @ 175 updates) (writing took 1.8066415786743164 seconds)\n",
            "| epoch 036 | loss 1.663 | ppl 3.17 | wps 7546 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 180 | lr 0.001 | gnorm 1.383 | clip 0.000 | oom 0.000 | wall 26 | train_wall 7\n",
            "| epoch 036 | valid on 'valid' subset | loss 5.804 | ppl 55.88 | num_updates 180 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 36 @ 180 updates) (writing took 0.09771728515625 seconds)\n",
            "| epoch 037 | loss 1.520 | ppl 2.87 | wps 8477 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 185 | lr 0.001 | gnorm 1.356 | clip 0.000 | oom 0.000 | wall 26 | train_wall 7\n",
            "| epoch 037 | valid on 'valid' subset | loss 5.932 | ppl 61.04 | num_updates 185 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 37 @ 185 updates) (writing took 1.3983745574951172 seconds)\n",
            "| epoch 038 | loss 1.429 | ppl 2.69 | wps 7226 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 190 | lr 0.001 | gnorm 1.251 | clip 0.000 | oom 0.000 | wall 28 | train_wall 7\n",
            "| epoch 038 | valid on 'valid' subset | loss 5.810 | ppl 56.12 | num_updates 190 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 38 @ 190 updates) (writing took 0.18758583068847656 seconds)\n",
            "| epoch 039 | loss 1.272 | ppl 2.42 | wps 7101 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 195 | lr 0.001 | gnorm 1.203 | clip 0.000 | oom 0.000 | wall 29 | train_wall 7\n",
            "| epoch 039 | valid on 'valid' subset | loss 5.889 | ppl 59.27 | num_updates 195 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 39 @ 195 updates) (writing took 0.0950467586517334 seconds)\n",
            "| epoch 040 | loss 1.172 | ppl 2.25 | wps 8097 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 200 | lr 0.001 | gnorm 1.190 | clip 0.000 | oom 0.000 | wall 29 | train_wall 7\n",
            "| epoch 040 | valid on 'valid' subset | loss 5.988 | ppl 63.48 | num_updates 200 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 40 @ 200 updates) (writing took 1.3686518669128418 seconds)\n",
            "| epoch 041 | loss 1.138 | ppl 2.2 | wps 7100 | ups 18 | wpb 386.000 | bsz 52.000 | num_updates 205 | lr 0.001 | gnorm 1.188 | clip 0.000 | oom 0.000 | wall 31 | train_wall 7\n",
            "| epoch 041 | valid on 'valid' subset | loss 6.046 | ppl 66.07 | num_updates 205 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 41 @ 205 updates) (writing took 0.3749063014984131 seconds)\n",
            "| epoch 042 | loss 1.046 | ppl 2.06 | wps 6980 | ups 23 | wpb 386.000 | bsz 52.000 | num_updates 210 | lr 0.001 | gnorm 1.139 | clip 0.000 | oom 0.000 | wall 32 | train_wall 8\n",
            "| epoch 042 | valid on 'valid' subset | loss 5.912 | ppl 60.22 | num_updates 210 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 42 @ 210 updates) (writing took 1.3398301601409912 seconds)\n",
            "| epoch 043 | loss 0.985 | ppl 1.98 | wps 7511 | ups 18 | wpb 386.000 | bsz 52.000 | num_updates 215 | lr 0.001 | gnorm 1.125 | clip 0.000 | oom 0.000 | wall 34 | train_wall 8\n",
            "| epoch 043 | valid on 'valid' subset | loss 6.228 | ppl 74.96 | num_updates 215 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 43 @ 215 updates) (writing took 0.10087060928344727 seconds)\n",
            "| epoch 044 | loss 0.933 | ppl 1.91 | wps 7912 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 220 | lr 0.001 | gnorm 1.206 | clip 0.000 | oom 0.000 | wall 35 | train_wall 8\n",
            "| epoch 044 | valid on 'valid' subset | loss 6.222 | ppl 74.66 | num_updates 220 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 44 @ 220 updates) (writing took 1.361126184463501 seconds)\n",
            "| epoch 045 | loss 0.861 | ppl 1.82 | wps 6674 | ups 22 | wpb 386.000 | bsz 52.000 | num_updates 225 | lr 0.001 | gnorm 1.232 | clip 0.000 | oom 0.000 | wall 36 | train_wall 8\n",
            "| epoch 045 | valid on 'valid' subset | loss 6.078 | ppl 67.54 | num_updates 225 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 45 @ 225 updates) (writing took 1.3005807399749756 seconds)\n",
            "| epoch 046 | loss 0.834 | ppl 1.78 | wps 7050 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 230 | lr 0.001 | gnorm 1.304 | clip 0.000 | oom 0.000 | wall 38 | train_wall 8\n",
            "| epoch 046 | valid on 'valid' subset | loss 6.377 | ppl 83.09 | num_updates 230 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 46 @ 230 updates) (writing took 0.4417283535003662 seconds)\n",
            "| epoch 047 | loss 0.820 | ppl 1.77 | wps 7110 | ups 20 | wpb 386.000 | bsz 52.000 | num_updates 235 | lr 0.001 | gnorm 1.242 | clip 0.000 | oom 0.000 | wall 39 | train_wall 9\n",
            "| epoch 047 | valid on 'valid' subset | loss 6.465 | ppl 88.34 | num_updates 235 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 47 @ 235 updates) (writing took 0.09791445732116699 seconds)\n",
            "| epoch 048 | loss 0.760 | ppl 1.69 | wps 10701 | ups 23 | wpb 386.000 | bsz 52.000 | num_updates 240 | lr 0.001 | gnorm 1.211 | clip 0.000 | oom 0.000 | wall 40 | train_wall 9\n",
            "| epoch 048 | valid on 'valid' subset | loss 6.377 | ppl 83.14 | num_updates 240 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 48 @ 240 updates) (writing took 1.367539882659912 seconds)\n",
            "| epoch 049 | loss 0.717 | ppl 1.64 | wps 6520 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 245 | lr 0.001 | gnorm 1.094 | clip 0.000 | oom 0.000 | wall 41 | train_wall 9\n",
            "| epoch 049 | valid on 'valid' subset | loss 6.352 | ppl 81.7 | num_updates 245 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 49 @ 245 updates) (writing took 1.307988166809082 seconds)\n",
            "| epoch 050 | loss 0.717 | ppl 1.64 | wps 7336 | ups 21 | wpb 386.000 | bsz 52.000 | num_updates 250 | lr 0.001 | gnorm 1.043 | clip 0.000 | oom 0.000 | wall 43 | train_wall 9\n",
            "| epoch 050 | valid on 'valid' subset | loss 6.628 | ppl 98.9 | num_updates 250 | best_loss 5.31208\n",
            "| saved checkpoint checkpoints/lstm/checkpoint_last.pt (epoch 50 @ 250 updates) (writing took 0.4395318031311035 seconds)\n",
            "| done training in 43.7 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9lizQ7TlIwp",
        "colab_type": "text"
      },
      "source": [
        "### Generate Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzX-1ZDHOEgx",
        "colab_type": "code",
        "outputId": "ef850855-b559-4b6e-aacf-a72da1c97254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python generate.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(beam=3, bpe=None, cpu=False, criterion='cross_entropy', data='data/fairseq_binaries', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='checkpoints/lstm/checkpoint_last.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
            "| [sentence] dictionary: 664 types\n",
            "| [question] dictionary: 552 types\n",
            "| loaded 51 examples from: data/fairseq_binaries/test.sentence-question.sentence\n",
            "| loaded 51 examples from: data/fairseq_binaries/test.sentence-question.question\n",
            "| data/fairseq_binaries test sentence-question 51 examples\n",
            "| loading model(s) from checkpoints/lstm/checkpoint_last.pt\n",
            "| Found 335/664 types in embedding file.\n",
            "| Found 247/552 types in embedding file.\n",
            "S-20\tI'd got a <unk> new bike for my <unk> so I was <unk> to <unk> it.\n",
            "T-20\tI'd _ a <<unk>> new bike for my <<unk>> so I was <<unk>> to <<unk>> it.\n",
            "H-20\t-0.4419388771057129\tI _ a bike for long when I wet.\n",
            "P-20\t-0.1136 -0.0001 -0.0028 -1.2427 -0.0050 -1.1966 -0.0957 -0.2509 -1.4931 -0.0190\n",
            "S-46\tHe was in <unk> with the <unk> because he hadn't been going to <unk>\n",
            "T-46\t<<unk>> He was in <<unk>> with the <<unk>> because he _ to <<unk>>\n",
            "H-46\t-0.60396808385849\tHe We _ in the York but he went to Paris?\n",
            "P-46\t-0.6308 -0.5949 -0.0398 -0.1389 -0.1839 -2.1826 -0.1130 -0.4008 -0.9696 -0.3017 -1.6884 -0.0033\n",
            "S-44\tWhen we <unk> you had been working at that company for six <unk>\n",
            "T-44\tWhen we <<unk>> you _ at that company for six <<unk>>\n",
            "H-44\t-0.7523512244224548\t_ when we _ for Paris.\n",
            "P-44\t-0.1376 -0.9986 -1.0152 -0.4366 -0.0765 -2.5772 -0.0246\n",
            "S-14\tHe hadn't <unk> <unk> <unk> so I <unk> him <unk> to <unk> it.\n",
            "T-14\tHe _ <<unk>> <<unk>> so I <<unk>> him <<unk>> to <<unk>> it.\n",
            "H-14\t-0.7525390386581421\tHe _ terrible healthy, to the waitress.\n",
            "P-14\t-0.1572 -0.0593 -1.8245 -1.8603 -0.0066 -1.5342 -0.5775 -0.0008\n",
            "S-26\tDavid <unk> into the <unk> He had <unk> on a <unk> <unk>\n",
            "T-26\tDavid <<unk>> into the <<unk>> He had _ on a <<unk>> <<unk>>\n",
            "H-26\t-0.89215087890625\tHe _ the bill, the winter.\n",
            "P-26\t-0.5673 -0.2301 -0.4452 -1.2185 -1.5289 -2.2517 -0.0034\n",
            "S-15\tWe were <unk> for the <unk> because we had forgotten our <unk>\n",
            "T-15\tWe were <<unk>> for the <<unk>> because we <<unk>> <<unk>>\n",
            "H-15\t-0.36003705859184265\tWe _ for the for when we leaving?\n",
            "P-15\t-0.0013 -0.0051 -0.9226 -0.0826 -0.5316 -0.1757 -0.0220 -1.4861 -0.0132\n",
            "S-31\tWhat will you have done by the <unk> of the <unk>\n",
            "T-31\tWhat _ by the <<unk>> of the <<unk>>\n",
            "H-31\t-0.5014091730117798\tWhy _ the report?\n",
            "P-31\t-0.7966 -0.0006 -0.0213 -1.6862 -0.0024\n",
            "S-45\tYou hadn't studied for the <unk> so you were very <unk>\n",
            "T-45\tYou _ for the <<unk>> so you were very <<unk>>\n",
            "H-45\t-0.28498950600624084\t5. We _ in the for three when we got married! It was more like five years.\n",
            "P-45\t-0.3334 -0.0764 -0.0001 -1.2201 -0.3955 -0.1128 -0.9655 -0.0360 -0.0116 -0.3858 -0.3953 -0.0603 -0.0938 -0.3110 -0.3515 -0.2142 -0.1650 -0.0016\n",
            "S-28\tWe will have been working, so we'll have <unk> of <unk>\n",
            "T-28\tWe _, so we'll have <<unk>> of <<unk>>\n",
            "H-28\t-0.34672337770462036\tWe _ all day,\n",
            "P-28\t-0.0017 -0.0007 -0.9351 -0.6127 -0.1834\n",
            "S-37\tJohn had never <unk> English before he came to London.\n",
            "T-37\tJohn had never _ English before he came to London.\n",
            "H-37\t-0.679970920085907\tHow _ to fix to cooker?\n",
            "P-37\t-0.1831 -0.0011 -1.1139 -0.3451 -1.5449 -1.5655 -0.0061\n",
            "S-25\tI will have been sleeping for three hours at <unk>\n",
            "T-25\tI _ for three hours at <<unk>>\n",
            "H-25\t-0.27314648032188416\tI _ for ten years next week, four years at most.\n",
            "P-25\t-0.0113 -0.0000 -0.2872 -0.3815 -1.0985 -0.4143 -0.6530 -0.2623 -0.0902 -0.0514 -0.0228 -0.0054\n",
            "S-47\tI <unk> a cold because I hadn't been eating <unk>\n",
            "T-47\t<<unk>> I <<unk>> a cold because I _ <<unk>>\n",
            "H-47\t-0.2839651107788086\tI _ tired tired I _.\n",
            "P-47\t-0.0253 -0.0017 -0.3213 -0.3172 -0.8760 -0.4461 -0.0002\n",
            "S-34\tHow many <unk> had she drunk before the <unk>\n",
            "T-34\tHow many <<unk>> _ before the <<unk>>\n",
            "H-34\t-0.3720472753047943\tShe _ before the exam?\n",
            "P-34\t-0.7481 -0.0672 -0.7168 -0.0838 -0.6156 -0.0008\n",
            "S-3\tIn your opinion, will she be a <unk> <unk>\n",
            "T-3\tIn your opinion, _ a <<unk>> <<unk>>\n",
            "H-3\t-0.4996446967124939\tIn my opinion, _ a doctor.\n",
            "P-3\t-0.3686 -0.7527 -1.2932 -0.1094 -0.0440 -0.9285 -0.0012\n",
            "S-36\tWhen you got <unk> had you been eating <unk>\n",
            "T-36\tWhen you got <<unk>> _ <<unk>>\n",
            "H-36\t-0.43253040313720703\tHave you _ Turkish coffee?\n",
            "P-36\t-0.9950 -0.0226 -0.1283 -0.6762 -0.7723 -0.0008\n",
            "S-30\tYou have bought a lot of new <unk> <unk>\n",
            "T-30\tYou have _ a lot of new <<unk>> <<unk>>\n",
            "H-30\t-0.7087547183036804\tYou _ a cake, for cake,\n",
            "P-30\t-0.4149 -0.0072 -0.0956 -1.4018 -0.5314 -2.1702 -0.3401\n",
            "S-23\tYou were in the <unk> when I called you.\n",
            "T-23\tYou _ in the <<unk>> when I <<unk>> you.\n",
            "H-23\t-0.3793428838253021\t5. _ in London for three when I I retire.\n",
            "P-23\t-0.1653 -0.3967 -0.0016 -0.3817 -0.0210 -0.8005 -0.0172 -0.2249 -0.4564 -1.7074 -0.0002\n",
            "S-8\tShe has <unk> about the <unk> for three <unk>\n",
            "T-8\tShe has _ about the <<unk>> for three <<unk>>\n",
            "H-8\t-0.4514073431491852\tShe _ sick for three years years week,\n",
            "P-8\t-0.0033 -0.0203 -2.2159 -0.0049 -0.1322 -0.0076 -0.9539 -0.6467 -0.0778\n",
            "S-33\tI hadn't been working there long when she <unk>\n",
            "T-33\tI _ there long when she <<unk>>\n",
            "H-33\t-0.3365252614021301\tYou _ here when when came.\n",
            "P-33\t-0.2683 -0.0007 -0.3255 -0.3973 -0.7142 -0.6490 -0.0007\n",
            "S-18\tWe have <unk> this <unk> for three years.\n",
            "T-18\tWe have _ this <<unk>> for three years.\n",
            "H-18\t-0.3374011814594269\tWe _ sick for three years next week,\n",
            "P-18\t-0.0359 -0.0015 -0.6748 -0.0026 -0.1256 -0.0176 -1.1577 -0.9821 -0.0387\n",
            "S-32\tThey weren't on the bus when I <unk>\n",
            "T-32\tThey _ on the bus when I <<unk>>\n",
            "H-32\t-0.49613475799560547\tThey _ on the library.\n",
            "P-32\t-0.0269 -0.0003 -0.8035 -0.0015 -2.1425 -0.0021\n",
            "S-49\tThe meeting will take place at 6 <unk>\n",
            "T-49\tThe meeting _ place at 6 <<unk>>\n",
            "H-49\t-0.45873594284057617\tJulie _ at the train.\n",
            "P-49\t-0.4597 -0.0001 -0.0570 -0.1040 -2.1294 -0.0022\n",
            "S-11\tHave you ever <unk> in the <unk> <unk>\n",
            "T-11\tHave you ever _ in the <<unk>> <<unk>>\n",
            "H-11\t-0.2833261489868164\tHave you _ in the garden.\n",
            "P-11\t-0.4590 -0.0303 -0.0176 -0.6365 -0.0006 -0.8393 -0.0001\n",
            "S-12\tI'm <unk> I'm so tired. I haven't <unk>\n",
            "T-12\tI'm <<unk>> I'm so tired. I haven't _.\n",
            "H-12\t-0.6056636571884155\tI _ tired I'll I I you?\n",
            "P-12\t-0.0383 -0.2261 -2.0239 -0.7690 -0.3892 -0.6823 -0.7160 -0.0004\n",
            "S-0\tAre you <unk> I have felt <unk>\n",
            "T-0\tAre you <<unk>> I have _ <<unk>>\n",
            "H-0\t-0.4574136734008789\tHave you _ Turkish coffee?\n",
            "P-0\t-0.5051 -0.0451 -0.1598 -1.1382 -0.8956 -0.0008\n",
            "S-27\tThey put their <unk> in the <unk>\n",
            "T-27\tThey _ their <<unk>> in the <<unk>>\n",
            "H-27\t-0.24500450491905212\tThey _ in the garden.\n",
            "P-27\t-0.0014 -0.0003 -0.3067 -0.0039 -1.1565 -0.0013\n",
            "S-42\tHad they <unk> by <unk> train <unk>\n",
            "T-42\t_ by <<unk>> train <<unk>>\n",
            "H-42\t-0.16879916191101074\t_ by 6?\n",
            "P-42\t-0.0002 -0.0986 -0.5733 -0.0031\n",
            "S-6\tWhat time will the <unk> <unk> <unk>\n",
            "T-6\tWhat time _ <<unk>>\n",
            "H-6\t-0.5793647766113281\tJoan _ the report?\n",
            "P-6\t-1.3147 -0.0012 -0.0007 -1.5792 -0.0010\n",
            "S-19\tWhy had you been studying so hard?\n",
            "T-19\t_ so hard?\n",
            "H-19\t-0.7481563091278076\t_ by 6?\n",
            "P-19\t-0.0184 -1.3733 -1.5984 -0.0025\n",
            "S-48\tYou will have <unk> the <unk>\n",
            "T-48\tYou _ the <<unk>>\n",
            "H-48\t-0.3948667645454407\tYou _ the emails.\n",
            "P-48\t-0.7212 -0.0000 -0.0016 -1.2513 -0.0002\n",
            "S-50\tWe will have read the <unk>\n",
            "T-50\tWe _ the <<unk>>\n",
            "H-50\t-0.29356977343559265\tThey _ the office.\n",
            "P-50\t-0.0976 -0.0000 -0.0023 -1.3678 -0.0001\n",
            "S-1\tWill she have arrived by <unk>\n",
            "T-1\t_ by <<unk>>\n",
            "H-1\t-0.271101713180542\t_ by Tuesday?\n",
            "P-1\t-0.0002 -0.0042 -1.0780 -0.0020\n",
            "S-10\tI sent you an <unk> <unk>\n",
            "T-10\tI _ you an <<unk>> <<unk>>\n",
            "H-10\t-0.2675466537475586\tI _ your university yet?\n",
            "P-10\t-0.2163 -0.0026 -0.6225 -0.5341 -0.2295 -0.0002\n",
            "S-7\tHe had met her before <unk>\n",
            "T-7\tHe _ her before <<unk>>\n",
            "H-7\t-0.41929322481155396\tShe _ her homework\n",
            "P-7\t-0.1469 -0.0010 -0.2506 -1.5311 -0.1669\n",
            "S-21\tI will come and <unk> you.\n",
            "T-21\tI _ and <<unk>> you.\n",
            "H-21\t-0.3395099639892578\tI _ it.\n",
            "P-21\t-0.0098 -0.0002 -1.3427 -0.0055\n",
            "S-9\tYou lost your keys last week.\n",
            "T-9\tYou _ your keys last week.\n",
            "H-9\t-0.45365697145462036\tI _ my umbrella\n",
            "P-9\t-0.3888 -0.0012 -0.1554 -0.8826 -0.8403\n",
            "S-17\tWill they be <unk> a meeting?\n",
            "T-17\t_ a meeting?\n",
            "H-17\t-0.22627878189086914\t_ a doctor.\n",
            "P-17\t-0.0216 -0.0027 -0.8781 -0.0027\n",
            "S-38\tWe lent John <unk> .\n",
            "T-38\tWe _ John <<unk>>\n",
            "H-38\t-0.6284852027893066\tLucy _ sleepy.\n",
            "P-38\t-0.8279 -0.0000 -1.6600 -0.0260\n",
            "S-24\tWill he be <unk> coffee?\n",
            "T-24\t_ coffee?\n",
            "H-24\t-0.1043609008193016\t_ lunch?\n",
            "P-24\t-0.0003 -0.3100 -0.0028\n",
            "S-39\t<unk> <unk> after the bus.\n",
            "T-39\t<<unk>> _ after the bus.\n",
            "H-39\t-0.6306322813034058\tHe _ on the prawns.\n",
            "P-39\t-0.5083 -0.0074 -1.4286 -0.0009 -1.8385 -0.0001\n",
            "S-4\tIt was cold last night.\n",
            "T-4\tIt _ cold last night.\n",
            "H-4\t-0.16892783343791962\tIt _ cold last night.\n",
            "P-4\t-0.0018 -0.0005 -0.0762 -0.7233 -0.2115 -0.0003\n",
            "S-29\tYou will have fallen <unk>\n",
            "T-29\tYou _ <<unk>>\n",
            "H-29\t-0.5447378158569336\tThey _ snowing.\n",
            "P-29\t-1.0351 -0.0000 -1.1437 -0.0002\n",
            "S-5\tIt will be <unk>\n",
            "T-5\tIt _.\n",
            "H-5\t-0.08068975061178207\tYou _./pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "\n",
            "P-5\t-0.2400 -0.0016 -0.0004\n",
            "S-16\tIt will be snowing.\n",
            "T-16\tIt _.\n",
            "H-16\t-0.2367064207792282\tYou _.\n",
            "P-16\t-0.7079 -0.0019 -0.0003\n",
            "S-40\tWill you be <unk>\n",
            "T-40\t_?\n",
            "H-40\t-0.30997657775878906\t_ here?\n",
            "P-40\t-0.0006 -0.9258 -0.0035\n",
            "S-41\tYou aren't an <unk>\n",
            "T-41\tYou _ an <<unk>>\n",
            "H-41\t-0.26601409912109375\tYou _ late.\n",
            "P-41\t-0.1634 -0.0000 -0.9005 -0.0001\n",
            "S-13\tYou aren't a teacher.\n",
            "T-13\tYou _ a teacher.\n",
            "H-13\t-0.23066997528076172\tYou _ a teacher.\n",
            "P-13\t-0.0622 -0.0000 -0.0004 -1.0904 -0.0003\n",
            "S-35\tWe aren't thirsty.\n",
            "T-35\tWe _ thirsty.\n",
            "H-35\t-0.12230110168457031\tWe _ late.\n",
            "P-35\t-0.0223 -0.0000 -0.4668 -0.0001\n",
            "S-2\tI am <unk>\n",
            "T-2\tI _ <<unk>>\n",
            "H-2\t-0.02842116355895996\tI _ thirsty.\n",
            "P-2\t-0.0006 -0.0009 -0.1117 -0.0005\n",
            "S-43\tYou are <unk>\n",
            "T-43\tYou _ <<unk>>\n",
            "H-43\t-0.10206103324890137\tYou _ tired.\n",
            "P-43\t-0.0481 -0.0005 -0.3595 -0.0001\n",
            "S-22\tWe are hungry.\n",
            "T-22\tWe _ hungry.\n",
            "H-22\t-0.11888623237609863\tWe _ tired.\n",
            "P-22\t-0.0581 -0.0001 -0.4172 -0.0002\n",
            "| Translated 51 sentences (316 tokens) in 0.1s (811.66 sentences/s, 5029.08 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 10.12, 48.7/21.0/7.4/3.4 (BP=0.797, ratio=0.815, syslen=265, reflen=325)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO_iBdrblD2O",
        "colab_type": "text"
      },
      "source": [
        "## CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz-BH9ckhgcW",
        "colab_type": "code",
        "outputId": "a8cb1dad-38cc-4856-8279-01e4f8e69842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -n 50 -m CNN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, arch='fconv_iwslt_de_en', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='cross_entropy', curriculum=0, data='data/fairseq_binaries', dataset_impl=None, ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=300, decoder_embed_path='glove.6B.300d.txt', decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=300, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_embed_dim=300, encoder_embed_path='glove.6B.300d.txt', encoder_layers='[(256, 3)] * 4', fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=50, max_sentences=64, max_sentences_valid=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, num_workers=3, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/conv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n",
            "| [sentence] dictionary: 664 types\n",
            "| [question] dictionary: 552 types\n",
            "| loaded 29 examples from: data/fairseq_binaries/valid.sentence-question.sentence\n",
            "| loaded 29 examples from: data/fairseq_binaries/valid.sentence-question.question\n",
            "| data/fairseq_binaries valid sentence-question 29 examples\n",
            "| Found 335/664 types in embedding file.\n",
            "| Found 247/552 types in embedding file.\n",
            "FConvModel(\n",
            "(encoder): FConvEncoder(\n",
            "(embed_tokens): Embedding(664, 300, padding_idx=1)\n",
            "(embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "(fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "(projections): ModuleList(\n",
            "(0): None\n",
            "(1): None\n",
            "(2): None\n",
            "(3): None\n",
            ")\n",
            "(convolutions): ModuleList(\n",
            "(0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "(1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "(2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            "(3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))\n",
            ")\n",
            "(fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            ")\n",
            "(decoder): FConvDecoder(\n",
            "(embed_tokens): Embedding(552, 300, padding_idx=1)\n",
            "(embed_positions): LearnedPositionalEmbedding(1024, 300, padding_idx=1)\n",
            "(fc1): Linear(in_features=300, out_features=256, bias=True)\n",
            "(projections): ModuleList(\n",
            "(0): None\n",
            "(1): None\n",
            "(2): None\n",
            ")\n",
            "(convolutions): ModuleList(\n",
            "(0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "(1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            "(2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))\n",
            ")\n",
            "(attention): ModuleList(\n",
            "(0): AttentionLayer(\n",
            "(in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "(out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            ")\n",
            "(1): AttentionLayer(\n",
            "(in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "(out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            ")\n",
            "(2): AttentionLayer(\n",
            "(in_projection): Linear(in_features=256, out_features=300, bias=True)\n",
            "(out_projection): Linear(in_features=300, out_features=256, bias=True)\n",
            ")\n",
            ")\n",
            "(fc2): Linear(in_features=256, out_features=300, bias=True)\n",
            "(fc3): Linear(in_features=300, out_features=552, bias=True)\n",
            ")\n",
            ")\n",
            "| model fconv_iwslt_de_en, criterion CrossEntropyCriterion\n",
            "| num. model params: 4679144 (num. trained: 4679144)\n",
            "| training on 1 GPUs\n",
            "| max tokens per GPU = None and max sentences per GPU = 64\n",
            "| no existing checkpoint found checkpoints/conv/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 260 examples from: data/fairseq_binaries/train.sentence-question.sentence\n",
            "| loaded 260 examples from: data/fairseq_binaries/train.sentence-question.question\n",
            "| data/fairseq_binaries train sentence-question 260 examples\n",
            "| epoch 001 | loss 8.643 | ppl 399.7 | wps 11146 | ups 31 | wpb 386.000 | bsz 52.000 | num_updates 5 | lr 0.001 | gnorm 2.829 | clip 0.000 | oom 0.000 | wall 0 | train_wall 0\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "add_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "add_(Tensor other, *, Number alpha)\n",
            "| epoch 001 | valid on 'valid' subset | loss 7.354 | ppl 163.62 | num_updates 5\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 1 @ 5 updates) (writing took 0.13306617736816406 seconds)\n",
            "| epoch 002 | loss 8.212 | ppl 296.56 | wps 8612 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 10 | lr 0.001 | gnorm 3.642 | clip 0.000 | oom 0.000 | wall 1 | train_wall 0\n",
            "| epoch 002 | valid on 'valid' subset | loss 7.430 | ppl 172.41 | num_updates 10 | best_loss 7.3542\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 2 @ 10 updates) (writing took 0.08638811111450195 seconds)\n",
            "| epoch 003 | loss 7.231 | ppl 150.21 | wps 12005 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 15 | lr 0.001 | gnorm 4.158 | clip 0.000 | oom 0.000 | wall 1 | train_wall 0\n",
            "| epoch 003 | valid on 'valid' subset | loss 6.663 | ppl 101.3 | num_updates 15 | best_loss 6.66256\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 3 @ 15 updates) (writing took 1.1890897750854492 seconds)\n",
            "| epoch 004 | loss 6.950 | ppl 123.67 | wps 12579 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 20 | lr 0.001 | gnorm 2.290 | clip 0.000 | oom 0.000 | wall 3 | train_wall 0\n",
            "| epoch 004 | valid on 'valid' subset | loss 6.572 | ppl 95.15 | num_updates 20 | best_loss 6.57211\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 4 @ 20 updates) (writing took 2.7164080142974854 seconds)\n",
            "| epoch 005 | loss 6.699 | ppl 103.92 | wps 8476 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 25 | lr 0.001 | gnorm 2.420 | clip 0.000 | oom 0.000 | wall 6 | train_wall 1\n",
            "| epoch 005 | valid on 'valid' subset | loss 6.417 | ppl 85.45 | num_updates 25 | best_loss 6.41701\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 5 @ 25 updates) (writing took 1.1975624561309814 seconds)\n",
            "| epoch 006 | loss 6.489 | ppl 89.81 | wps 12023 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 30 | lr 0.001 | gnorm 2.454 | clip 0.000 | oom 0.000 | wall 8 | train_wall 1\n",
            "| epoch 006 | valid on 'valid' subset | loss 6.333 | ppl 80.6 | num_updates 30 | best_loss 6.3328\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 6 @ 30 updates) (writing took 2.7193267345428467 seconds)\n",
            "| epoch 007 | loss 6.274 | ppl 77.39 | wps 14367 | ups 31 | wpb 386.000 | bsz 52.000 | num_updates 35 | lr 0.001 | gnorm 2.552 | clip 0.000 | oom 0.000 | wall 11 | train_wall 1\n",
            "| epoch 007 | valid on 'valid' subset | loss 6.110 | ppl 69.06 | num_updates 35 | best_loss 6.10973\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 7 @ 35 updates) (writing took 1.1424498558044434 seconds)\n",
            "| epoch 008 | loss 6.061 | ppl 66.77 | wps 8028 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 40 | lr 0.001 | gnorm 2.554 | clip 0.000 | oom 0.000 | wall 12 | train_wall 1\n",
            "| epoch 008 | valid on 'valid' subset | loss 6.008 | ppl 64.37 | num_updates 40 | best_loss 6.00842\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 8 @ 40 updates) (writing took 2.6999552249908447 seconds)\n",
            "| epoch 009 | loss 5.831 | ppl 56.91 | wps 10709 | ups 30 | wpb 386.000 | bsz 52.000 | num_updates 45 | lr 0.001 | gnorm 2.278 | clip 0.000 | oom 0.000 | wall 15 | train_wall 1\n",
            "| epoch 009 | valid on 'valid' subset | loss 5.939 | ppl 61.33 | num_updates 45 | best_loss 5.93857\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 9 @ 45 updates) (writing took 2.567455291748047 seconds)\n",
            "| epoch 010 | loss 5.621 | ppl 49.23 | wps 8295 | ups 23 | wpb 386.000 | bsz 52.000 | num_updates 50 | lr 0.001 | gnorm 2.430 | clip 0.000 | oom 0.000 | wall 18 | train_wall 1\n",
            "| epoch 010 | valid on 'valid' subset | loss 5.961 | ppl 62.27 | num_updates 50 | best_loss 5.93857\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 10 @ 50 updates) (writing took 0.21248245239257812 seconds)\n",
            "| epoch 011 | loss 5.499 | ppl 45.24 | wps 11866 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 55 | lr 0.001 | gnorm 2.306 | clip 0.000 | oom 0.000 | wall 19 | train_wall 1\n",
            "| epoch 011 | valid on 'valid' subset | loss 5.885 | ppl 59.08 | num_updates 55 | best_loss 5.88469\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 11 @ 55 updates) (writing took 1.136995553970337 seconds)\n",
            "| epoch 012 | loss 5.315 | ppl 39.8 | wps 11276 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 60 | lr 0.001 | gnorm 2.327 | clip 0.000 | oom 0.000 | wall 20 | train_wall 1\n",
            "| epoch 012 | valid on 'valid' subset | loss 5.830 | ppl 56.88 | num_updates 60 | best_loss 5.82974\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 12 @ 60 updates) (writing took 1.2840886116027832 seconds)\n",
            "| epoch 013 | loss 5.157 | ppl 35.69 | wps 11310 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 65 | lr 0.001 | gnorm 2.681 | clip 0.000 | oom 0.000 | wall 22 | train_wall 1\n",
            "| epoch 013 | valid on 'valid' subset | loss 5.938 | ppl 61.32 | num_updates 65 | best_loss 5.82974\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 13 @ 65 updates) (writing took 2.681520938873291 seconds)\n",
            "| epoch 014 | loss 5.066 | ppl 33.5 | wps 9034 | ups 25 | wpb 386.000 | bsz 52.000 | num_updates 70 | lr 0.001 | gnorm 2.584 | clip 0.000 | oom 0.000 | wall 25 | train_wall 2\n",
            "| epoch 014 | valid on 'valid' subset | loss 5.866 | ppl 58.34 | num_updates 70 | best_loss 5.82974\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 14 @ 70 updates) (writing took 0.09711360931396484 seconds)\n",
            "| epoch 015 | loss 4.810 | ppl 28.04 | wps 12148 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 75 | lr 0.001 | gnorm 2.407 | clip 0.000 | oom 0.000 | wall 26 | train_wall 2\n",
            "| epoch 015 | valid on 'valid' subset | loss 5.949 | ppl 61.76 | num_updates 75 | best_loss 5.82974\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 15 @ 75 updates) (writing took 1.199944019317627 seconds)\n",
            "| epoch 016 | loss 4.644 | ppl 25 | wps 13114 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 80 | lr 0.001 | gnorm 2.438 | clip 0.000 | oom 0.000 | wall 27 | train_wall 2\n",
            "| epoch 016 | valid on 'valid' subset | loss 5.772 | ppl 54.64 | num_updates 80 | best_loss 5.772\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 16 @ 80 updates) (writing took 1.1906239986419678 seconds)\n",
            "| epoch 017 | loss 4.468 | ppl 22.13 | wps 14113 | ups 30 | wpb 386.000 | bsz 52.000 | num_updates 85 | lr 0.001 | gnorm 2.514 | clip 0.000 | oom 0.000 | wall 29 | train_wall 2\n",
            "| epoch 017 | valid on 'valid' subset | loss 5.721 | ppl 52.74 | num_updates 85 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_best.pt (epoch 17 @ 85 updates) (writing took 2.7577602863311768 seconds)\n",
            "| epoch 018 | loss 4.299 | ppl 19.69 | wps 12000 | ups 31 | wpb 386.000 | bsz 52.000 | num_updates 90 | lr 0.001 | gnorm 2.258 | clip 0.000 | oom 0.000 | wall 32 | train_wall 2\n",
            "| epoch 018 | valid on 'valid' subset | loss 5.739 | ppl 53.42 | num_updates 90 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 18 @ 90 updates) (writing took 0.09136509895324707 seconds)\n",
            "| epoch 019 | loss 4.016 | ppl 16.18 | wps 9999 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 95 | lr 0.001 | gnorm 2.711 | clip 0.000 | oom 0.000 | wall 32 | train_wall 2\n",
            "| epoch 019 | valid on 'valid' subset | loss 5.741 | ppl 53.47 | num_updates 95 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 19 @ 95 updates) (writing took 1.1445505619049072 seconds)\n",
            "| epoch 020 | loss 3.852 | ppl 14.44 | wps 13387 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 100 | lr 0.001 | gnorm 2.174 | clip 0.000 | oom 0.000 | wall 34 | train_wall 2\n",
            "| epoch 020 | valid on 'valid' subset | loss 5.852 | ppl 57.78 | num_updates 100 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 20 @ 100 updates) (writing took 1.2509639263153076 seconds)\n",
            "| epoch 021 | loss 3.700 | ppl 13 | wps 9894 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 105 | lr 0.001 | gnorm 2.489 | clip 0.000 | oom 0.000 | wall 36 | train_wall 2\n",
            "| epoch 021 | valid on 'valid' subset | loss 5.767 | ppl 54.46 | num_updates 105 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 21 @ 105 updates) (writing took 0.08933877944946289 seconds)\n",
            "| epoch 022 | loss 3.565 | ppl 11.83 | wps 13443 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 110 | lr 0.001 | gnorm 2.498 | clip 0.000 | oom 0.000 | wall 36 | train_wall 2\n",
            "| epoch 022 | valid on 'valid' subset | loss 5.907 | ppl 60.01 | num_updates 110 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 22 @ 110 updates) (writing took 1.1811730861663818 seconds)\n",
            "| epoch 023 | loss 3.396 | ppl 10.53 | wps 13178 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 115 | lr 0.001 | gnorm 2.424 | clip 0.000 | oom 0.000 | wall 38 | train_wall 3\n",
            "| epoch 023 | valid on 'valid' subset | loss 6.134 | ppl 70.24 | num_updates 115 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 23 @ 115 updates) (writing took 1.207937479019165 seconds)\n",
            "| epoch 024 | loss 3.189 | ppl 9.12 | wps 12334 | ups 30 | wpb 386.000 | bsz 52.000 | num_updates 120 | lr 0.001 | gnorm 2.200 | clip 0.000 | oom 0.000 | wall 39 | train_wall 3\n",
            "| epoch 024 | valid on 'valid' subset | loss 5.979 | ppl 63.05 | num_updates 120 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 24 @ 120 updates) (writing took 1.156954288482666 seconds)\n",
            "| epoch 025 | loss 2.971 | ppl 7.84 | wps 12801 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 125 | lr 0.001 | gnorm 2.080 | clip 0.000 | oom 0.000 | wall 41 | train_wall 3\n",
            "| epoch 025 | valid on 'valid' subset | loss 5.974 | ppl 62.84 | num_updates 125 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 25 @ 125 updates) (writing took 0.08634233474731445 seconds)\n",
            "| epoch 026 | loss 2.843 | ppl 7.17 | wps 10366 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 130 | lr 0.001 | gnorm 2.385 | clip 0.000 | oom 0.000 | wall 41 | train_wall 3\n",
            "| epoch 026 | valid on 'valid' subset | loss 5.920 | ppl 60.56 | num_updates 130 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 26 @ 130 updates) (writing took 1.194640874862671 seconds)\n",
            "| epoch 027 | loss 2.624 | ppl 6.16 | wps 8989 | ups 25 | wpb 386.000 | bsz 52.000 | num_updates 135 | lr 0.001 | gnorm 2.169 | clip 0.000 | oom 0.000 | wall 43 | train_wall 3\n",
            "| epoch 027 | valid on 'valid' subset | loss 5.946 | ppl 61.66 | num_updates 135 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 27 @ 135 updates) (writing took 1.1933603286743164 seconds)\n",
            "| epoch 028 | loss 2.394 | ppl 5.26 | wps 10346 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 140 | lr 0.001 | gnorm 1.973 | clip 0.000 | oom 0.000 | wall 44 | train_wall 3\n",
            "| epoch 028 | valid on 'valid' subset | loss 6.071 | ppl 67.25 | num_updates 140 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 28 @ 140 updates) (writing took 1.1853065490722656 seconds)\n",
            "| epoch 029 | loss 2.296 | ppl 4.91 | wps 8108 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 145 | lr 0.001 | gnorm 1.790 | clip 0.000 | oom 0.000 | wall 46 | train_wall 3\n",
            "| epoch 029 | valid on 'valid' subset | loss 6.132 | ppl 70.13 | num_updates 145 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 29 @ 145 updates) (writing took 0.0899345874786377 seconds)\n",
            "| epoch 030 | loss 2.108 | ppl 4.31 | wps 11137 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 150 | lr 0.001 | gnorm 1.832 | clip 0.000 | oom 0.000 | wall 46 | train_wall 3\n",
            "| epoch 030 | valid on 'valid' subset | loss 6.207 | ppl 73.86 | num_updates 150 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 30 @ 150 updates) (writing took 1.171175479888916 seconds)\n",
            "| epoch 031 | loss 1.959 | ppl 3.89 | wps 13405 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 155 | lr 0.001 | gnorm 2.053 | clip 0.000 | oom 0.000 | wall 48 | train_wall 3\n",
            "| epoch 031 | valid on 'valid' subset | loss 6.081 | ppl 67.68 | num_updates 155 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 31 @ 155 updates) (writing took 1.1919236183166504 seconds)\n",
            "| epoch 032 | loss 1.832 | ppl 3.56 | wps 10703 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 160 | lr 0.001 | gnorm 1.998 | clip 0.000 | oom 0.000 | wall 50 | train_wall 4\n",
            "| epoch 032 | valid on 'valid' subset | loss 6.176 | ppl 72.31 | num_updates 160 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 32 @ 160 updates) (writing took 1.1548869609832764 seconds)\n",
            "| epoch 033 | loss 1.683 | ppl 3.21 | wps 11972 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 165 | lr 0.001 | gnorm 1.754 | clip 0.000 | oom 0.000 | wall 51 | train_wall 4\n",
            "| epoch 033 | valid on 'valid' subset | loss 6.318 | ppl 79.76 | num_updates 165 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 33 @ 165 updates) (writing took 0.09030008316040039 seconds)\n",
            "| epoch 034 | loss 1.547 | ppl 2.92 | wps 10676 | ups 30 | wpb 386.000 | bsz 52.000 | num_updates 170 | lr 0.001 | gnorm 2.007 | clip 0.000 | oom 0.000 | wall 52 | train_wall 4\n",
            "| epoch 034 | valid on 'valid' subset | loss 6.208 | ppl 73.9 | num_updates 170 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 34 @ 170 updates) (writing took 1.1904830932617188 seconds)\n",
            "| epoch 035 | loss 1.582 | ppl 2.99 | wps 8300 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 175 | lr 0.001 | gnorm 1.852 | clip 0.000 | oom 0.000 | wall 53 | train_wall 4\n",
            "| epoch 035 | valid on 'valid' subset | loss 6.314 | ppl 79.58 | num_updates 175 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 35 @ 175 updates) (writing took 1.191021203994751 seconds)\n",
            "| epoch 036 | loss 1.448 | ppl 2.73 | wps 9599 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 180 | lr 0.001 | gnorm 1.779 | clip 0.000 | oom 0.000 | wall 55 | train_wall 4\n",
            "| epoch 036 | valid on 'valid' subset | loss 6.263 | ppl 76.78 | num_updates 180 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 36 @ 180 updates) (writing took 1.1405036449432373 seconds)\n",
            "| epoch 037 | loss 1.295 | ppl 2.45 | wps 10560 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 185 | lr 0.001 | gnorm 1.931 | clip 0.000 | oom 0.000 | wall 56 | train_wall 4\n",
            "| epoch 037 | valid on 'valid' subset | loss 6.301 | ppl 78.86 | num_updates 185 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 37 @ 185 updates) (writing took 0.09015417098999023 seconds)\n",
            "| epoch 038 | loss 1.146 | ppl 2.21 | wps 9958 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 190 | lr 0.001 | gnorm 1.708 | clip 0.000 | oom 0.000 | wall 57 | train_wall 4\n",
            "| epoch 038 | valid on 'valid' subset | loss 6.441 | ppl 86.88 | num_updates 190 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 38 @ 190 updates) (writing took 1.1771647930145264 seconds)\n",
            "| epoch 039 | loss 1.174 | ppl 2.26 | wps 10006 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 195 | lr 0.001 | gnorm 1.821 | clip 0.000 | oom 0.000 | wall 58 | train_wall 4\n",
            "| epoch 039 | valid on 'valid' subset | loss 6.389 | ppl 83.81 | num_updates 195 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 39 @ 195 updates) (writing took 1.190441608428955 seconds)\n",
            "| epoch 040 | loss 1.032 | ppl 2.04 | wps 11251 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 200 | lr 0.001 | gnorm 1.324 | clip 0.000 | oom 0.000 | wall 60 | train_wall 4\n",
            "| epoch 040 | valid on 'valid' subset | loss 6.460 | ppl 88.04 | num_updates 200 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 40 @ 200 updates) (writing took 1.1316015720367432 seconds)\n",
            "| epoch 041 | loss 0.921 | ppl 1.89 | wps 10281 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 205 | lr 0.001 | gnorm 1.713 | clip 0.000 | oom 0.000 | wall 62 | train_wall 4\n",
            "| epoch 041 | valid on 'valid' subset | loss 6.449 | ppl 87.35 | num_updates 205 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 41 @ 205 updates) (writing took 0.08758878707885742 seconds)\n",
            "| epoch 042 | loss 0.882 | ppl 1.84 | wps 8994 | ups 29 | wpb 386.000 | bsz 52.000 | num_updates 210 | lr 0.001 | gnorm 1.415 | clip 0.000 | oom 0.000 | wall 62 | train_wall 5\n",
            "| epoch 042 | valid on 'valid' subset | loss 6.579 | ppl 95.61 | num_updates 210 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 42 @ 210 updates) (writing took 1.3905889987945557 seconds)\n",
            "| epoch 043 | loss 0.858 | ppl 1.81 | wps 11617 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 215 | lr 0.001 | gnorm 1.653 | clip 0.000 | oom 0.000 | wall 64 | train_wall 5\n",
            "| epoch 043 | valid on 'valid' subset | loss 6.565 | ppl 94.71 | num_updates 215 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 43 @ 215 updates) (writing took 0.9946112632751465 seconds)\n",
            "| epoch 044 | loss 0.766 | ppl 1.7 | wps 10558 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 220 | lr 0.001 | gnorm 2.036 | clip 0.000 | oom 0.000 | wall 65 | train_wall 5\n",
            "| epoch 044 | valid on 'valid' subset | loss 6.475 | ppl 88.97 | num_updates 220 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 44 @ 220 updates) (writing took 1.127274990081787 seconds)\n",
            "| epoch 045 | loss 0.752 | ppl 1.68 | wps 7183 | ups 23 | wpb 386.000 | bsz 52.000 | num_updates 225 | lr 0.001 | gnorm 1.521 | clip 0.000 | oom 0.000 | wall 67 | train_wall 5\n",
            "| epoch 045 | valid on 'valid' subset | loss 6.834 | ppl 114.05 | num_updates 225 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 45 @ 225 updates) (writing took 0.09178900718688965 seconds)\n",
            "| epoch 046 | loss 0.768 | ppl 1.7 | wps 9180 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 230 | lr 0.001 | gnorm 2.072 | clip 0.000 | oom 0.000 | wall 67 | train_wall 5\n",
            "| epoch 046 | valid on 'valid' subset | loss 6.598 | ppl 96.89 | num_updates 230 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 46 @ 230 updates) (writing took 1.1684587001800537 seconds)\n",
            "| epoch 047 | loss 0.680 | ppl 1.6 | wps 9868 | ups 28 | wpb 386.000 | bsz 52.000 | num_updates 235 | lr 0.001 | gnorm 1.681 | clip 0.000 | oom 0.000 | wall 69 | train_wall 5\n",
            "| epoch 047 | valid on 'valid' subset | loss 6.532 | ppl 92.54 | num_updates 235 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 47 @ 235 updates) (writing took 1.1916389465332031 seconds)\n",
            "| epoch 048 | loss 0.612 | ppl 1.53 | wps 12536 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 240 | lr 0.001 | gnorm 1.368 | clip 0.000 | oom 0.000 | wall 70 | train_wall 5\n",
            "| epoch 048 | valid on 'valid' subset | loss 6.825 | ppl 113.4 | num_updates 240 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 48 @ 240 updates) (writing took 1.136800765991211 seconds)\n",
            "| epoch 049 | loss 0.644 | ppl 1.56 | wps 8344 | ups 27 | wpb 386.000 | bsz 52.000 | num_updates 245 | lr 0.001 | gnorm 2.004 | clip 0.000 | oom 0.000 | wall 72 | train_wall 5\n",
            "| epoch 049 | valid on 'valid' subset | loss 6.773 | ppl 109.37 | num_updates 245 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 49 @ 245 updates) (writing took 0.08921957015991211 seconds)\n",
            "| epoch 050 | loss 0.618 | ppl 1.53 | wps 9393 | ups 26 | wpb 386.000 | bsz 52.000 | num_updates 250 | lr 0.001 | gnorm 1.240 | clip 0.000 | oom 0.000 | wall 72 | train_wall 6\n",
            "| epoch 050 | valid on 'valid' subset | loss 6.879 | ppl 117.67 | num_updates 250 | best_loss 5.72095\n",
            "| saved checkpoint checkpoints/conv/checkpoint_last.pt (epoch 50 @ 250 updates) (writing took 1.1803882122039795 seconds)\n",
            "| done training in 73.7 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1F40LRzlLyh",
        "colab_type": "text"
      },
      "source": [
        "### Generate Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoOpk-xkiE9d",
        "colab_type": "code",
        "outputId": "783010b4-3025-4649-a4c2-0871696d6828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python generate.py -m CNN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(beam=3, bpe=None, cpu=False, criterion='cross_entropy', data='data/fairseq_binaries', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=64, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='checkpoints/conv/checkpoint_last.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\n",
            "| [sentence] dictionary: 664 types\n",
            "| [question] dictionary: 552 types\n",
            "| loaded 51 examples from: data/fairseq_binaries/test.sentence-question.sentence\n",
            "| loaded 51 examples from: data/fairseq_binaries/test.sentence-question.question\n",
            "| data/fairseq_binaries test sentence-question 51 examples\n",
            "| loading model(s) from checkpoints/conv/checkpoint_last.pt\n",
            "| Found 335/664 types in embedding file.\n",
            "| Found 247/552 types in embedding file.\n",
            "  0% 0/1 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "S-20\tI'd got a <unk> new bike for my <unk> so I was <unk> to <unk> it.\n",
            "T-20\tI'd _ a <<unk>> new bike for my <<unk>> so I was <<unk>> to <<unk>> it.\n",
            "H-20\t-0.5073559284210205\tWe _ a Beijing for dinner. years next relax four years at most.\n",
            "P-20\t-1.8820 -0.0199 -0.0023 -1.4727 -0.0937 -0.2659 -0.3691 -0.9458 -0.6409 -0.1419 -0.0014 -1.2592 -0.0082 -0.0000\n",
            "S-46\tHe was in <unk> with the <unk> because he hadn't been going to <unk>\n",
            "T-46\t<<unk>> He was in <<unk>> with the <<unk>> because he _ to <<unk>>\n",
            "H-46\t-0.2594870626926422\tHe _ here for ten years when he lost married.\n",
            "P-46\t-0.0010 -0.0064 -1.0643 -0.0245 -0.6990 -0.0209 -0.0121 -0.1886 -0.1886 -0.6492 0.0000\n",
            "S-44\tWhen we <unk> you had been working at that company for six <unk>\n",
            "T-44\tWhen we <<unk>> you _ at that company for six <<unk>>\n",
            "H-44\t-0.2746877670288086\tWhen _ for very long when we arrive to Paris.\n",
            "P-44\t-0.0106 -0.0355 -0.3545 -1.0441 -0.2425 -0.0078 -0.0435 -0.3245 -0.6270 -0.3316 -0.0000\n",
            "S-14\tHe hadn't <unk> <unk> <unk> so I <unk> him <unk> to <unk> it.\n",
            "T-14\tHe _ <<unk>> <<unk>> so I <<unk>> him <<unk>> to <<unk>> it.\n",
            "H-14\t-0.4456745684146881\tHe _ for ten hours when I retire.\n",
            "P-14\t-0.0112 -0.0545 -1.3222 -1.4118 -0.5414 -0.0940 -0.0143 -0.5575 -0.0042\n",
            "S-26\tDavid <unk> into the <unk> He had <unk> on a <unk> <unk>\n",
            "T-26\tDavid <<unk>> into the <<unk>> He had _ on a <<unk>> <<unk>>\n",
            "H-26\t-0.5259908437728882\tWe _ the car, he he he had _ the problem,\n",
            "P-26\t-1.6326 -0.0001 -0.1918 -0.7969 -0.9817 -0.5986 -0.1668 -1.1221 -0.1584 -0.1660 -0.0354 -0.4616\n",
            "S-15\tWe were <unk> for the <unk> because we had forgotten our <unk>\n",
            "T-15\tWe were <<unk>> for the <<unk>> because we <<unk>> <<unk>>\n",
            "H-15\t-0.45362433791160583\tWhere couldn't go into the concert because we _ our tickets.\n",
            "P-15\t-1.5838 -0.6211 -1.2552 -1.3896 -0.0066 -0.4463 -0.0028 -0.0417 -0.0015 -0.0244 -0.0657 -0.0048\n",
            "S-31\tWhat will you have done by the <unk> of the <unk>\n",
            "T-31\tWhat _ by the <<unk>> of the <<unk>>\n",
            "H-31\t-0.5504070520401001\tWhen _ the report?\n",
            "P-31\t-0.3198 -0.0731 -0.0309 -2.2260 -0.1021\n",
            "S-45\tYou hadn't studied for the <unk> so you were very <unk>\n",
            "T-45\tYou _ for the <<unk>> so you were very <<unk>>\n",
            "H-45\t-0.449734091758728\tYou _ the car, so she _ dry all summer.\n",
            "P-45\t-0.7140 -0.1229 -0.2461 -0.6223 -0.1344 -1.0330 -0.7048 -0.0589 -1.3077 -0.0029 0.0000\n",
            "S-28\tWe will have been working, so we'll have <unk> of <unk>\n",
            "T-28\tWe _, so we'll have <<unk>> of <<unk>>\n",
            "H-28\t-0.36144107580184937\tWe _ here for ten years next month.\n",
            "P-28\t-0.0028 -0.0113 -1.3365 -0.4187 -0.1069 -0.0055 -0.8470 -0.0230 -0.5012\n",
            "S-37\tJohn had never <unk> English before he came to London.\n",
            "T-37\tJohn had never _ English before he came to London.\n",
            "H-37\t-0.6621265411376953\tWhy _?\n",
            "P-37\t-0.7941 -1.1188 -0.0734\n",
            "S-25\tI will have been sleeping for three hours at <unk>\n",
            "T-25\tI _ for three hours at <<unk>>\n",
            "H-25\t-0.16562625765800476\tI _ in the for six years next week, four years at most.\n",
            "P-25\t-0.0079 -0.0009 -1.4308 -0.6454 -0.0229 -0.0429 -0.0001 -0.0006 -0.1530 -0.0035 -0.0084 -0.0001 -0.0021 0.0000\n",
            "S-47\tI <unk> a cold because I hadn't been eating <unk>\n",
            "T-47\t<<unk>> I <<unk>> a cold because I _ <<unk>>\n",
            "H-47\t-0.2668534517288208\tI _ a cake to the party,\n",
            "P-47\t-0.0026 -0.0000 -0.3503 -1.1381 -0.2154 -0.1686 -0.0298 -0.2299\n",
            "S-34\tHow many <unk> had she drunk before the <unk>\n",
            "T-34\tHow many <<unk>> _ before the <<unk>>\n",
            "H-34\t-0.5316128134727478\t_ there before we went together?\n",
            "P-34\t-2.2783 -0.4210 -0.6101 -0.2332 -0.0087 -0.1690 -0.0009\n",
            "S-3\tIn your opinion, will she be a <unk> <unk>\n",
            "T-3\tIn your opinion, _ a <<unk>> <<unk>>\n",
            "H-3\t-0.28438133001327515\tYou _ my mother a CD for Christmas.\n",
            "P-3\t-1.6455 -0.0527 -0.6153 -0.1406 -0.0448 -0.0048 -0.0217 -0.0340 -0.0001\n",
            "S-36\tWhen you got <unk> had you been eating <unk>\n",
            "T-36\tWhen you got <<unk>> _ <<unk>>\n",
            "H-36\t-0.49068281054496765\tWhen you ever _ Turkish Are but love you.\n",
            "P-36\t-0.4110 -0.0828 -0.9027 -0.0008 -0.7672 -1.2342 -0.4253 -0.9651 -0.1144 -0.0033\n",
            "S-30\tYou have bought a lot of new <unk> <unk>\n",
            "T-30\tYou have _ a lot of new <<unk>> <<unk>>\n",
            "H-30\t-0.2021300047636032\tYou _ a new bike for her birthday.\n",
            "P-30\t-0.3340 -0.0016 -0.4022 -0.3377 -0.0166 -0.0547 -0.6705 -0.0019 0.0000\n",
            "S-23\tYou were in the <unk> when I called you.\n",
            "T-23\tYou _ in the <<unk>> when I <<unk>> you.\n",
            "H-23\t-0.590146541595459\tYou _ in my office.\n",
            "P-23\t-0.8123 -0.0015 -0.5114 -2.1722 -0.0427 -0.0008\n",
            "S-8\tShe has <unk> about the <unk> for three <unk>\n",
            "T-8\tShe has _ about the <<unk>> for three <<unk>>\n",
            "H-8\t-0.469555526971817\tShe _ the when we went a restaurant.\n",
            "P-8\t-0.0013 -0.0308 -1.2587 -0.9534 -0.6826 -0.1586 -0.2803 -0.8602 0.0000\n",
            "S-33\tI hadn't been working there long when she <unk>\n",
            "T-33\tI _ there long when she <<unk>>\n",
            "H-33\t-0.2265758514404297\tI _ here for ten years when I got married.\n",
            "P-33\t-0.1427 -0.0002 -0.7220 -0.0453 -0.9596 -0.0400 -0.0522 -0.0099 -0.1560 -0.2365 -0.1279\n",
            "S-18\tWe have <unk> this <unk> for three years.\n",
            "T-18\tWe have _ this <<unk>> for three years.\n",
            "H-18\t-0.31563708186149597\tWe _ TV for three years when we got married.\n",
            "P-18\t-0.0095 -0.0054 -1.7234 -0.4222 -0.3564 -0.0177 -0.0232 -0.0302 -0.0185 -0.8649 -0.0006\n",
            "S-32\tThey weren't on the bus when I <unk>\n",
            "T-32\tThey _ on the bus when I <<unk>>\n",
            "H-32\t-0.5548502802848816\tThey _ in the library.\n",
            "P-32\t-0.0027 -0.0003 -1.8037 -0.1712 -1.3476 -0.0037\n",
            "S-49\tThe meeting will take place at 6 <unk>\n",
            "T-49\tThe meeting _ place at 6 <<unk>>\n",
            "H-49\t-0.4163566827774048\tThe _ at home at 10 o'clock\n",
            "P-49\t-0.2443 -0.4881 -1.5814 -0.5945 -0.3282 -0.0867 -0.0076 -0.0000\n",
            "S-11\tHave you ever <unk> in the <unk> <unk>\n",
            "T-11\tHave you ever _ in the <<unk>> <<unk>>\n",
            "H-11\t-0.306857705116272\tHave you ever _ a famous person?\n",
            "P-11\t-0.0168 -0.0056 -0.4112 -0.0001 -1.2166 -0.5049 -0.2997 0.0000\n",
            "S-12\tI'm <unk> I'm so tired. I haven't <unk>\n",
            "T-12\tI'm <<unk>> I'm so tired. I haven't _.\n",
            "H-12\t-0.5617719888687134\tI _ the meeting.\n",
            "P-12\t-0.1623 -0.0243 -1.3658 -1.2198 -0.0366\n",
            "S-0\tAre you <unk> I have felt <unk>\n",
            "T-0\tAre you <<unk>> I have _ <<unk>>\n",
            "H-0\t-0.41571199893951416\tHave you ever _ a famous person?\n",
            "P-0\t-0.8581 -0.0581 -0.5850 -0.0000 -1.3926 -0.3378 -0.0942 -0.0000\n",
            "S-27\tThey put their <unk> in the <unk>\n",
            "T-27\tThey _ their <<unk>> in the <<unk>>\n",
            "H-27\t-0.19634947180747986\tThey _ about the library.\n",
            "P-27\t-0.0018 -0.0169 -0.6564 -0.0119 -0.4880 -0.0032\n",
            "S-42\tHad they <unk> by <unk> train <unk>\n",
            "T-42\t_ by <<unk>> train <<unk>>\n",
            "H-42\t-0.42685946822166443\t_ by 10 o'clock?\n",
            "P-42\t-0.0607 -1.3114 -0.5363 -0.2258 -0.0000\n",
            "S-6\tWhat time will the <unk> <unk> <unk>\n",
            "T-6\tWhat time _ <<unk>>\n",
            "H-6\t-0.8490752577781677\tYou _ the report.\n",
            "P-6\t-1.6872 -0.3240 -0.3339 -1.8997 -0.0006\n",
            "S-19\tWhy had you been studying so hard?\n",
            "T-19\t_ so hard?\n",
            "H-19\t-0.41272127628326416\tWhy _ my football team would win.\n",
            "P-19\t-0.6086 -0.2343 -1.2090 -0.8396 -0.2536 -0.1223 -0.0344 -0.0000\n",
            "S-48\tYou will have <unk> the <unk>\n",
            "T-48\tYou _ the <<unk>>\n",
            "H-48\t-0.29793474078178406\tYou _ the internet.\n",
            "P-48\t-0.4578 -0.0054 -0.0153 -1.0109 -0.0002\n",
            "S-50\tWe will have read the <unk>\n",
            "T-50\tWe _ the <<unk>>\n",
            "H-50\t-0.18355827033519745\tWe _ the article.\n",
            "P-50\t-0.0289 -0.0010 -0.0365 -0.8513 -0.0001\n",
            "S-1\tWill she have arrived by <unk>\n",
            "T-1\t_ by <<unk>>\n",
            "H-1\t-0.11465978622436523\t_ by 6?\n",
            "P-1\t-0.0030 -0.2223 -0.2332 -0.0002\n",
            "S-10\tI sent you an <unk> <unk>\n",
            "T-10\tI _ you an <<unk>> <<unk>>\n",
            "H-10\t-0.38929256796836853\tI _ about the fire.\n",
            "P-10\t-0.0027 -0.0005 -1.8435 -0.0245 -0.4642 -0.0003\n",
            "S-7\tHe had met her before <unk>\n",
            "T-7\tHe _ her before <<unk>>\n",
            "H-7\t-0.2892932891845703\tHe has _ his wallet again.\n",
            "P-7\t-0.0501 -1.7502 -0.0105 -0.2059 -0.0034 -0.0049 0.0000\n",
            "S-21\tI will come and <unk> you.\n",
            "T-21\tI _ and <<unk>> you.\n",
            "H-21\t-0.3930908143520355\tI _ the internet.\n",
            "P-21\t-0.0068 -0.0021 -0.6501 -1.3053 -0.0012\n",
            "S-9\tYou lost your keys last week.\n",
            "T-9\tYou _ your keys last week.\n",
            "H-9\t-0.19477422535419464\tYou _ about books this morning.\n",
            "P-9\t-0.5001 -0.0006 -0.1686 -0.5510 -0.0745 -0.0686 -0.0000\n",
            "S-17\tWill they be <unk> a meeting?\n",
            "T-17\t_ a meeting?\n",
            "H-17\t-0.28093504905700684\t_ an email?\n",
            "P-17\t-0.0606 -0.9227 -0.1402 -0.0003\n",
            "S-38\tWe lent John <unk> .\n",
            "T-38\tWe _ John <<unk>>\n",
            "H-38\t-0.3443722724914551\tWe _ 500m.\n",
            "P-38\t-0.1878 -0.0005 -1.1865 -0.0027\n",
            "S-24\tWill he be <unk> coffee?\n",
            "T-24\t_ coffee?\n",
            "H-24\t-0.23700270056724548\t_ lunch?\n",
            "P-24\t-0.0168 -0.6939 -0.0003\n",
            "S-39\t<unk> <unk> after the bus.\n",
            "T-39\t<<unk>> _ after the bus.\n",
            "H-39\t-0.7344386577606201\tIt _ in the garden.\n",
            "P-39\t-1.8268 -0.0014 -2.1278 -0.0045 -0.4460 -0.0001\n",
            "S-4\tIt was cold last night.\n",
            "T-4\tIt _ cold last night.\n",
            "H-4\t-0.1067144125699997\tIt _ tired last night.\n",
            "P-4\t-0.0460 -0.0004 -0.4894 -0.0766 -0.0272 -0.0006\n",
            "S-29\tYou will have fallen <unk>\n",
            "T-29\tYou _ <<unk>>\n",
            "H-29\t-0.46969151496887207\tYou _ Paris.\n",
            "P-29\t-0.7368 -0.0089 -1.1330 -0.0000\n",
            "S-5\tIt will be <unk>\n",
            "T-5\tIt _.\n",
            "H-5\t-0.3294146955013275\tYou _.\n",
            "P-5\t-0.9572 -0.0298 -0.0012\n",
            "S-16\tIt will be snowing.\n",
            "T-16\tIt _.\n",
            "H-16\t-0.29985174536705017\tYou _.\n",
            "P-16\t-0.8596 -0.0374 -0.0026\n",
            "S-40\tWill you be <unk>\n",
            "T-40\t_?\n",
            "H-40\t-0.07196776568889618\t_ tomorrow?\n",
            "P-40\t-0.1007 -0.1121 -0.0031\n",
            "S-41\tYou aren't an <unk>\n",
            "T-41\tYou _ an <<unk>>\n",
            "H-41\t-0.3411029875278473\tYou _ a doctor.\n",
            "P-41\t-0.4995 -0.0001 -0.8892 -0.3167 -0.0000\n",
            "S-13\tYou aren't a teacher.\n",
            "T-13\tYou _ a teacher.\n",
            "H-13\t-0.187001034617424\tYou _ a teacher.\n",
            "P-13\t-0.2760 -0.0002 -0.0020 -0.6565 -0.0003\n",
            "S-35\tWe aren't thirsty.\n",
            "T-35\tWe _ thirsty.\n",
            "H-35\t-0.4318103790283203\tWe _ sunny.\n",
            "P-35\t-0.1131 -0.0005 -1.6137 -0.0000\n",
            "S-2\tI am <unk>\n",
            "T-2\tI _ <<unk>>\n",
            "H-2\t-0.12822175025939941\tI _ thirsty.\n",
            "P-2\t-0.0030 -0.0005 -0.5094 -0.0000\n",
            "S-43\tYou are <unk>\n",
            "T-43\tYou _ <<unk>>\n",
            "H-43\t-0.3875420093536377\tYou _ tired.\n",
            "P-43\t-0.6780 -0.0050 -0.8672 -0.0000\n",
            "S-22\tWe are hungry.\n",
            "T-22\tWe _ hungry.\n",
            "H-22\t-0.21858620643615723\tWe _ tired.\n",
            "P-22\t-0.0506 -0.0015 -0.8222 -0.0000\n",
            "| Translated 51 sentences (351 tokens) in 0.1s (714.48 sentences/s, 4917.30 tokens/s)\n",
            "| Generate test with beam=3: BLEU4 = 6.77, 40.3/15.7/3.5/1.3 (BP=0.920, ratio=0.923, syslen=300, reflen=325)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}